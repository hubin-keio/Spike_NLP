{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tqdm\n",
    "import torch\n",
    "import textwrap\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from typing import Union\n",
    "from prettytable import PrettyTable\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "from datetime import date\n",
    "from os import path\n",
    "from typing import Union\n",
    "import logging\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from collections import defaultdict, OrderedDict\n",
    "from pnlp.db.dataset import SeqDataset, initialize_db\n",
    "from pnlp.embedding.tokenizer import ProteinTokenizer, token_to_index, index_to_token\n",
    "from pnlp.embedding.nlp_embedding import NLPEmbedding\n",
    "from pnlp.model.language import ProteinLM\n",
    "from pnlp.model.bert import BERT\n",
    "from runner_util import plot_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"\n",
    "    Count model parameters and print a summary\n",
    "\n",
    "    A nice hack from:\n",
    "    https://stackoverflow.com/a/62508086/1992369\n",
    "    \"\"\"\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\\n\")\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class FCN(nn.Module):\n",
    "    \"\"\" Fully Connected Network \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 fcn_input_size,     # The number of input features\n",
    "                 fcn_hidden_size,    # The number of features in hidden layer of FCN.\n",
    "                 device):            # Device ('cpu' or 'cuda')\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # FCN layers\n",
    "        self.fcn = nn.Sequential(nn.Linear(fcn_input_size, fcn_hidden_size),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(fcn_hidden_size, 1))  # Adjust this line based on the required output size\n",
    "\n",
    "    def forward(self, x):\n",
    "        fcn_out = self.fcn(x)\n",
    "        fcn_final_out = fcn_out[:, -1, :]\n",
    "        prediction = fcn_final_out.to(self.device)\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+\n",
      "|   Modules    | Parameters |\n",
      "+--------------+------------+\n",
      "| fcn.0.weight |   102400   |\n",
      "|  fcn.0.bias  |    320     |\n",
      "| fcn.2.weight |    320     |\n",
      "|  fcn.2.bias  |     1      |\n",
      "+--------------+------------+\n",
      "Total Trainable Params: 103041\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "103041"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddedDMSDataset(Dataset):\n",
    "    \"\"\" Binding or Expression DMS Dataset \"\"\"\n",
    "    \n",
    "    def __init__(self, pickle_file:str):\n",
    "        \"\"\"\n",
    "        Load from pickle file:\n",
    "        - sequence label (seq_id), \n",
    "        - binding or expression numerical target (log10Ka or ML_meanF), and \n",
    "        - embeddings\n",
    "        \"\"\"\n",
    "        with open(pickle_file, 'rb') as f:\n",
    "            dms_list = pickle.load(f)\n",
    "        \n",
    "            self.labels = [entry['seq_id'] for entry in dms_list]\n",
    "            self.numerical = [entry[\"log10Ka\" if \"binding\" in pickle_file else \"ML_meanF\"] for entry in dms_list]\n",
    "            self.embeddings = [entry['embedding'] for entry in dms_list]\n",
    " \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert to pytorch geometric graph\n",
    "        embedding = self.embeddings[idx]\n",
    "        edges = [(i, i+1) for i in range(embedding.size(0) - 1)]\n",
    "        edge_index = torch.tensor(edges, dtype=torch.int64).t().contiguous()\n",
    "        y = torch.tensor([self.numerical[idx]], dtype=torch.float32).view(-1, 1)\n",
    "        \n",
    "        return Data(x=embedding, edge_index=edge_index, y=y)\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "embedded_train_pkl = '../../../data/pickles/dms_mutation_binding_Kds_train_esm_embedded.pkl' \n",
    "train_dataset = EmbeddedDMSDataset(embedded_train_pkl)\n",
    "\n",
    "#  FCN input\n",
    "fcn_input_size = train_dataset.embeddings[0].size(1)   \n",
    "fcn_hidden_size = fcn_input_size\n",
    "model = FCN(fcn_input_size, fcn_hidden_size, device)\n",
    "\n",
    "# Run\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import SAGEConv, global_mean_pool\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "class GraphSAGE(nn.Module):\n",
    "    \"\"\" GraphSAGE. \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, 16)\n",
    "        self.conv2 = SAGEConv(16, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320\n",
      "+--------------------+------------+\n",
      "|      Modules       | Parameters |\n",
      "+--------------------+------------+\n",
      "| conv1.lin_l.weight |    5120    |\n",
      "|  conv1.lin_l.bias  |     16     |\n",
      "| conv1.lin_r.weight |    5120    |\n",
      "| conv2.lin_l.weight |     16     |\n",
      "|  conv2.lin_l.bias  |     1      |\n",
      "| conv2.lin_r.weight |     16     |\n",
      "+--------------------+------------+\n",
      "Total Trainable Params: 10289\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10289"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddedDMSDataset(Dataset):\n",
    "    \"\"\" Binding or Expression DMS Dataset \"\"\"\n",
    "    \n",
    "    def __init__(self, pickle_file:str):\n",
    "        \"\"\"\n",
    "        Load from pickle file:\n",
    "        - sequence label (seq_id), \n",
    "        - binding or expression numerical target (log10Ka or ML_meanF), and \n",
    "        - embeddings\n",
    "        \"\"\"\n",
    "        with open(pickle_file, 'rb') as f:\n",
    "            dms_list = pickle.load(f)\n",
    "        \n",
    "            self.labels = [entry['seq_id'] for entry in dms_list]\n",
    "            self.numerical = [entry[\"log10Ka\" if \"binding\" in pickle_file else \"ML_meanF\"] for entry in dms_list]\n",
    "            self.embeddings = [entry['embedding'] for entry in dms_list]\n",
    " \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert to pytorch geometric graph\n",
    "        embedding = self.embeddings[idx]\n",
    "        edges = [(i, i+1) for i in range(embedding.size(0) - 1)]\n",
    "        edge_index = torch.tensor(edges, dtype=torch.int64).t().contiguous()\n",
    "        y = torch.tensor([self.numerical[idx]], dtype=torch.float32).view(-1, 1)\n",
    "        \n",
    "        return Data(x=embedding, edge_index=edge_index, y=y)\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "embedded_train_pkl = '../../../data/pickles/dms_mutation_binding_Kds_train_esm_embedded.pkl' \n",
    "train_dataset = EmbeddedDMSDataset(embedded_train_pkl)\n",
    "\n",
    "# GraphSAGE input\n",
    "input_channels = train_dataset.embeddings[0].size(1) # number of input channels (dimensions of the embeddings)\n",
    "out_channels = 1  # For regression output\n",
    "model = GraphSAGE(input_channels, out_channels).to(device)\n",
    "\n",
    "# Run\n",
    "print(input_channels)\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" BLSTM model with FCN layer. \"\"\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class BLSTM(nn.Module):\n",
    "    \"\"\" Bidirectional LSTM with FCN layer. \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 lstm_input_size,    # The number of expected features.\n",
    "                 lstm_hidden_size,   # The number of features in hidden state h.\n",
    "                 lstm_num_layers,    # Number of recurrent layers in LSTM.\n",
    "                 lstm_bidirectional, # Bidrectional LSTM.\n",
    "                 fcn_hidden_size):   # The number of features in hidden layer of CN.\n",
    "        super().__init__()\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=lstm_input_size,\n",
    "                            hidden_size=lstm_hidden_size,\n",
    "                            num_layers=lstm_num_layers,\n",
    "                            bidirectional=lstm_bidirectional,\n",
    "                            batch_first=True)           \n",
    "\n",
    "        # FCN\n",
    "        if lstm_bidirectional:\n",
    "            self.fcn = nn.Sequential(nn.Linear(2 * lstm_hidden_size, fcn_hidden_size),\n",
    "                                     nn.ReLU())\n",
    "        else:\n",
    "            self.fcn = nn.Sequential(nn.Linear(lstm_hidden_size, fcn_hidden_size),\n",
    "                                     nn.ReLU())\n",
    "\n",
    "        # FCN output layer\n",
    "        self.out = nn.Linear(fcn_hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        num_directions = 2 if self.lstm.bidirectional else 1\n",
    "        h_0 = torch.zeros(num_directions * self.lstm.num_layers, x.size(0), self.lstm.hidden_size, device=x.device)\n",
    "        c_0 = torch.zeros(num_directions * self.lstm.num_layers, x.size(0), self.lstm.hidden_size, device=x.device)\n",
    "\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x, (h_0, c_0))\n",
    "        h_n.detach()\n",
    "        c_n.detach()\n",
    "        lstm_final_out = lstm_out[:, -1, :]\n",
    "        fcn_out = self.fcn(lstm_final_out)\n",
    "        prediction = self.out(fcn_out)\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+------------+\n",
      "|          Modules          | Parameters |\n",
      "+---------------------------+------------+\n",
      "|     lstm.weight_ih_l0     |   409600   |\n",
      "|     lstm.weight_hh_l0     |   409600   |\n",
      "|      lstm.bias_ih_l0      |    1280    |\n",
      "|      lstm.bias_hh_l0      |    1280    |\n",
      "| lstm.weight_ih_l0_reverse |   409600   |\n",
      "| lstm.weight_hh_l0_reverse |   409600   |\n",
      "|  lstm.bias_ih_l0_reverse  |    1280    |\n",
      "|  lstm.bias_hh_l0_reverse  |    1280    |\n",
      "|        fcn.0.weight       |   204800   |\n",
      "|         fcn.0.bias        |    320     |\n",
      "|         out.weight        |    320     |\n",
      "|          out.bias         |     1      |\n",
      "+---------------------------+------------+\n",
      "Total Trainable Params: 1848961\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1848961"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BLSTM input\n",
    "lstm_input_size = 320\n",
    "lstm_hidden_size = 320\n",
    "lstm_num_layers = 1        \n",
    "lstm_bidrectional = True   \n",
    "fcn_hidden_size = 320\n",
    "model = BLSTM(lstm_input_size, lstm_hidden_size, lstm_num_layers, lstm_bidrectional, fcn_hidden_size)\n",
    "\n",
    "# Run\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESM-BLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, EsmModel \n",
    "\n",
    "class ESM_BLSTM(nn.Module):\n",
    "    def __init__(self, esm, blstm):\n",
    "        super().__init__()\n",
    "        self.esm = esm\n",
    "        self.blstm = blstm\n",
    "\n",
    "    def forward(self, tokenized_seqs):\n",
    "        with torch.set_grad_enabled(self.training):  # Enable gradients, managed by model.eval() or model.train() in epoch_iteration\n",
    "            esm_output = self.esm(**tokenized_seqs).last_hidden_state\n",
    "            reshaped_output = esm_output.squeeze(0)  \n",
    "            output = self.blstm(reshaped_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------+------------+\n",
      "|                      Modules                      | Parameters |\n",
      "+---------------------------------------------------+------------+\n",
      "|       esm.embeddings.word_embeddings.weight       |   10560    |\n",
      "|     esm.embeddings.position_embeddings.weight     |   328320   |\n",
      "|  esm.encoder.layer.0.attention.self.query.weight  |   102400   |\n",
      "|   esm.encoder.layer.0.attention.self.query.bias   |    320     |\n",
      "|   esm.encoder.layer.0.attention.self.key.weight   |   102400   |\n",
      "|    esm.encoder.layer.0.attention.self.key.bias    |    320     |\n",
      "|  esm.encoder.layer.0.attention.self.value.weight  |   102400   |\n",
      "|   esm.encoder.layer.0.attention.self.value.bias   |    320     |\n",
      "| esm.encoder.layer.0.attention.output.dense.weight |   102400   |\n",
      "|  esm.encoder.layer.0.attention.output.dense.bias  |    320     |\n",
      "|   esm.encoder.layer.0.attention.LayerNorm.weight  |    320     |\n",
      "|    esm.encoder.layer.0.attention.LayerNorm.bias   |    320     |\n",
      "|   esm.encoder.layer.0.intermediate.dense.weight   |   409600   |\n",
      "|    esm.encoder.layer.0.intermediate.dense.bias    |    1280    |\n",
      "|      esm.encoder.layer.0.output.dense.weight      |   409600   |\n",
      "|       esm.encoder.layer.0.output.dense.bias       |    320     |\n",
      "|        esm.encoder.layer.0.LayerNorm.weight       |    320     |\n",
      "|         esm.encoder.layer.0.LayerNorm.bias        |    320     |\n",
      "|  esm.encoder.layer.1.attention.self.query.weight  |   102400   |\n",
      "|   esm.encoder.layer.1.attention.self.query.bias   |    320     |\n",
      "|   esm.encoder.layer.1.attention.self.key.weight   |   102400   |\n",
      "|    esm.encoder.layer.1.attention.self.key.bias    |    320     |\n",
      "|  esm.encoder.layer.1.attention.self.value.weight  |   102400   |\n",
      "|   esm.encoder.layer.1.attention.self.value.bias   |    320     |\n",
      "| esm.encoder.layer.1.attention.output.dense.weight |   102400   |\n",
      "|  esm.encoder.layer.1.attention.output.dense.bias  |    320     |\n",
      "|   esm.encoder.layer.1.attention.LayerNorm.weight  |    320     |\n",
      "|    esm.encoder.layer.1.attention.LayerNorm.bias   |    320     |\n",
      "|   esm.encoder.layer.1.intermediate.dense.weight   |   409600   |\n",
      "|    esm.encoder.layer.1.intermediate.dense.bias    |    1280    |\n",
      "|      esm.encoder.layer.1.output.dense.weight      |   409600   |\n",
      "|       esm.encoder.layer.1.output.dense.bias       |    320     |\n",
      "|        esm.encoder.layer.1.LayerNorm.weight       |    320     |\n",
      "|         esm.encoder.layer.1.LayerNorm.bias        |    320     |\n",
      "|  esm.encoder.layer.2.attention.self.query.weight  |   102400   |\n",
      "|   esm.encoder.layer.2.attention.self.query.bias   |    320     |\n",
      "|   esm.encoder.layer.2.attention.self.key.weight   |   102400   |\n",
      "|    esm.encoder.layer.2.attention.self.key.bias    |    320     |\n",
      "|  esm.encoder.layer.2.attention.self.value.weight  |   102400   |\n",
      "|   esm.encoder.layer.2.attention.self.value.bias   |    320     |\n",
      "| esm.encoder.layer.2.attention.output.dense.weight |   102400   |\n",
      "|  esm.encoder.layer.2.attention.output.dense.bias  |    320     |\n",
      "|   esm.encoder.layer.2.attention.LayerNorm.weight  |    320     |\n",
      "|    esm.encoder.layer.2.attention.LayerNorm.bias   |    320     |\n",
      "|   esm.encoder.layer.2.intermediate.dense.weight   |   409600   |\n",
      "|    esm.encoder.layer.2.intermediate.dense.bias    |    1280    |\n",
      "|      esm.encoder.layer.2.output.dense.weight      |   409600   |\n",
      "|       esm.encoder.layer.2.output.dense.bias       |    320     |\n",
      "|        esm.encoder.layer.2.LayerNorm.weight       |    320     |\n",
      "|         esm.encoder.layer.2.LayerNorm.bias        |    320     |\n",
      "|  esm.encoder.layer.3.attention.self.query.weight  |   102400   |\n",
      "|   esm.encoder.layer.3.attention.self.query.bias   |    320     |\n",
      "|   esm.encoder.layer.3.attention.self.key.weight   |   102400   |\n",
      "|    esm.encoder.layer.3.attention.self.key.bias    |    320     |\n",
      "|  esm.encoder.layer.3.attention.self.value.weight  |   102400   |\n",
      "|   esm.encoder.layer.3.attention.self.value.bias   |    320     |\n",
      "| esm.encoder.layer.3.attention.output.dense.weight |   102400   |\n",
      "|  esm.encoder.layer.3.attention.output.dense.bias  |    320     |\n",
      "|   esm.encoder.layer.3.attention.LayerNorm.weight  |    320     |\n",
      "|    esm.encoder.layer.3.attention.LayerNorm.bias   |    320     |\n",
      "|   esm.encoder.layer.3.intermediate.dense.weight   |   409600   |\n",
      "|    esm.encoder.layer.3.intermediate.dense.bias    |    1280    |\n",
      "|      esm.encoder.layer.3.output.dense.weight      |   409600   |\n",
      "|       esm.encoder.layer.3.output.dense.bias       |    320     |\n",
      "|        esm.encoder.layer.3.LayerNorm.weight       |    320     |\n",
      "|         esm.encoder.layer.3.LayerNorm.bias        |    320     |\n",
      "|  esm.encoder.layer.4.attention.self.query.weight  |   102400   |\n",
      "|   esm.encoder.layer.4.attention.self.query.bias   |    320     |\n",
      "|   esm.encoder.layer.4.attention.self.key.weight   |   102400   |\n",
      "|    esm.encoder.layer.4.attention.self.key.bias    |    320     |\n",
      "|  esm.encoder.layer.4.attention.self.value.weight  |   102400   |\n",
      "|   esm.encoder.layer.4.attention.self.value.bias   |    320     |\n",
      "| esm.encoder.layer.4.attention.output.dense.weight |   102400   |\n",
      "|  esm.encoder.layer.4.attention.output.dense.bias  |    320     |\n",
      "|   esm.encoder.layer.4.attention.LayerNorm.weight  |    320     |\n",
      "|    esm.encoder.layer.4.attention.LayerNorm.bias   |    320     |\n",
      "|   esm.encoder.layer.4.intermediate.dense.weight   |   409600   |\n",
      "|    esm.encoder.layer.4.intermediate.dense.bias    |    1280    |\n",
      "|      esm.encoder.layer.4.output.dense.weight      |   409600   |\n",
      "|       esm.encoder.layer.4.output.dense.bias       |    320     |\n",
      "|        esm.encoder.layer.4.LayerNorm.weight       |    320     |\n",
      "|         esm.encoder.layer.4.LayerNorm.bias        |    320     |\n",
      "|  esm.encoder.layer.5.attention.self.query.weight  |   102400   |\n",
      "|   esm.encoder.layer.5.attention.self.query.bias   |    320     |\n",
      "|   esm.encoder.layer.5.attention.self.key.weight   |   102400   |\n",
      "|    esm.encoder.layer.5.attention.self.key.bias    |    320     |\n",
      "|  esm.encoder.layer.5.attention.self.value.weight  |   102400   |\n",
      "|   esm.encoder.layer.5.attention.self.value.bias   |    320     |\n",
      "| esm.encoder.layer.5.attention.output.dense.weight |   102400   |\n",
      "|  esm.encoder.layer.5.attention.output.dense.bias  |    320     |\n",
      "|   esm.encoder.layer.5.attention.LayerNorm.weight  |    320     |\n",
      "|    esm.encoder.layer.5.attention.LayerNorm.bias   |    320     |\n",
      "|   esm.encoder.layer.5.intermediate.dense.weight   |   409600   |\n",
      "|    esm.encoder.layer.5.intermediate.dense.bias    |    1280    |\n",
      "|      esm.encoder.layer.5.output.dense.weight      |   409600   |\n",
      "|       esm.encoder.layer.5.output.dense.bias       |    320     |\n",
      "|        esm.encoder.layer.5.LayerNorm.weight       |    320     |\n",
      "|         esm.encoder.layer.5.LayerNorm.bias        |    320     |\n",
      "|      esm.encoder.emb_layer_norm_after.weight      |    320     |\n",
      "|       esm.encoder.emb_layer_norm_after.bias       |    320     |\n",
      "|              esm.pooler.dense.weight              |   102400   |\n",
      "|               esm.pooler.dense.bias               |    320     |\n",
      "|         esm.contact_head.regression.weight        |    120     |\n",
      "|          esm.contact_head.regression.bias         |     1      |\n",
      "|              blstm.lstm.weight_ih_l0              |   409600   |\n",
      "|              blstm.lstm.weight_hh_l0              |   409600   |\n",
      "|               blstm.lstm.bias_ih_l0               |    1280    |\n",
      "|               blstm.lstm.bias_hh_l0               |    1280    |\n",
      "|          blstm.lstm.weight_ih_l0_reverse          |   409600   |\n",
      "|          blstm.lstm.weight_hh_l0_reverse          |   409600   |\n",
      "|           blstm.lstm.bias_ih_l0_reverse           |    1280    |\n",
      "|           blstm.lstm.bias_hh_l0_reverse           |    1280    |\n",
      "|                 blstm.fcn.0.weight                |   204800   |\n",
      "|                  blstm.fcn.0.bias                 |    320     |\n",
      "|                  blstm.out.weight                 |    320     |\n",
      "|                   blstm.out.bias                  |     1      |\n",
      "+---------------------------------------------------+------------+\n",
      "Total Trainable Params: 9689082\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9689082"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ESM input\n",
    "esm = EsmModel.from_pretrained(\"facebook/esm2_t6_8M_UR50D\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "\n",
    "# BLSTM input\n",
    "lstm_input_size = 320\n",
    "lstm_hidden_size = 320\n",
    "lstm_num_layers = 1        \n",
    "lstm_bidrectional = True   \n",
    "fcn_hidden_size = 320\n",
    "blstm = BLSTM(lstm_input_size, lstm_hidden_size, lstm_num_layers, lstm_bidrectional, fcn_hidden_size)\n",
    "\n",
    "model = ESM_BLSTM(esm, blstm)\n",
    "\n",
    "# Run\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT-BLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" BLSTM with FCN layer, MLM, and BERT. \"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pnlp.model.language import ProteinMaskedLanguageModel, BERT\n",
    "\n",
    "class BERT_BLSTM(nn.Module):\n",
    "    \"\"\"\" BLSTM with FCN layer, MLM, and BERT. \"\"\"\n",
    "    \n",
    "    def __init__(self, bert: BERT, blstm:BLSTM, vocab_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bert = bert\n",
    "        self.mlm = ProteinMaskedLanguageModel(self.bert.hidden, vocab_size)\n",
    "        self.blstm = blstm\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.bert(x)\n",
    "        error_1 = self.mlm(x) # error from masked language\n",
    "        error_2 = self.blstm(x) # error from regession\n",
    "\n",
    "        return error_1, error_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------+------------+\n",
      "|                          Modules                          | Parameters |\n",
      "+-----------------------------------------------------------+------------+\n",
      "|           bert.embedding.token_embedding.weight           |    8960    |\n",
      "|    bert.transformer_blocks.0.attention.linears.0.weight   |   102400   |\n",
      "|     bert.transformer_blocks.0.attention.linears.0.bias    |    320     |\n",
      "|    bert.transformer_blocks.0.attention.linears.1.weight   |   102400   |\n",
      "|     bert.transformer_blocks.0.attention.linears.1.bias    |    320     |\n",
      "|    bert.transformer_blocks.0.attention.linears.2.weight   |   102400   |\n",
      "|     bert.transformer_blocks.0.attention.linears.2.bias    |    320     |\n",
      "|    bert.transformer_blocks.0.attention.linears.3.weight   |   102400   |\n",
      "|     bert.transformer_blocks.0.attention.linears.3.bias    |    320     |\n",
      "|  bert.transformer_blocks.0.attention.output_linear.weight |   102400   |\n",
      "|   bert.transformer_blocks.0.attention.output_linear.bias  |    320     |\n",
      "|     bert.transformer_blocks.0.feed_forward.w_1.weight     |   409600   |\n",
      "|      bert.transformer_blocks.0.feed_forward.w_1.bias      |    1280    |\n",
      "|     bert.transformer_blocks.0.feed_forward.w_2.weight     |   409600   |\n",
      "|      bert.transformer_blocks.0.feed_forward.w_2.bias      |    320     |\n",
      "|     bert.transformer_blocks.0.input_sublayer.norm.a_2     |    320     |\n",
      "|     bert.transformer_blocks.0.input_sublayer.norm.b_2     |    320     |\n",
      "|     bert.transformer_blocks.0.output_sublayer.norm.a_2    |    320     |\n",
      "|     bert.transformer_blocks.0.output_sublayer.norm.b_2    |    320     |\n",
      "|    bert.transformer_blocks.1.attention.linears.0.weight   |   102400   |\n",
      "|     bert.transformer_blocks.1.attention.linears.0.bias    |    320     |\n",
      "|    bert.transformer_blocks.1.attention.linears.1.weight   |   102400   |\n",
      "|     bert.transformer_blocks.1.attention.linears.1.bias    |    320     |\n",
      "|    bert.transformer_blocks.1.attention.linears.2.weight   |   102400   |\n",
      "|     bert.transformer_blocks.1.attention.linears.2.bias    |    320     |\n",
      "|    bert.transformer_blocks.1.attention.linears.3.weight   |   102400   |\n",
      "|     bert.transformer_blocks.1.attention.linears.3.bias    |    320     |\n",
      "|  bert.transformer_blocks.1.attention.output_linear.weight |   102400   |\n",
      "|   bert.transformer_blocks.1.attention.output_linear.bias  |    320     |\n",
      "|     bert.transformer_blocks.1.feed_forward.w_1.weight     |   409600   |\n",
      "|      bert.transformer_blocks.1.feed_forward.w_1.bias      |    1280    |\n",
      "|     bert.transformer_blocks.1.feed_forward.w_2.weight     |   409600   |\n",
      "|      bert.transformer_blocks.1.feed_forward.w_2.bias      |    320     |\n",
      "|     bert.transformer_blocks.1.input_sublayer.norm.a_2     |    320     |\n",
      "|     bert.transformer_blocks.1.input_sublayer.norm.b_2     |    320     |\n",
      "|     bert.transformer_blocks.1.output_sublayer.norm.a_2    |    320     |\n",
      "|     bert.transformer_blocks.1.output_sublayer.norm.b_2    |    320     |\n",
      "|    bert.transformer_blocks.2.attention.linears.0.weight   |   102400   |\n",
      "|     bert.transformer_blocks.2.attention.linears.0.bias    |    320     |\n",
      "|    bert.transformer_blocks.2.attention.linears.1.weight   |   102400   |\n",
      "|     bert.transformer_blocks.2.attention.linears.1.bias    |    320     |\n",
      "|    bert.transformer_blocks.2.attention.linears.2.weight   |   102400   |\n",
      "|     bert.transformer_blocks.2.attention.linears.2.bias    |    320     |\n",
      "|    bert.transformer_blocks.2.attention.linears.3.weight   |   102400   |\n",
      "|     bert.transformer_blocks.2.attention.linears.3.bias    |    320     |\n",
      "|  bert.transformer_blocks.2.attention.output_linear.weight |   102400   |\n",
      "|   bert.transformer_blocks.2.attention.output_linear.bias  |    320     |\n",
      "|     bert.transformer_blocks.2.feed_forward.w_1.weight     |   409600   |\n",
      "|      bert.transformer_blocks.2.feed_forward.w_1.bias      |    1280    |\n",
      "|     bert.transformer_blocks.2.feed_forward.w_2.weight     |   409600   |\n",
      "|      bert.transformer_blocks.2.feed_forward.w_2.bias      |    320     |\n",
      "|     bert.transformer_blocks.2.input_sublayer.norm.a_2     |    320     |\n",
      "|     bert.transformer_blocks.2.input_sublayer.norm.b_2     |    320     |\n",
      "|     bert.transformer_blocks.2.output_sublayer.norm.a_2    |    320     |\n",
      "|     bert.transformer_blocks.2.output_sublayer.norm.b_2    |    320     |\n",
      "|    bert.transformer_blocks.3.attention.linears.0.weight   |   102400   |\n",
      "|     bert.transformer_blocks.3.attention.linears.0.bias    |    320     |\n",
      "|    bert.transformer_blocks.3.attention.linears.1.weight   |   102400   |\n",
      "|     bert.transformer_blocks.3.attention.linears.1.bias    |    320     |\n",
      "|    bert.transformer_blocks.3.attention.linears.2.weight   |   102400   |\n",
      "|     bert.transformer_blocks.3.attention.linears.2.bias    |    320     |\n",
      "|    bert.transformer_blocks.3.attention.linears.3.weight   |   102400   |\n",
      "|     bert.transformer_blocks.3.attention.linears.3.bias    |    320     |\n",
      "|  bert.transformer_blocks.3.attention.output_linear.weight |   102400   |\n",
      "|   bert.transformer_blocks.3.attention.output_linear.bias  |    320     |\n",
      "|     bert.transformer_blocks.3.feed_forward.w_1.weight     |   409600   |\n",
      "|      bert.transformer_blocks.3.feed_forward.w_1.bias      |    1280    |\n",
      "|     bert.transformer_blocks.3.feed_forward.w_2.weight     |   409600   |\n",
      "|      bert.transformer_blocks.3.feed_forward.w_2.bias      |    320     |\n",
      "|     bert.transformer_blocks.3.input_sublayer.norm.a_2     |    320     |\n",
      "|     bert.transformer_blocks.3.input_sublayer.norm.b_2     |    320     |\n",
      "|     bert.transformer_blocks.3.output_sublayer.norm.a_2    |    320     |\n",
      "|     bert.transformer_blocks.3.output_sublayer.norm.b_2    |    320     |\n",
      "|    bert.transformer_blocks.4.attention.linears.0.weight   |   102400   |\n",
      "|     bert.transformer_blocks.4.attention.linears.0.bias    |    320     |\n",
      "|    bert.transformer_blocks.4.attention.linears.1.weight   |   102400   |\n",
      "|     bert.transformer_blocks.4.attention.linears.1.bias    |    320     |\n",
      "|    bert.transformer_blocks.4.attention.linears.2.weight   |   102400   |\n",
      "|     bert.transformer_blocks.4.attention.linears.2.bias    |    320     |\n",
      "|    bert.transformer_blocks.4.attention.linears.3.weight   |   102400   |\n",
      "|     bert.transformer_blocks.4.attention.linears.3.bias    |    320     |\n",
      "|  bert.transformer_blocks.4.attention.output_linear.weight |   102400   |\n",
      "|   bert.transformer_blocks.4.attention.output_linear.bias  |    320     |\n",
      "|     bert.transformer_blocks.4.feed_forward.w_1.weight     |   409600   |\n",
      "|      bert.transformer_blocks.4.feed_forward.w_1.bias      |    1280    |\n",
      "|     bert.transformer_blocks.4.feed_forward.w_2.weight     |   409600   |\n",
      "|      bert.transformer_blocks.4.feed_forward.w_2.bias      |    320     |\n",
      "|     bert.transformer_blocks.4.input_sublayer.norm.a_2     |    320     |\n",
      "|     bert.transformer_blocks.4.input_sublayer.norm.b_2     |    320     |\n",
      "|     bert.transformer_blocks.4.output_sublayer.norm.a_2    |    320     |\n",
      "|     bert.transformer_blocks.4.output_sublayer.norm.b_2    |    320     |\n",
      "|    bert.transformer_blocks.5.attention.linears.0.weight   |   102400   |\n",
      "|     bert.transformer_blocks.5.attention.linears.0.bias    |    320     |\n",
      "|    bert.transformer_blocks.5.attention.linears.1.weight   |   102400   |\n",
      "|     bert.transformer_blocks.5.attention.linears.1.bias    |    320     |\n",
      "|    bert.transformer_blocks.5.attention.linears.2.weight   |   102400   |\n",
      "|     bert.transformer_blocks.5.attention.linears.2.bias    |    320     |\n",
      "|    bert.transformer_blocks.5.attention.linears.3.weight   |   102400   |\n",
      "|     bert.transformer_blocks.5.attention.linears.3.bias    |    320     |\n",
      "|  bert.transformer_blocks.5.attention.output_linear.weight |   102400   |\n",
      "|   bert.transformer_blocks.5.attention.output_linear.bias  |    320     |\n",
      "|     bert.transformer_blocks.5.feed_forward.w_1.weight     |   409600   |\n",
      "|      bert.transformer_blocks.5.feed_forward.w_1.bias      |    1280    |\n",
      "|     bert.transformer_blocks.5.feed_forward.w_2.weight     |   409600   |\n",
      "|      bert.transformer_blocks.5.feed_forward.w_2.bias      |    320     |\n",
      "|     bert.transformer_blocks.5.input_sublayer.norm.a_2     |    320     |\n",
      "|     bert.transformer_blocks.5.input_sublayer.norm.b_2     |    320     |\n",
      "|     bert.transformer_blocks.5.output_sublayer.norm.a_2    |    320     |\n",
      "|     bert.transformer_blocks.5.output_sublayer.norm.b_2    |    320     |\n",
      "|    bert.transformer_blocks.6.attention.linears.0.weight   |   102400   |\n",
      "|     bert.transformer_blocks.6.attention.linears.0.bias    |    320     |\n",
      "|    bert.transformer_blocks.6.attention.linears.1.weight   |   102400   |\n",
      "|     bert.transformer_blocks.6.attention.linears.1.bias    |    320     |\n",
      "|    bert.transformer_blocks.6.attention.linears.2.weight   |   102400   |\n",
      "|     bert.transformer_blocks.6.attention.linears.2.bias    |    320     |\n",
      "|    bert.transformer_blocks.6.attention.linears.3.weight   |   102400   |\n",
      "|     bert.transformer_blocks.6.attention.linears.3.bias    |    320     |\n",
      "|  bert.transformer_blocks.6.attention.output_linear.weight |   102400   |\n",
      "|   bert.transformer_blocks.6.attention.output_linear.bias  |    320     |\n",
      "|     bert.transformer_blocks.6.feed_forward.w_1.weight     |   409600   |\n",
      "|      bert.transformer_blocks.6.feed_forward.w_1.bias      |    1280    |\n",
      "|     bert.transformer_blocks.6.feed_forward.w_2.weight     |   409600   |\n",
      "|      bert.transformer_blocks.6.feed_forward.w_2.bias      |    320     |\n",
      "|     bert.transformer_blocks.6.input_sublayer.norm.a_2     |    320     |\n",
      "|     bert.transformer_blocks.6.input_sublayer.norm.b_2     |    320     |\n",
      "|     bert.transformer_blocks.6.output_sublayer.norm.a_2    |    320     |\n",
      "|     bert.transformer_blocks.6.output_sublayer.norm.b_2    |    320     |\n",
      "|    bert.transformer_blocks.7.attention.linears.0.weight   |   102400   |\n",
      "|     bert.transformer_blocks.7.attention.linears.0.bias    |    320     |\n",
      "|    bert.transformer_blocks.7.attention.linears.1.weight   |   102400   |\n",
      "|     bert.transformer_blocks.7.attention.linears.1.bias    |    320     |\n",
      "|    bert.transformer_blocks.7.attention.linears.2.weight   |   102400   |\n",
      "|     bert.transformer_blocks.7.attention.linears.2.bias    |    320     |\n",
      "|    bert.transformer_blocks.7.attention.linears.3.weight   |   102400   |\n",
      "|     bert.transformer_blocks.7.attention.linears.3.bias    |    320     |\n",
      "|  bert.transformer_blocks.7.attention.output_linear.weight |   102400   |\n",
      "|   bert.transformer_blocks.7.attention.output_linear.bias  |    320     |\n",
      "|     bert.transformer_blocks.7.feed_forward.w_1.weight     |   409600   |\n",
      "|      bert.transformer_blocks.7.feed_forward.w_1.bias      |    1280    |\n",
      "|     bert.transformer_blocks.7.feed_forward.w_2.weight     |   409600   |\n",
      "|      bert.transformer_blocks.7.feed_forward.w_2.bias      |    320     |\n",
      "|     bert.transformer_blocks.7.input_sublayer.norm.a_2     |    320     |\n",
      "|     bert.transformer_blocks.7.input_sublayer.norm.b_2     |    320     |\n",
      "|     bert.transformer_blocks.7.output_sublayer.norm.a_2    |    320     |\n",
      "|     bert.transformer_blocks.7.output_sublayer.norm.b_2    |    320     |\n",
      "|    bert.transformer_blocks.8.attention.linears.0.weight   |   102400   |\n",
      "|     bert.transformer_blocks.8.attention.linears.0.bias    |    320     |\n",
      "|    bert.transformer_blocks.8.attention.linears.1.weight   |   102400   |\n",
      "|     bert.transformer_blocks.8.attention.linears.1.bias    |    320     |\n",
      "|    bert.transformer_blocks.8.attention.linears.2.weight   |   102400   |\n",
      "|     bert.transformer_blocks.8.attention.linears.2.bias    |    320     |\n",
      "|    bert.transformer_blocks.8.attention.linears.3.weight   |   102400   |\n",
      "|     bert.transformer_blocks.8.attention.linears.3.bias    |    320     |\n",
      "|  bert.transformer_blocks.8.attention.output_linear.weight |   102400   |\n",
      "|   bert.transformer_blocks.8.attention.output_linear.bias  |    320     |\n",
      "|     bert.transformer_blocks.8.feed_forward.w_1.weight     |   409600   |\n",
      "|      bert.transformer_blocks.8.feed_forward.w_1.bias      |    1280    |\n",
      "|     bert.transformer_blocks.8.feed_forward.w_2.weight     |   409600   |\n",
      "|      bert.transformer_blocks.8.feed_forward.w_2.bias      |    320     |\n",
      "|     bert.transformer_blocks.8.input_sublayer.norm.a_2     |    320     |\n",
      "|     bert.transformer_blocks.8.input_sublayer.norm.b_2     |    320     |\n",
      "|     bert.transformer_blocks.8.output_sublayer.norm.a_2    |    320     |\n",
      "|     bert.transformer_blocks.8.output_sublayer.norm.b_2    |    320     |\n",
      "|    bert.transformer_blocks.9.attention.linears.0.weight   |   102400   |\n",
      "|     bert.transformer_blocks.9.attention.linears.0.bias    |    320     |\n",
      "|    bert.transformer_blocks.9.attention.linears.1.weight   |   102400   |\n",
      "|     bert.transformer_blocks.9.attention.linears.1.bias    |    320     |\n",
      "|    bert.transformer_blocks.9.attention.linears.2.weight   |   102400   |\n",
      "|     bert.transformer_blocks.9.attention.linears.2.bias    |    320     |\n",
      "|    bert.transformer_blocks.9.attention.linears.3.weight   |   102400   |\n",
      "|     bert.transformer_blocks.9.attention.linears.3.bias    |    320     |\n",
      "|  bert.transformer_blocks.9.attention.output_linear.weight |   102400   |\n",
      "|   bert.transformer_blocks.9.attention.output_linear.bias  |    320     |\n",
      "|     bert.transformer_blocks.9.feed_forward.w_1.weight     |   409600   |\n",
      "|      bert.transformer_blocks.9.feed_forward.w_1.bias      |    1280    |\n",
      "|     bert.transformer_blocks.9.feed_forward.w_2.weight     |   409600   |\n",
      "|      bert.transformer_blocks.9.feed_forward.w_2.bias      |    320     |\n",
      "|     bert.transformer_blocks.9.input_sublayer.norm.a_2     |    320     |\n",
      "|     bert.transformer_blocks.9.input_sublayer.norm.b_2     |    320     |\n",
      "|     bert.transformer_blocks.9.output_sublayer.norm.a_2    |    320     |\n",
      "|     bert.transformer_blocks.9.output_sublayer.norm.b_2    |    320     |\n",
      "|   bert.transformer_blocks.10.attention.linears.0.weight   |   102400   |\n",
      "|    bert.transformer_blocks.10.attention.linears.0.bias    |    320     |\n",
      "|   bert.transformer_blocks.10.attention.linears.1.weight   |   102400   |\n",
      "|    bert.transformer_blocks.10.attention.linears.1.bias    |    320     |\n",
      "|   bert.transformer_blocks.10.attention.linears.2.weight   |   102400   |\n",
      "|    bert.transformer_blocks.10.attention.linears.2.bias    |    320     |\n",
      "|   bert.transformer_blocks.10.attention.linears.3.weight   |   102400   |\n",
      "|    bert.transformer_blocks.10.attention.linears.3.bias    |    320     |\n",
      "| bert.transformer_blocks.10.attention.output_linear.weight |   102400   |\n",
      "|  bert.transformer_blocks.10.attention.output_linear.bias  |    320     |\n",
      "|     bert.transformer_blocks.10.feed_forward.w_1.weight    |   409600   |\n",
      "|      bert.transformer_blocks.10.feed_forward.w_1.bias     |    1280    |\n",
      "|     bert.transformer_blocks.10.feed_forward.w_2.weight    |   409600   |\n",
      "|      bert.transformer_blocks.10.feed_forward.w_2.bias     |    320     |\n",
      "|     bert.transformer_blocks.10.input_sublayer.norm.a_2    |    320     |\n",
      "|     bert.transformer_blocks.10.input_sublayer.norm.b_2    |    320     |\n",
      "|    bert.transformer_blocks.10.output_sublayer.norm.a_2    |    320     |\n",
      "|    bert.transformer_blocks.10.output_sublayer.norm.b_2    |    320     |\n",
      "|   bert.transformer_blocks.11.attention.linears.0.weight   |   102400   |\n",
      "|    bert.transformer_blocks.11.attention.linears.0.bias    |    320     |\n",
      "|   bert.transformer_blocks.11.attention.linears.1.weight   |   102400   |\n",
      "|    bert.transformer_blocks.11.attention.linears.1.bias    |    320     |\n",
      "|   bert.transformer_blocks.11.attention.linears.2.weight   |   102400   |\n",
      "|    bert.transformer_blocks.11.attention.linears.2.bias    |    320     |\n",
      "|   bert.transformer_blocks.11.attention.linears.3.weight   |   102400   |\n",
      "|    bert.transformer_blocks.11.attention.linears.3.bias    |    320     |\n",
      "| bert.transformer_blocks.11.attention.output_linear.weight |   102400   |\n",
      "|  bert.transformer_blocks.11.attention.output_linear.bias  |    320     |\n",
      "|     bert.transformer_blocks.11.feed_forward.w_1.weight    |   409600   |\n",
      "|      bert.transformer_blocks.11.feed_forward.w_1.bias     |    1280    |\n",
      "|     bert.transformer_blocks.11.feed_forward.w_2.weight    |   409600   |\n",
      "|      bert.transformer_blocks.11.feed_forward.w_2.bias     |    320     |\n",
      "|     bert.transformer_blocks.11.input_sublayer.norm.a_2    |    320     |\n",
      "|     bert.transformer_blocks.11.input_sublayer.norm.b_2    |    320     |\n",
      "|    bert.transformer_blocks.11.output_sublayer.norm.a_2    |    320     |\n",
      "|    bert.transformer_blocks.11.output_sublayer.norm.b_2    |    320     |\n",
      "|                     mlm.linear.weight                     |    8960    |\n",
      "|                      mlm.linear.bias                      |     28     |\n",
      "|                  blstm.lstm.weight_ih_l0                  |   409600   |\n",
      "|                  blstm.lstm.weight_hh_l0                  |   409600   |\n",
      "|                   blstm.lstm.bias_ih_l0                   |    1280    |\n",
      "|                   blstm.lstm.bias_hh_l0                   |    1280    |\n",
      "|              blstm.lstm.weight_ih_l0_reverse              |   409600   |\n",
      "|              blstm.lstm.weight_hh_l0_reverse              |   409600   |\n",
      "|               blstm.lstm.bias_ih_l0_reverse               |    1280    |\n",
      "|               blstm.lstm.bias_hh_l0_reverse               |    1280    |\n",
      "|                     blstm.fcn.0.weight                    |   204800   |\n",
      "|                      blstm.fcn.0.bias                     |    320     |\n",
      "|                      blstm.out.weight                     |    320     |\n",
      "|                       blstm.out.bias                      |     1      |\n",
      "+-----------------------------------------------------------+------------+\n",
      "Total Trainable Params: 17895069\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17895069"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERT input\n",
    "max_len = 280\n",
    "mask_prob = 0.15\n",
    "embedding_dim = 320 \n",
    "dropout = 0.1\n",
    "n_transformer_layers = 12\n",
    "n_attn_heads = 10\n",
    "tokenizer = ProteinTokenizer(max_len, mask_prob)\n",
    "bert = BERT(embedding_dim, dropout, max_len, mask_prob, n_transformer_layers, n_attn_heads)\n",
    "\n",
    "# BLSTM input\n",
    "lstm_input_size = 320\n",
    "lstm_hidden_size = 320\n",
    "lstm_num_layers = 1        \n",
    "lstm_bidrectional = True   \n",
    "fcn_hidden_size = 320\n",
    "blstm = BLSTM(lstm_input_size, lstm_hidden_size, lstm_num_layers, lstm_bidrectional, fcn_hidden_size)\n",
    "\n",
    "# BERT_BLSTM input\n",
    "vocab_size = len(token_to_index)\n",
    "model = BERT_BLSTM(bert, blstm, vocab_size)\n",
    "\n",
    "# Run\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------+------------+\n",
      "|                    Modules                    | Parameters |\n",
      "+-----------------------------------------------+------------+\n",
      "|       embeddings.word_embeddings.weight       |   10560    |\n",
      "|     embeddings.position_embeddings.weight     |   328320   |\n",
      "|  encoder.layer.0.attention.self.query.weight  |   102400   |\n",
      "|   encoder.layer.0.attention.self.query.bias   |    320     |\n",
      "|   encoder.layer.0.attention.self.key.weight   |   102400   |\n",
      "|    encoder.layer.0.attention.self.key.bias    |    320     |\n",
      "|  encoder.layer.0.attention.self.value.weight  |   102400   |\n",
      "|   encoder.layer.0.attention.self.value.bias   |    320     |\n",
      "| encoder.layer.0.attention.output.dense.weight |   102400   |\n",
      "|  encoder.layer.0.attention.output.dense.bias  |    320     |\n",
      "|   encoder.layer.0.attention.LayerNorm.weight  |    320     |\n",
      "|    encoder.layer.0.attention.LayerNorm.bias   |    320     |\n",
      "|   encoder.layer.0.intermediate.dense.weight   |   409600   |\n",
      "|    encoder.layer.0.intermediate.dense.bias    |    1280    |\n",
      "|      encoder.layer.0.output.dense.weight      |   409600   |\n",
      "|       encoder.layer.0.output.dense.bias       |    320     |\n",
      "|        encoder.layer.0.LayerNorm.weight       |    320     |\n",
      "|         encoder.layer.0.LayerNorm.bias        |    320     |\n",
      "|  encoder.layer.1.attention.self.query.weight  |   102400   |\n",
      "|   encoder.layer.1.attention.self.query.bias   |    320     |\n",
      "|   encoder.layer.1.attention.self.key.weight   |   102400   |\n",
      "|    encoder.layer.1.attention.self.key.bias    |    320     |\n",
      "|  encoder.layer.1.attention.self.value.weight  |   102400   |\n",
      "|   encoder.layer.1.attention.self.value.bias   |    320     |\n",
      "| encoder.layer.1.attention.output.dense.weight |   102400   |\n",
      "|  encoder.layer.1.attention.output.dense.bias  |    320     |\n",
      "|   encoder.layer.1.attention.LayerNorm.weight  |    320     |\n",
      "|    encoder.layer.1.attention.LayerNorm.bias   |    320     |\n",
      "|   encoder.layer.1.intermediate.dense.weight   |   409600   |\n",
      "|    encoder.layer.1.intermediate.dense.bias    |    1280    |\n",
      "|      encoder.layer.1.output.dense.weight      |   409600   |\n",
      "|       encoder.layer.1.output.dense.bias       |    320     |\n",
      "|        encoder.layer.1.LayerNorm.weight       |    320     |\n",
      "|         encoder.layer.1.LayerNorm.bias        |    320     |\n",
      "|  encoder.layer.2.attention.self.query.weight  |   102400   |\n",
      "|   encoder.layer.2.attention.self.query.bias   |    320     |\n",
      "|   encoder.layer.2.attention.self.key.weight   |   102400   |\n",
      "|    encoder.layer.2.attention.self.key.bias    |    320     |\n",
      "|  encoder.layer.2.attention.self.value.weight  |   102400   |\n",
      "|   encoder.layer.2.attention.self.value.bias   |    320     |\n",
      "| encoder.layer.2.attention.output.dense.weight |   102400   |\n",
      "|  encoder.layer.2.attention.output.dense.bias  |    320     |\n",
      "|   encoder.layer.2.attention.LayerNorm.weight  |    320     |\n",
      "|    encoder.layer.2.attention.LayerNorm.bias   |    320     |\n",
      "|   encoder.layer.2.intermediate.dense.weight   |   409600   |\n",
      "|    encoder.layer.2.intermediate.dense.bias    |    1280    |\n",
      "|      encoder.layer.2.output.dense.weight      |   409600   |\n",
      "|       encoder.layer.2.output.dense.bias       |    320     |\n",
      "|        encoder.layer.2.LayerNorm.weight       |    320     |\n",
      "|         encoder.layer.2.LayerNorm.bias        |    320     |\n",
      "|  encoder.layer.3.attention.self.query.weight  |   102400   |\n",
      "|   encoder.layer.3.attention.self.query.bias   |    320     |\n",
      "|   encoder.layer.3.attention.self.key.weight   |   102400   |\n",
      "|    encoder.layer.3.attention.self.key.bias    |    320     |\n",
      "|  encoder.layer.3.attention.self.value.weight  |   102400   |\n",
      "|   encoder.layer.3.attention.self.value.bias   |    320     |\n",
      "| encoder.layer.3.attention.output.dense.weight |   102400   |\n",
      "|  encoder.layer.3.attention.output.dense.bias  |    320     |\n",
      "|   encoder.layer.3.attention.LayerNorm.weight  |    320     |\n",
      "|    encoder.layer.3.attention.LayerNorm.bias   |    320     |\n",
      "|   encoder.layer.3.intermediate.dense.weight   |   409600   |\n",
      "|    encoder.layer.3.intermediate.dense.bias    |    1280    |\n",
      "|      encoder.layer.3.output.dense.weight      |   409600   |\n",
      "|       encoder.layer.3.output.dense.bias       |    320     |\n",
      "|        encoder.layer.3.LayerNorm.weight       |    320     |\n",
      "|         encoder.layer.3.LayerNorm.bias        |    320     |\n",
      "|  encoder.layer.4.attention.self.query.weight  |   102400   |\n",
      "|   encoder.layer.4.attention.self.query.bias   |    320     |\n",
      "|   encoder.layer.4.attention.self.key.weight   |   102400   |\n",
      "|    encoder.layer.4.attention.self.key.bias    |    320     |\n",
      "|  encoder.layer.4.attention.self.value.weight  |   102400   |\n",
      "|   encoder.layer.4.attention.self.value.bias   |    320     |\n",
      "| encoder.layer.4.attention.output.dense.weight |   102400   |\n",
      "|  encoder.layer.4.attention.output.dense.bias  |    320     |\n",
      "|   encoder.layer.4.attention.LayerNorm.weight  |    320     |\n",
      "|    encoder.layer.4.attention.LayerNorm.bias   |    320     |\n",
      "|   encoder.layer.4.intermediate.dense.weight   |   409600   |\n",
      "|    encoder.layer.4.intermediate.dense.bias    |    1280    |\n",
      "|      encoder.layer.4.output.dense.weight      |   409600   |\n",
      "|       encoder.layer.4.output.dense.bias       |    320     |\n",
      "|        encoder.layer.4.LayerNorm.weight       |    320     |\n",
      "|         encoder.layer.4.LayerNorm.bias        |    320     |\n",
      "|  encoder.layer.5.attention.self.query.weight  |   102400   |\n",
      "|   encoder.layer.5.attention.self.query.bias   |    320     |\n",
      "|   encoder.layer.5.attention.self.key.weight   |   102400   |\n",
      "|    encoder.layer.5.attention.self.key.bias    |    320     |\n",
      "|  encoder.layer.5.attention.self.value.weight  |   102400   |\n",
      "|   encoder.layer.5.attention.self.value.bias   |    320     |\n",
      "| encoder.layer.5.attention.output.dense.weight |   102400   |\n",
      "|  encoder.layer.5.attention.output.dense.bias  |    320     |\n",
      "|   encoder.layer.5.attention.LayerNorm.weight  |    320     |\n",
      "|    encoder.layer.5.attention.LayerNorm.bias   |    320     |\n",
      "|   encoder.layer.5.intermediate.dense.weight   |   409600   |\n",
      "|    encoder.layer.5.intermediate.dense.bias    |    1280    |\n",
      "|      encoder.layer.5.output.dense.weight      |   409600   |\n",
      "|       encoder.layer.5.output.dense.bias       |    320     |\n",
      "|        encoder.layer.5.LayerNorm.weight       |    320     |\n",
      "|         encoder.layer.5.LayerNorm.bias        |    320     |\n",
      "|      encoder.emb_layer_norm_after.weight      |    320     |\n",
      "|       encoder.emb_layer_norm_after.bias       |    320     |\n",
      "|              pooler.dense.weight              |   102400   |\n",
      "|               pooler.dense.bias               |    320     |\n",
      "|         contact_head.regression.weight        |    120     |\n",
      "|          contact_head.regression.bias         |     1      |\n",
      "+-----------------------------------------------+------------+\n",
      "Total Trainable Params: 7840121\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7840121"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esm = EsmModel.from_pretrained(\"facebook/esm2_t6_8M_UR50D\").to(device)\n",
    "count_parameters(esm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spike_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
