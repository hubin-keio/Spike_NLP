{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Runners\n",
    "We have several different models with runners that we utilized for our manuscript that I will cover in this notebook:\n",
    "- [BERT](#bert)\n",
    "    - [Non-Parallelized BERT](#non-parallelized-bert)\n",
    "    - [DistributedDataParallel (DDP) BERT](#distributeddataparallel-bert)  \n",
    "- [Fully Connected Network (FCN)](#fully-connected-network)\n",
    "- [GraphSAGE (GCN)](#graphsage) \n",
    "- [BLSTM](#blstm)\n",
    "- [BERT-BLSTM](#bert-blstm)\n",
    "    - [Initialized with RBD_Learned](#initialized-with-rbd_learned)\n",
    "    - [Initialized with ESM](#initialized-with-esm)\n",
    "- [ESM-BLSTM](#esm-blstm)\n",
    "\n",
    "For running our models, we utilized 1-4 NVIDIA a100 80 GB GPUs. It is recommended to utilize GPU over CPU due to improved performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tqdm\n",
    "import torch\n",
    "import textwrap\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from typing import Union\n",
    "from prettytable import PrettyTable\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pnlp.embedding.nlp_embedding import NLPEmbedding\n",
    "from pnlp.model.transformer import TransformerBlock\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is from a script `runner_util.py` that includes several \"utility\" functions that are called from the model runners.\n",
    "- `save_model`\n",
    "- `count_parameters`\n",
    "- `calc_train_test_history`\n",
    "- `plot_rmse_history`\n",
    "- `plot_run`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer: torch.optim.SGD, epoch: int, save_as: str):\n",
    "    \"\"\"\n",
    "    Save model parameters.\n",
    "\n",
    "    model: a model object\n",
    "    optimizer: model optimizer\n",
    "    epoch: number of epochs in the end of the model running\n",
    "    save_as: file name for saveing the model.\n",
    "    \"\"\"\n",
    "    torch.save({'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict()},\n",
    "                save_as)\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"\n",
    "    Count model parameters and print a summary\n",
    "\n",
    "    A nice hack from:\n",
    "    https://stackoverflow.com/a/62508086/1992369\n",
    "    \"\"\"\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\\n\")\n",
    "    return total_params\n",
    "\n",
    "def calc_train_test_history(metrics_csv: str, n_train: int, n_test: int, save_as: str):\n",
    "    \"\"\" Calculate the average mse per item and rmse \"\"\"\n",
    "\n",
    "    history_df = pd.read_csv(metrics_csv, sep=',', header=0)\n",
    "\n",
    "    history_df['train_loss_per'] = history_df['train_loss']/n_train  # average mse per item\n",
    "    history_df['test_loss_per'] = history_df['test_loss']/n_test\n",
    "\n",
    "    history_df['train_rmse'] = np.sqrt(history_df['train_loss_per'].values)  # rmse\n",
    "    history_df['test_rmse'] = np.sqrt(history_df['test_loss_per'].values)\n",
    "\n",
    "    history_df.to_csv(metrics_csv.replace('.csv', '_per.csv'), index=False)\n",
    "    plot_rmse_history(history_df, save_as)\n",
    "\n",
    "def plot_rmse_history(history_df, save_as: str):\n",
    "    \"\"\" Plot RMSE training and testing history per epoch. \"\"\"\n",
    "\n",
    "    sns.set_theme()\n",
    "    sns.set_context('talk')\n",
    "    sns.set(style=\"darkgrid\")\n",
    "\n",
    "    # Converting mm to inches for figsize\n",
    "    width_in = 88/25.4 # mm to inches\n",
    "    ratio = 16/9\n",
    "    height_in = width_in/ratio \n",
    "    fig, ax = plt.subplots(figsize=(width_in, height_in))\n",
    "    plt.rcParams['font.family'] = 'sans-serif'\n",
    "\n",
    "    # Plot\n",
    "    sns.lineplot(data=history_df, x=history_df.index, y='test_rmse', color='tab:blue', linewidth=0.5, ax=ax) # add label='Test RMSE' for legend\n",
    "    sns.lineplot(data=history_df, x=history_df.index, y='train_rmse', color='tab:orange', linewidth=0.5,ax=ax) # add label='Train RMSE' for legend\n",
    "    \n",
    "    # Set the font size\n",
    "    font_size = 8\n",
    "    ax.set_xlabel('Epoch', fontsize=font_size)\n",
    "    ax.set_ylabel(f'RMSE', fontsize=font_size)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=font_size)\n",
    "    # ax.legend(fontsize=font_size)\n",
    "\n",
    "    # Skipping every other x-axis tick mark\n",
    "    #ax.set_ylim(-0.1, 1.8)\n",
    "\n",
    "    ax_xticks = ax.get_xticks()\n",
    "    new_xlabels = ['' if i % 2 else label for i, label in enumerate(ax.get_xticklabels())]\n",
    "    ax.set_xticks(ax_xticks)\n",
    "    ax.set_xticklabels(new_xlabels)\n",
    "    #ax.set_xlim(-100, 5000)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_as + '_rmse.pdf', format='pdf')\n",
    "\n",
    "def plot_run(csv_name: str, save: bool = True):\n",
    "    '''\n",
    "    Generate a single figure with subplots for training loss and training accuracy\n",
    "    from the model run csv file.\n",
    "\n",
    "    For runner.py and gpu_ddp_runner.py\n",
    "    '''\n",
    "    df = pd.read_csv(csv_name)\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    sns.set_theme()\n",
    "    sns.set_context('talk')\n",
    "    sns.set(style=\"darkgrid\")\n",
    "\n",
    "    # Converting mm to inches for figsize\n",
    "    width_in = 88/25.4 # mm to inches\n",
    "    ratio = 16/9\n",
    "    height_in = width_in/ratio \n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(width_in, 2*height_in))\n",
    "    plt.rcParams['font.family'] = 'sans-serif'\n",
    "\n",
    "    # Set the font size\n",
    "    font_size = 8\n",
    "\n",
    "    # Plot Training Loss\n",
    "    train_loss_line = ax1.plot(df['epoch'], df['train_loss'], color='red', linewidth=0.5, label='Train Loss')\n",
    "    test_loss_line = ax1.plot(df['epoch'], df['test_loss'], color='orange', linewidth=0.5, label='Test Loss')\n",
    "    ax1.set_ylabel('Loss', fontsize=font_size)\n",
    "    ax1.legend(loc='upper right', fontsize=font_size)\n",
    "    ax1.tick_params(axis='both', which='major', labelsize=font_size)\n",
    "    ax1.yaxis.get_offset_text().set_fontsize(font_size) \n",
    "    ax1.set_ylim(-0.5e6, 8e6) \n",
    "\n",
    "    # Plot Training Accuracy\n",
    "    train_accuracy_line = ax2.plot(df['epoch'], df['train_accuracy']*100, color='blue', linewidth=0.5, label='Train Accuracy')\n",
    "    test_accuracy_line = ax2.plot(df['epoch'], df['test_accuracy']*100, color='green', linewidth=0.5, label='Test Accuracy')\n",
    "    ax2.set_xlabel('Epoch', fontsize=font_size)\n",
    "    ax2.set_ylabel('Accuracy', fontsize=font_size)\n",
    "    ax2.set_ylim(0, 100) \n",
    "    ax2.legend(loc='lower right', fontsize=font_size)\n",
    "    ax2.tick_params(axis='both', which='major', labelsize=font_size)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save:\n",
    "        combined_fname = csv_name.replace('.csv', '_loss_acc.pdf')\n",
    "        plt.savefig(combined_fname, format='pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" BERT Model. \"\"\"\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    \"\"\" BERT model. \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 embedding_dim: int,\n",
    "                 dropout: float,\n",
    "                 max_len: int,\n",
    "                 mask_prob: float,\n",
    "                 # hidden: int,\n",
    "                 n_transformer_layers: int,\n",
    "                 attn_heads: int):\n",
    "        \"\"\"\n",
    "        embedding_dim: dimensions of embedding\n",
    "        hidden: BERT model size (used as input size and hidden size)\n",
    "        n_layers: number of Transformer layers\n",
    "        attn_heads: attenion heads\n",
    "        dropout: dropout ratio\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dropout = dropout\n",
    "        self.max_len = max_len\n",
    "        self.mask_prob = mask_prob\n",
    "\n",
    "        self.hidden  = embedding_dim\n",
    "        self.n_transformer_layers = n_transformer_layers\n",
    "        self.attn_heads = attn_heads\n",
    "        self.feed_forward_hidden = self.hidden * 4         # 4 * hidden_size for FFN\n",
    "\n",
    "        self.embedding = NLPEmbedding(self.embedding_dim, self.max_len, self.dropout)\n",
    "\n",
    "        def clones(module, n):\n",
    "            \"\"\"Produce N identical layers\"\"\"\n",
    "            return nn.ModuleList([copy.deepcopy(module) for _ in range(n)])\n",
    "\n",
    "        self.transformer_blocks = clones(TransformerBlock(self.hidden, self.attn_heads, self.feed_forward_hidden, self.dropout), self.n_transformer_layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        embedded_seqs, mask_tensor = self.embedding(x)   # tokenized batch sequences\n",
    "\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer.forward(embedded_seqs, mask_tensor)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Parallelized BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DistributedDataParallel BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Fully Connected Network\n",
    "This section will cover the model runner found in `fcn_model_runner.py`. First, let's take a look at the fully connected network (FCN) model found in `fcn.py`, which we run using our model runner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class FCN(nn.Module):\n",
    "    \"\"\" Fully Connected Network \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 fcn_input_size,     # The number of input features\n",
    "                 fcn_hidden_size,    # The number of features in hidden layer of FCN.\n",
    "                 device):            # Device ('cpu' or 'cuda')\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # FCN layers\n",
    "        self.fcn = nn.Sequential(nn.Linear(fcn_input_size, fcn_hidden_size),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(fcn_hidden_size, 1))  # Adjust this line based on the required output size\n",
    "\n",
    "    def forward(self, x):\n",
    "        fcn_out = self.fcn(x)\n",
    "        fcn_final_out = fcn_out[:, -1, :]\n",
    "        prediction = fcn_final_out.to(self.device)\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tqdm\n",
    "import torch\n",
    "import pickle\n",
    "import datetime\n",
    "from typing import Union\n",
    "from collections import defaultdict\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#from runner_util import save_model, count_parameters, calc_train_test_history\n",
    "\n",
    "class EmbeddedDMSDataset(Dataset):\n",
    "    \"\"\" Binding or Expression DMS Dataset \"\"\"\n",
    "    \n",
    "    def __init__(self, pickle_file:str):\n",
    "        \"\"\"\n",
    "        Load from pickle file:\n",
    "        - sequence label (seq_id), \n",
    "        - binding or expression numerical target (log10Ka or ML_meanF), and \n",
    "        - embeddings\n",
    "        \"\"\"\n",
    "        with open(pickle_file, 'rb') as f:\n",
    "            dms_list = pickle.load(f)\n",
    "        \n",
    "            self.labels = [entry['seq_id'] for entry in dms_list]\n",
    "            self.numerical = [entry[\"log10Ka\" if \"binding\" in pickle_file else \"ML_meanF\"] for entry in dms_list]\n",
    "            self.embeddings = [entry['embedding'] for entry in dms_list]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # label, feature, target\n",
    "        return self.labels[idx], self.embeddings[idx], self.numerical[idx]\n",
    "\n",
    "def run_model(model, train_set, test_set, n_epochs: int, batch_size: int, lr:float, max_batch: Union[int, None], device: str, save_as: str):\n",
    "    \"\"\" Run a model through train and test epochs. \"\"\"\n",
    "    \n",
    "    if not max_batch:\n",
    "        max_batch = len(train_set)\n",
    "\n",
    "    model = model.to(device)\n",
    "    loss_fn = nn.MSELoss(reduction='sum').to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_set, batch_size, shuffle=True)\n",
    "    \n",
    "    metrics_csv = save_as + \"_metrics.csv\"\n",
    "\n",
    "    with open(metrics_csv, \"w\") as fh:\n",
    "        fh.write(f\"epoch,\"\n",
    "                 f\"train_loss,test_loss\\n\")\n",
    "\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            train_loss = epoch_iteration(model, loss_fn, optimizer, train_loader, epoch, max_batch, device, mode='train')\n",
    "            test_loss = epoch_iteration(model, loss_fn, optimizer, test_loader, epoch, max_batch, device, mode='test')\n",
    "\n",
    "            print(f'Epoch {epoch} | Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\\n')\n",
    "           \n",
    "            fh.write(f\"{epoch},\"\n",
    "                     f\"{train_loss},{test_loss}\\n\")\n",
    "            fh.flush()\n",
    "                \n",
    "            save_model(model, optimizer, epoch, save_as + '.model_save')\n",
    "\n",
    "    return metrics_csv\n",
    "\n",
    "def epoch_iteration(model, loss_fn, optimizer, data_loader, num_epochs: int, max_batch: int, device: str, mode: str):\n",
    "    \"\"\" Used in run_model. \"\"\"\n",
    "    \n",
    "    model.train() if mode=='train' else model.eval()\n",
    "\n",
    "    data_iter = tqdm.tqdm(enumerate(data_loader),\n",
    "                          desc=f'Epoch_{mode}: {num_epochs}',\n",
    "                          total=len(data_loader),\n",
    "                          bar_format='{l_bar}{r_bar}')\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch, batch_data in data_iter:\n",
    "        if max_batch > 0 and batch >= max_batch:\n",
    "            break \n",
    "        \n",
    "        label, feature, target = batch_data\n",
    "        feature, target = feature.to(device), target.to(device) \n",
    "        target = target.float()\n",
    "\n",
    "        if mode == 'train':\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(feature).flatten()\n",
    "            batch_loss = loss_fn(pred, target)\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                pred = model(feature).flatten()\n",
    "                batch_loss = loss_fn(pred, target)\n",
    "                \n",
    "        total_loss += batch_loss.item()\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "EmbeddedDMSDataset.__init__() missing 1 required positional argument: 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 21\u001b[0m\n\u001b[1;32m     16\u001b[0m embedded_test_pkl \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdms_mutation_binding_Kds_test_esm_embedded.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# embedded_train_pkl = os.path.join(data_dir, 'dms_mutation_expression_meanFs_train_rbd_learned_embedded_320.pkl') # fcn-rbd_learned_320_dms_expression\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# embedded_test_pkl = os.path.join(data_dir, 'dms_mutation_expression_meanFs_test_rbd_learned_embedded_320.pkl')\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# embedded_train_pkl = os.path.join(data_dir, 'dms_mutation_binding_Kds_train_rbd_learned_embedded_320.pkl') # fcn-rbd_learned_320_dms_binding\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# embedded_test_pkl = os.path.join(data_dir, 'dms_mutation_binding_Kds_test_rbd_learned_embedded_320.pkl')\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mEmbeddedDMSDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded_train_pkl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m EmbeddedDMSDataset(embedded_test_pkl)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Run setup\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: EmbeddedDMSDataset.__init__() missing 1 required positional argument: 'device'"
     ]
    }
   ],
   "source": [
    "# Data/results directories\n",
    "result_tag = 'fcn-esm_dms_binding' # specify rbd_learned or esm, and expression or binding\n",
    "data_dir = '../data/pickles'\n",
    "results_dir = '../results/run_results/fcn'\n",
    "\n",
    "# Create run directory for results\n",
    "now = datetime.datetime.now()\n",
    "date_hour_minute = now.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "run_dir = os.path.join(results_dir, f\"{result_tag}-{date_hour_minute}\")\n",
    "os.makedirs(run_dir, exist_ok = True)\n",
    "\n",
    "# Load in data\n",
    "# embedded_train_pkl = os.path.join(data_dir, 'dms_mutation_expression_meanFs_train_esm_embedded.pkl') # fcn-esm_dms_expression\n",
    "# embedded_test_pkl = os.path.join(data_dir, 'dms_mutation_expression_meanFs_test_esm_embedded.pkl')\n",
    "embedded_train_pkl = os.path.join(data_dir, 'dms_mutation_binding_Kds_train_esm_embedded.pkl') # fcn-esm_dms_binding\n",
    "embedded_test_pkl = os.path.join(data_dir, 'dms_mutation_binding_Kds_test_esm_embedded.pkl')\n",
    "# embedded_train_pkl = os.path.join(data_dir, 'dms_mutation_expression_meanFs_train_rbd_learned_embedded_320.pkl') # fcn-rbd_learned_320_dms_expression\n",
    "# embedded_test_pkl = os.path.join(data_dir, 'dms_mutation_expression_meanFs_test_rbd_learned_embedded_320.pkl')\n",
    "# embedded_train_pkl = os.path.join(data_dir, 'dms_mutation_binding_Kds_train_rbd_learned_embedded_320.pkl') # fcn-rbd_learned_320_dms_binding\n",
    "# embedded_test_pkl = os.path.join(data_dir, 'dms_mutation_binding_Kds_test_rbd_learned_embedded_320.pkl')\n",
    "train_dataset = EmbeddedDMSDataset(embedded_train_pkl)\n",
    "test_dataset = EmbeddedDMSDataset(embedded_test_pkl)\n",
    "\n",
    "# Run setup\n",
    "n_epochs = 10\n",
    "batch_size = 32\n",
    "max_batch = 10\n",
    "lr = 1e-5\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# FCN input\n",
    "fcn_input_size = train_dataset.embeddings[0].size(1)   \n",
    "fcn_hidden_size = fcn_input_size\n",
    "model = FCN(fcn_input_size, fcn_hidden_size, device)\n",
    "\n",
    "# Run\n",
    "count_parameters(model)\n",
    "model_result = os.path.join(run_dir, f\"{result_tag}-{date_hour_minute}_train_{len(train_dataset)}_test_{len(test_dataset)}\")\n",
    "metrics_csv = run_model(model, train_dataset, test_dataset, n_epochs, batch_size, lr, max_batch, device, model_result)\n",
    "calc_train_test_history(metrics_csv, len(train_dataset), len(test_dataset), model_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## GraphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" GraphSAGE model. \"\"\"\n",
    "\n",
    "from torch import nn\n",
    "from torch_geometric.nn import SAGEConv, global_mean_pool\n",
    "\n",
    "class GraphSAGE(nn.Module):\n",
    "    \"\"\" GraphSAGE. \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, 16)\n",
    "        self.conv2 = SAGEConv(16, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tqdm\n",
    "import torch\n",
    "import pickle\n",
    "import datetime\n",
    "from typing import Union\n",
    "from collections import defaultdict\n",
    "from torch import nn\n",
    "from torch_geometric.nn import SAGEConv, global_mean_pool\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "#from runner_util import save_model, count_parameters, calc_train_test_history\n",
    "\n",
    "class EmbeddedDMSDataset(Dataset):\n",
    "    \"\"\" Binding or Expression DMS Dataset \"\"\"\n",
    "    \n",
    "    def __init__(self, pickle_file:str, device:str):\n",
    "        \"\"\"\n",
    "        Load from pickle file:\n",
    "        - sequence label (seq_id), \n",
    "        - binding or expression numerical target (log10Ka or ML_meanF), and \n",
    "        - embeddings\n",
    "        \"\"\"\n",
    "        with open(pickle_file, 'rb') as f:\n",
    "            dms_list = pickle.load(f)\n",
    "        \n",
    "            self.labels = [entry['seq_id'] for entry in dms_list]\n",
    "            self.numerical = [entry[\"log10Ka\" if \"binding\" in pickle_file else \"ML_meanF\"] for entry in dms_list]\n",
    "            self.embeddings = [entry['embedding'] for entry in dms_list]\n",
    " \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert to pytorch geometric graph\n",
    "        embedding = self.embeddings[idx].to(device) \n",
    "        edges = [(i, i+1) for i in range(embedding.size(0) - 1)]\n",
    "        edge_index = torch.tensor(edges, dtype=torch.int64).t().contiguous()\n",
    "        y = torch.tensor([self.numerical[idx]], dtype=torch.float32).view(-1, 1)\n",
    "        \n",
    "        return Data(x=embedding, edge_index=edge_index, y=y)\n",
    "\n",
    "def run_model(model, train_set, test_set, n_epochs: int, batch_size: int, lr:float, max_batch: Union[int, None], device: str, save_as: str):\n",
    "    \"\"\" Run a model through train and test epochs. \"\"\"\n",
    "    \n",
    "    if not max_batch:\n",
    "        max_batch = len(train_set)\n",
    "\n",
    "    model = model.to(device)\n",
    "    loss_fn = nn.MSELoss(reduction='sum').to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_set, batch_size, shuffle=True)\n",
    "    \n",
    "    metrics_csv = save_as + \"_metrics.csv\"\n",
    "\n",
    "    with open(metrics_csv, \"w\") as fh:\n",
    "        fh.write(f\"epoch,\"\n",
    "                 f\"train_loss,test_loss\\n\")\n",
    "\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            train_loss = epoch_iteration(model, loss_fn, optimizer, train_loader, epoch, max_batch, device, mode='train')\n",
    "            test_loss = epoch_iteration(model, loss_fn, optimizer, test_loader, epoch, max_batch, device, mode='test')\n",
    "\n",
    "            print(f'Epoch {epoch} | Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\\n')\n",
    "           \n",
    "            fh.write(f\"{epoch},\"\n",
    "                     f\"{train_loss},{test_loss}\\n\")\n",
    "            fh.flush()\n",
    "                \n",
    "            save_model(model, optimizer, epoch, save_as + '.model_save')\n",
    "\n",
    "    return metrics_csv\n",
    "\n",
    "def epoch_iteration(model, loss_fn, optimizer, data_loader, num_epochs: int, max_batch: int, device: str, mode: str):\n",
    "    \"\"\" Used in run_model. \"\"\"\n",
    "\n",
    "    model.train() if mode=='train' else model.eval()\n",
    "\n",
    "    data_iter = tqdm.tqdm(enumerate(data_loader),\n",
    "                          desc=f'Epoch_{mode}: {num_epochs}',\n",
    "                          total=len(data_loader),\n",
    "                          bar_format='{l_bar}{r_bar}')\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch, batch_data in data_iter:\n",
    "        if max_batch > 0 and batch >= max_batch:\n",
    "            break\n",
    "\n",
    "        batch_data = batch_data.to(device)\n",
    "\n",
    "        if mode == 'train':\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(batch_data.x, batch_data.edge_index, batch_data.batch)\n",
    "            batch_loss = loss_fn(pred, batch_data.y)\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                pred = model(batch_data.x, batch_data.edge_index, batch_data.batch)\n",
    "                batch_loss = loss_fn(pred, batch_data.y)\n",
    "\n",
    "        total_loss += batch_loss.item()\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|      Modules       | Parameters |\n",
      "+--------------------+------------+\n",
      "| conv1.lin_l.weight |    5120    |\n",
      "|  conv1.lin_l.bias  |     16     |\n",
      "| conv1.lin_r.weight |    5120    |\n",
      "| conv2.lin_l.weight |     16     |\n",
      "|  conv2.lin_l.bias  |     1      |\n",
      "| conv2.lin_r.weight |     16     |\n",
      "+--------------------+------------+\n",
      "Total Trainable Params: 10289\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_train: 1:   0%|| 10/2639 [00:00<00:43, 60.44it/s]\n",
      "Epoch_test: 1:   2%|| 10/660 [00:00<00:04, 155.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 20681.9192, Test Loss: 18543.7797\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_train: 2:   0%|| 10/2639 [00:00<00:19, 131.54it/s]\n",
      "Epoch_test: 2:   2%|| 10/660 [00:00<00:04, 156.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 16591.7802, Test Loss: 13549.2529\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_train: 3:   0%|| 10/2639 [00:00<00:20, 129.79it/s]\n",
      "Epoch_test: 3:   2%|| 10/660 [00:00<00:04, 156.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 9697.9343, Test Loss: 6644.0107\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_train: 4:   0%|| 10/2639 [00:00<00:20, 129.78it/s]\n",
      "Epoch_test: 4:   2%|| 10/660 [00:00<00:04, 157.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 4004.3677, Test Loss: 2296.3543\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_train: 5:   0%|| 10/2639 [00:00<00:20, 130.48it/s]\n",
      "Epoch_test: 5:   2%|| 10/660 [00:00<00:04, 156.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 1697.1881, Test Loss: 1258.4189\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_train: 6:   0%|| 10/2639 [00:00<00:20, 128.03it/s]\n",
      "Epoch_test: 6:   2%|| 10/660 [00:00<00:04, 156.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss: 1290.3947, Test Loss: 1206.8367\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_train: 7:   0%|| 10/2639 [00:00<00:20, 129.96it/s]\n",
      "Epoch_test: 7:   2%|| 10/660 [00:00<00:04, 154.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss: 1125.0086, Test Loss: 1217.6553\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_train: 8:   0%|| 10/2639 [00:00<00:20, 129.20it/s]\n",
      "Epoch_test: 8:   2%|| 10/660 [00:00<00:04, 156.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss: 1144.1322, Test Loss: 1108.9380\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_train: 9:   0%|| 10/2639 [00:00<00:20, 130.81it/s]\n",
      "Epoch_test: 9:   2%|| 10/660 [00:00<00:04, 155.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train Loss: 1112.6130, Test Loss: 1077.0939\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_train: 10:   0%|| 10/2639 [00:00<00:20, 129.73it/s]\n",
      "Epoch_test: 10:   2%|| 10/660 [00:00<00:04, 156.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 1209.6979, Test Loss: 1165.5435\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 346.457x194.882 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data/results directories\n",
    "result_tag = 'graphsage-esm_dms_binding' # specify rbd_learned or esm, and expression or binding\n",
    "data_dir = '../data/pickles'\n",
    "results_dir = '../results/run_results/graphsage'\n",
    "\n",
    "# Create run directory for results\n",
    "now = datetime.datetime.now()\n",
    "date_hour_minute = now.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "run_dir = os.path.join(results_dir, f\"{result_tag}-{date_hour_minute}\")\n",
    "os.makedirs(run_dir, exist_ok = True)\n",
    "\n",
    "# Load in data\n",
    "# embedded_train_pkl = os.path.join(data_dir, 'dms_mutation_expression_meanFs_train_esm_embedded.pkl') # graphsage-esm_dms_expression\n",
    "# embedded_test_pkl = os.path.join(data_dir, 'dms_mutation_expression_meanFs_test_esm_embedded.pkl')\n",
    "embedded_train_pkl = os.path.join(data_dir, 'dms_mutation_binding_Kds_train_esm_embedded.pkl') # graphsage-esm_dms_binding\n",
    "embedded_test_pkl = os.path.join(data_dir, 'dms_mutation_binding_Kds_test_esm_embedded.pkl')\n",
    "# embedded_train_pkl = os.path.join(data_dir, 'dms_mutation_expression_meanFs_train_rbd_learned_embedded_320.pkl') # graphsage-rbd_learned_320_dms_expression\n",
    "# embedded_test_pkl = os.path.join(data_dir, 'dms_mutation_expression_meanFs_test_rbd_learned_embedded_320.pkl')\n",
    "# embedded_train_pkl = os.path.join(data_dir, 'dms_mutation_binding_Kds_train_rbd_learned_embedded_320.pkl') # graphsage-rbd_learned_320_dms_binding\n",
    "# embedded_test_pkl = os.path.join(data_dir, 'dms_mutation_binding_Kds_test_rbd_learned_embedded_320.pkl')\n",
    "\n",
    "device = torch.device(\"cuda:3\")\n",
    "train_dataset = EmbeddedDMSDataset(embedded_train_pkl, device)\n",
    "test_dataset = EmbeddedDMSDataset(embedded_test_pkl, device)\n",
    "\n",
    "# Run setup\n",
    "n_epochs = 10\n",
    "batch_size = 32\n",
    "max_batch = 10\n",
    "lr = 1e-5\n",
    "\n",
    "# GraphSAGE input\n",
    "input_channels = train_dataset.embeddings[0].size(1) # number of input channels (dimensions of the embeddings)\n",
    "out_channels = 1  # For regression output\n",
    "model = GraphSAGE(input_channels, out_channels).to(device)\n",
    "\n",
    "# Run\n",
    "count_parameters(model)\n",
    "model_result = os.path.join(run_dir, f\"{result_tag}-{date_hour_minute}_train_{len(train_dataset)}_test_{len(test_dataset)}\")\n",
    "metrics_csv = run_model(model, train_dataset, test_dataset, n_epochs, batch_size, lr, max_batch, device, model_result)\n",
    "calc_train_test_history(metrics_csv, len(train_dataset), len(test_dataset), model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## BLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" BLSTM model with FCN layer. \"\"\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class BLSTM(nn.Module):\n",
    "    \"\"\" Bidirectional LSTM with FCN layer. \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 lstm_input_size,    # The number of expected features.\n",
    "                 lstm_hidden_size,   # The number of features in hidden state h.\n",
    "                 lstm_num_layers,    # Number of recurrent layers in LSTM.\n",
    "                 lstm_bidirectional, # Bidrectional LSTM.\n",
    "                 fcn_hidden_size):   # The number of features in hidden layer of CN.\n",
    "        super().__init__()\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=lstm_input_size,\n",
    "                            hidden_size=lstm_hidden_size,\n",
    "                            num_layers=lstm_num_layers,\n",
    "                            bidirectional=lstm_bidirectional,\n",
    "                            batch_first=True)           \n",
    "\n",
    "        # FCN\n",
    "        if lstm_bidirectional:\n",
    "            self.fcn = nn.Sequential(nn.Linear(2 * lstm_hidden_size, fcn_hidden_size),\n",
    "                                     nn.ReLU())\n",
    "        else:\n",
    "            self.fcn = nn.Sequential(nn.Linear(lstm_hidden_size, fcn_hidden_size),\n",
    "                                     nn.ReLU())\n",
    "\n",
    "        # FCN output layer\n",
    "        self.out = nn.Linear(fcn_hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        num_directions = 2 if self.lstm.bidirectional else 1\n",
    "        h_0 = torch.zeros(num_directions * self.lstm.num_layers, x.size(0), self.lstm.hidden_size, device=x.device)\n",
    "        c_0 = torch.zeros(num_directions * self.lstm.num_layers, x.size(0), self.lstm.hidden_size, device=x.device)\n",
    "\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x, (h_0, c_0))\n",
    "        h_n.detach()\n",
    "        c_n.detach()\n",
    "        lstm_final_out = lstm_out[:, -1, :]\n",
    "        fcn_out = self.fcn(lstm_final_out)\n",
    "        prediction = self.out(fcn_out)\n",
    "\n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tqdm\n",
    "import torch\n",
    "import pickle\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Union\n",
    "from collections import defaultdict\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#from runner_util import save_model, count_parameters, calc_train_test_history\n",
    "from transformers import AutoTokenizer, EsmModel \n",
    "from pnlp.embedding.tokenizer import ProteinTokenizer, token_to_index\n",
    "from pnlp.model.language import BERT\n",
    "\n",
    "\n",
    "class EmbeddedDMSDataset(Dataset):\n",
    "    \"\"\" Binding or Expression DMS Dataset \"\"\"\n",
    "    \n",
    "    def __init__(self, pickle_file:str, device:str):\n",
    "        \"\"\"\n",
    "        Load from pickle file:\n",
    "        - sequence label (seq_id), \n",
    "        - binding or expression numerical target (log10Ka or ML_meanF), and \n",
    "        - embeddings\n",
    "        \"\"\"\n",
    "        with open(pickle_file, 'rb') as f:\n",
    "            dms_list = pickle.load(f)\n",
    "        \n",
    "            self.labels = [entry['seq_id'] for entry in dms_list]\n",
    "            self.numerical = [entry[\"log10Ka\" if \"binding\" in pickle_file else \"ML_meanF\"] for entry in dms_list]\n",
    "            self.embeddings = [entry['embedding'] for entry in dms_list]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # label, feature, target\n",
    "        return self.labels[idx], self.embeddings[idx].to(device), self.numerical[idx]\n",
    "\n",
    "def run_model(model, train_set, test_set, n_epochs: int, batch_size: int, lr:float, max_batch: Union[int, None], device: str, save_as: str):\n",
    "    \"\"\" Run a model through train and test epochs. \"\"\"\n",
    "    \n",
    "    if not max_batch:\n",
    "        max_batch = len(train_set)\n",
    "\n",
    "    model = model.to(device)\n",
    "    loss_fn = nn.MSELoss(reduction='sum').to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_set, batch_size, shuffle=True)\n",
    "    \n",
    "    metrics_csv = save_as + \"_metrics.csv\"\n",
    "\n",
    "    with open(metrics_csv, \"w\") as fh:\n",
    "        fh.write(f\"epoch,\"\n",
    "                 f\"train_loss,test_loss\\n\")\n",
    "\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            train_loss = epoch_iteration(model, loss_fn, optimizer, train_loader, epoch, max_batch, device, mode='train')\n",
    "            test_loss = epoch_iteration(model, loss_fn, optimizer, test_loader, epoch, max_batch, device, mode='test')\n",
    "\n",
    "            print(f'Epoch {epoch} | Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\\n')\n",
    "           \n",
    "            fh.write(f\"{epoch},\"\n",
    "                     f\"{train_loss},{test_loss}\\n\")\n",
    "            fh.flush()\n",
    "                \n",
    "            save_model(model, optimizer, epoch, save_as + '.model_save')\n",
    "\n",
    "    return metrics_csv\n",
    "\n",
    "def epoch_iteration(model, loss_fn, optimizer, data_loader, num_epochs: int, max_batch: int, device: str, mode: str):\n",
    "    \"\"\" Used in run_model. \"\"\"\n",
    "    \n",
    "    model.train() if mode=='train' else model.eval()\n",
    "\n",
    "    data_iter = tqdm.tqdm(enumerate(data_loader),\n",
    "                          desc=f'Epoch_{mode}: {num_epochs}',\n",
    "                          total=len(data_loader),\n",
    "                          bar_format='{l_bar}{r_bar}')\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch, batch_data in data_iter:\n",
    "        if max_batch > 0 and batch >= max_batch:\n",
    "            break \n",
    "        \n",
    "        label, feature, target = batch_data\n",
    "        feature, target = feature.to(device), target.to(device) \n",
    "        target = target.float()\n",
    "\n",
    "        if mode == 'train':\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(feature).flatten()\n",
    "            batch_loss = loss_fn(pred, target)\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                pred = model(feature).flatten()\n",
    "                batch_loss = loss_fn(pred, target)\n",
    "                \n",
    "        total_loss += batch_loss.item()\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+------------+\n",
      "|          Modules          | Parameters |\n",
      "+---------------------------+------------+\n",
      "|     lstm.weight_ih_l0     |   409600   |\n",
      "|     lstm.weight_hh_l0     |   409600   |\n",
      "|      lstm.bias_ih_l0      |    1280    |\n",
      "|      lstm.bias_hh_l0      |    1280    |\n",
      "| lstm.weight_ih_l0_reverse |   409600   |\n",
      "| lstm.weight_hh_l0_reverse |   409600   |\n",
      "|  lstm.bias_ih_l0_reverse  |    1280    |\n",
      "|  lstm.bias_hh_l0_reverse  |    1280    |\n",
      "|        fcn.0.weight       |   204800   |\n",
      "|         fcn.0.bias        |    320     |\n",
      "|         out.weight        |    320     |\n",
      "|          out.bias         |     1      |\n",
      "+---------------------------+------------+\n",
      "Total Trainable Params: 1848961\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_train: 1:   0%|| 10/2639 [00:00<00:32, 80.23it/s]\n",
      "Epoch_test: 1:   2%|| 10/660 [00:00<00:02, 278.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 20861.7759, Test Loss: 19848.5503\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_train: 2:   0%|| 10/2639 [00:00<00:24, 108.01it/s]\n",
      "Epoch_test: 2:   2%|| 10/660 [00:00<00:02, 311.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 20371.5575, Test Loss: 19515.6237\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_train: 3:   0%|| 10/2639 [00:00<00:23, 112.31it/s]\n",
      "Epoch_test: 3:   2%|| 10/660 [00:00<00:02, 310.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 20165.5474, Test Loss: 19689.0728\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_train: 4:   0%|| 10/2639 [00:00<00:23, 112.43it/s]\n",
      "Epoch_test: 4:   2%|| 10/660 [00:00<00:02, 306.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 18256.6000, Test Loss: 18978.3434\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_train: 5:   0%|| 10/2639 [00:00<00:23, 112.66it/s]\n",
      "Epoch_test: 5:   2%|| 10/660 [00:00<00:02, 312.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 16895.8206, Test Loss: 17339.1030\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_train: 6:   0%|| 10/2639 [00:00<00:23, 113.80it/s]\n",
      "Epoch_test: 6:   2%|| 10/660 [00:00<00:02, 310.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss: 16995.4865, Test Loss: 16734.6075\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_train: 7:   0%|| 10/2639 [00:00<00:23, 114.14it/s]\n",
      "Epoch_test: 7:   2%|| 10/660 [00:00<00:02, 311.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss: 15938.2415, Test Loss: 15637.3993\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_train: 8:   0%|| 10/2639 [00:00<00:22, 114.56it/s]\n",
      "Epoch_test: 8:   2%|| 10/660 [00:00<00:02, 312.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss: 14831.6156, Test Loss: 14599.1818\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_train: 9:   0%|| 10/2639 [00:00<00:22, 115.45it/s]\n",
      "Epoch_test: 9:   2%|| 10/660 [00:00<00:02, 312.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train Loss: 14011.8594, Test Loss: 12105.1151\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch_train: 10:   0%|| 10/2639 [00:00<00:23, 113.01it/s]\n",
      "Epoch_test: 10:   2%|| 10/660 [00:00<00:02, 310.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 11662.7029, Test Loss: 10288.1180\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 346.457x194.882 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data/results directories\n",
    "result_tag = 'blstm-esm_dms_binding' # specify expression or binding, esm or rbd_learned\n",
    "data_dir = '../data/pickles'\n",
    "results_dir = '../results/run_results/blstm'\n",
    "\n",
    "# Create run directory for results\n",
    "now = datetime.datetime.now()\n",
    "date_hour_minute = now.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "run_dir = os.path.join(results_dir, f\"{result_tag}-{date_hour_minute}\")\n",
    "os.makedirs(run_dir, exist_ok = True)\n",
    "\n",
    "# Load in data (from pickle)\n",
    "# embedded_train_pkl = os.path.join(data_dir, 'dms_mutation_expression_meanFs_train_esm_embedded.pkl') # blstm-esm_dms_expression\n",
    "# embedded_test_pkl = os.path.join(data_dir, 'dms_mutation_expression_meanFs_test_esm_embedded.pkl')\n",
    "embedded_train_pkl = os.path.join(data_dir, 'dms_mutation_binding_Kds_train_esm_embedded.pkl') # blstm-esm_dms_binding\n",
    "embedded_test_pkl = os.path.join(data_dir, 'dms_mutation_binding_Kds_test_esm_embedded.pkl')\n",
    "\n",
    "# embedded_train_pkl = os.path.join(data_dir, 'dms_mutation_expression_meanFs_train_rbd_learned_embedded_320.pkl') # blstm-rbd_learned_320_dms_expression\n",
    "# embedded_test_pkl = os.path.join(data_dir, 'dms_mutation_expression_meanFs_test_rbd_learned_embedded_320.pkl')\n",
    "# embedded_train_pkl = os.path.join(data_dir, 'dms_mutation_binding_Kds_train_rbd_learned_embedded_320.pkl') # blstm-rbd_learned_320_dms_binding\n",
    "# embedded_test_pkl = os.path.join(data_dir, 'dms_mutation_binding_Kds_test_rbd_learned_embedded_320.pkl')\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "train_dataset = EmbeddedDMSDataset(embedded_train_pkl, device)\n",
    "test_dataset = EmbeddedDMSDataset(embedded_test_pkl, device)\n",
    "\n",
    "# Run setup\n",
    "n_epochs = 10\n",
    "batch_size = 32\n",
    "max_batch = 10\n",
    "lr = 1e-5\n",
    "\n",
    "# BLSTM input\n",
    "lstm_input_size = 320\n",
    "lstm_hidden_size = 320\n",
    "lstm_num_layers = 1        \n",
    "lstm_bidrectional = True   \n",
    "fcn_hidden_size = 320\n",
    "model = BLSTM(lstm_input_size, lstm_hidden_size, lstm_num_layers, lstm_bidrectional, fcn_hidden_size)\n",
    "\n",
    "# Run\n",
    "count_parameters(model)\n",
    "model_result = os.path.join(run_dir, f\"{result_tag}-{date_hour_minute}_train_{len(train_dataset)}_test_{len(test_dataset)}\")\n",
    "metrics_csv = run_model(model, train_dataset, test_dataset, n_epochs, batch_size, lr, max_batch, device, model_result)\n",
    "calc_train_test_history(metrics_csv, len(train_dataset), len(test_dataset), model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## BERT-BLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" BLSTM with FCN layer, MLM, and BERT. \"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pnlp.model.language import ProteinMaskedLanguageModel, BERT\n",
    "from model.blstm import BLSTM\n",
    "\n",
    "class BERT_BLSTM(nn.Module):\n",
    "    \"\"\"\" BLSTM with FCN layer, MLM, and BERT. \"\"\"\n",
    "    \n",
    "    def __init__(self, bert: BERT, blstm:BLSTM, vocab_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bert = bert\n",
    "        self.mlm = ProteinMaskedLanguageModel(self.bert.hidden, vocab_size)\n",
    "        self.blstm = blstm\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.bert(x)\n",
    "        error_1 = self.mlm(x) # error from masked language\n",
    "        error_2 = self.blstm(x) # error from regession\n",
    "\n",
    "        return error_1, error_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_combined_history(history_df: str, save_as):\n",
    "    \"\"\"\n",
    "    Generate a single figure with subplots for combined training loss\n",
    "    from the model run csv file.\n",
    "    \"\"\"\n",
    "    sns.set_theme()\n",
    "    sns.set_context('talk')\n",
    "    sns.set(style=\"darkgrid\")\n",
    "    plt.ion()\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(16, 9))\n",
    "\n",
    "    # Plot Training Loss\n",
    "    train_loss_line = ax.plot(history_df['epoch'], history_df['train_combined_loss'], color='tab:orange', label='Train Loss')\n",
    "    test_loss_line = ax.plot(history_df['epoch'], history_df['test_combined_loss'],color='tab:blue', label='Test Loss')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "    # Skipping every other y-axis tick mark\n",
    "    yticks = ax.get_yticks()\n",
    "    ax.set_yticks(yticks[::2])  # Keep every other tick\n",
    "\n",
    "    plt.style.use('ggplot')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_as+'_combined_loss.pdf', format='pdf')\n",
    "\n",
    "def plot_mlm_history(history_df: str, save_as):\n",
    "    \"\"\"\n",
    "    Generate a single figure with subplots for training loss and training accuracy\n",
    "    from the model run csv file.\n",
    "    \"\"\"\n",
    "    sns.set_theme()\n",
    "    sns.set_context('talk')\n",
    "    sns.set(style=\"darkgrid\")\n",
    "    plt.ion()\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(16, 18))\n",
    "\n",
    "    # Plot Training Loss\n",
    "    train_loss_line = ax1.plot(history_df['epoch'], history_df['train_mlm_loss'], color='tab:red', label='Train Loss')\n",
    "    test_loss_line = ax1.plot(history_df['epoch'], history_df['test_mlm_loss'],color='tab:orange', label='Test Loss')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend(loc='upper right')\n",
    "\n",
    "    # Plot Training Accuracy\n",
    "    train_accuracy_line = ax2.plot(history_df['epoch'], history_df['train_mlm_accuracy'], color='tab:blue', label='Train Accuracy')\n",
    "    test_accuracy_line = ax2.plot(history_df['epoch'], history_df['test_mlm_accuracy'], color='tab:green', label='Test Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_ylim(0, 1) \n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    # Skipping every other y-axis tick mark\n",
    "    a1_yticks = ax1.get_yticks()\n",
    "    ax1.set_yticks(a1_yticks[::2])  # Keep every other tick\n",
    "\n",
    "    plt.style.use('ggplot')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_as+'_loss_acc.pdf', format='pdf')\n",
    "\n",
    "def plot_rmse_history(history_df, save_as: str):\n",
    "    \"\"\" Plot RMSE training and testing history per epoch. \"\"\"\n",
    "\n",
    "    sns.set_theme()\n",
    "    sns.set_context('talk')\n",
    "    sns.set(style=\"darkgrid\")\n",
    "\n",
    "    # Converting mm to inches for figsize\n",
    "    width_in = 88/25.4 # mm to inches\n",
    "    ratio = 16/9\n",
    "    height_in = width_in/ratio \n",
    "    fig, ax = plt.subplots(figsize=(width_in, height_in))\n",
    "    plt.rcParams['font.family'] = 'sans-serif'\n",
    "\n",
    "    # Plot\n",
    "    sns.lineplot(data=history_df, x=history_df.index, y='test_blstm_rmse', color='tab:blue', linewidth=0.5, ax=ax) # add label='Test RMSE' for legend\n",
    "    sns.lineplot(data=history_df, x=history_df.index, y='train_blstm_rmse', color='tab:orange', linewidth=0.5,ax=ax) # add label='Train RMSE' for legend\n",
    "    \n",
    "    # Set the font size\n",
    "    font_size = 8\n",
    "    ax.set_xlabel('Epoch', fontsize=font_size)\n",
    "    ax.set_ylabel(f'RMSE', fontsize=font_size)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=font_size)\n",
    "    # ax.legend(fontsize=font_size)\n",
    "\n",
    "    # Skipping every other y-, x-axis tick mark\n",
    "    ax_yticks = ax.get_yticks()\n",
    "    ax.set_ylim(-0.1, 1.8)\n",
    "\n",
    "    ax_xticks = ax.get_xticks()\n",
    "    new_xlabels = ['' if i % 2 else label for i, label in enumerate(ax.get_xticklabels())]\n",
    "    ax.set_xticks(ax_xticks)\n",
    "    ax.set_xticklabels(new_xlabels)\n",
    "    ax.set_xlim(-100, 5000)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_as + '_rmse.pdf', format='pdf')\n",
    "\n",
    "def plot_all_loss_history(history_df, save_as:str):\n",
    "    \"\"\" Plot error1 (MLM), error2 (BLSTM), and total_error training and testing history per epoch. \"\"\"\n",
    "    sns.set_theme()\n",
    "    sns.set_context('talk')\n",
    "    sns.set(style=\"darkgrid\")\n",
    "    fig, ax = plt.subplots(figsize=(16, 9))\n",
    "    plt.rcParams['font.family'] = 'sans-serif'\n",
    "    fontsize = 20\n",
    "\n",
    "    # Color mapping\n",
    "    palette = sns.color_palette(\"Paired\", 10)\n",
    "    \n",
    "    # Plot\n",
    "    sns.lineplot(data=history_df, x=history_df.index, y='test_mlm_loss', label='Test Error 1', color=palette[0], linewidth=2, ax=ax) \n",
    "    sns.lineplot(data=history_df, x=history_df.index, y='train_mlm_loss', label='Train Error 1', color=palette[1], linewidth=2, ax=ax)\n",
    "    sns.lineplot(data=history_df, x=history_df.index, y='test_blstm_rmse', label='Test Error 2', color=palette[4], linewidth=2, ax=ax)\n",
    "    sns.lineplot(data=history_df, x=history_df.index, y='train_blstm_rmse', label='Train Error 2', color=palette[5], linewidth=2, ax=ax)\n",
    "    sns.lineplot(data=history_df, x=history_df.index, y='test_combined_loss', label='Test Combined Error', color=palette[8], linewidth=2, ax=ax)\n",
    "    sns.lineplot(data=history_df, x=history_df.index, y='train_combined_loss', label='Train Combined Error', color=palette[9], linewidth=2, ax=ax)\n",
    "\n",
    "    ax.set_xlim(-100, 5000)\n",
    "    ax_yticks = ax.get_yticks()\n",
    "    ax.set_yticks(ax_yticks[::2])  # Keep every other tick\n",
    "    ax.tick_params(axis='both', which='major', labelsize=fontsize)\n",
    "    ax.yaxis.get_offset_text().set_fontsize(fontsize) \n",
    "\n",
    "    ax.legend(fontsize=fontsize)\n",
    "    ax.set_xlabel('Epoch', fontsize=fontsize)\n",
    "    ax.set_ylabel('Loss', fontsize=fontsize)\n",
    "\n",
    "    plt.style.use('ggplot')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_as + '_all_loss.pdf', format='pdf')\n",
    "\n",
    "def calc_train_test_history(metrics_csv: str, n_train: int, n_test: int, save_as: str):\n",
    "    \"\"\" Calculate the average mse per item and rmse \"\"\"\n",
    "\n",
    "    history_df = pd.read_csv(metrics_csv, sep=',', header=0)\n",
    "\n",
    "    history_df['train_blstm_loss_per'] = history_df['train_blstm_loss']/n_train  # average mse per item\n",
    "    history_df['test_blstm_loss_per'] = history_df['test_blstm_loss']/n_test\n",
    "\n",
    "    history_df['train_blstm_rmse'] = np.sqrt(history_df['train_blstm_loss_per'].values)  # rmse\n",
    "    history_df['test_blstm_rmse'] = np.sqrt(history_df['test_blstm_loss_per'].values)\n",
    "\n",
    "    history_df.to_csv(save_as+'_metrics_per.csv', index=False)\n",
    "    plot_mlm_history(history_df, save_as)\n",
    "    plot_rmse_history(history_df, save_as)\n",
    "    plot_combined_history(history_df, save_as)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialized with RBD_Learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tqdm\n",
    "import torch\n",
    "import pickle\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Union\n",
    "from collections import defaultdict\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pnlp.embedding.tokenizer import ProteinTokenizer, token_to_index\n",
    "from pnlp.model.language import BERT\n",
    "\n",
    "class DMSDataset(Dataset):\n",
    "    \"\"\" Binding or Expression DMS Dataset, not from pickle! \"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file:str):\n",
    "        \"\"\"\n",
    "        Load from csv file into pandas:\n",
    "        - sequence label ('labels'), \n",
    "        - binding or expression numerical target ('log10Ka' or 'ML_meanF'), and \n",
    "        - 'sequence'\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.full_df = pd.read_csv(csv_file, sep=',', header=0)\n",
    "            self.target = 'log10Ka' if 'binding' in csv_file else 'ML_meanF'\n",
    "        except (FileNotFoundError, pd.errors.ParserError, Exception) as e:\n",
    "            print(f\"Error reading in .csv file: {csv_file}\\n{e}\", file=sys.stderr)\n",
    "            sys.exit(1)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.full_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # label, seq, target\n",
    "        return self.full_df['labels'][idx], self.full_df['sequence'][idx], self.full_df[self.target][idx]\n",
    "\n",
    "def run_model(model, tokenizer, train_set, test_set, n_epochs: int, batch_size: int, max_batch: Union[int, None], alpha:float, lr:float, device: str, save_as: str):\n",
    "    \"\"\" Run a model through train and test epochs. \"\"\"\n",
    "    \n",
    "    if not max_batch:\n",
    "        max_batch = len(train_set)\n",
    "\n",
    "    model = model.to(device)\n",
    "    regression_loss_fn = nn.MSELoss(reduction='sum').to(device) # blstm\n",
    "    masked_language_loss_fn = nn.CrossEntropyLoss(reduction='sum').to(device) # mlm\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_set, batch_size, shuffle=True)\n",
    "\n",
    "    metrics_csv = save_as + \"_metrics.csv\"\n",
    "\n",
    "    with open(metrics_csv, \"w\") as fh:\n",
    "        fh.write(f\"epoch,\"\n",
    "                 f\"train_mlm_accuracy,test_mlm_accuracy,\"\n",
    "                 f\"train_mlm_loss,test_mlm_loss,\"\n",
    "                 f\"train_blstm_loss,test_blstm_loss,\"\n",
    "                 f\"train_combined_loss,test_combined_loss\\n\")\n",
    "\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            train_mlm_accuracy, train_mlm_loss, train_blstm_loss, train_combined_loss = epoch_iteration(model, tokenizer, regression_loss_fn, masked_language_loss_fn, optimizer, train_loader, epoch, max_batch, alpha, device, mode='train')\n",
    "            test_mlm_accuracy, test_mlm_loss, test_blstm_loss, test_combined_loss = epoch_iteration(model, tokenizer, regression_loss_fn, masked_language_loss_fn, optimizer, test_loader, epoch, max_batch, alpha, device, mode='test')\n",
    "\n",
    "            print(f'Epoch {epoch} | Train MLM Acc: {train_mlm_accuracy:.4f}, Test MLM Acc: {test_mlm_accuracy:.4f}\\n'\n",
    "                  f'{\" \"*(len(str(epoch))+7)}| Train MLM Loss: {train_mlm_loss:.4f}, Test MLM Loss: {test_mlm_loss:.4f}\\n'\n",
    "                  f'{\" \"*(len(str(epoch))+7)}| Train BLSTM Loss: {train_blstm_loss:.4f}, Test BLSTM Loss: {test_blstm_loss:.4f}\\n'\n",
    "                  f'{\" \"*(len(str(epoch))+7)}| Train Combined Loss: {train_combined_loss:.4f}, Test Combined Loss: {test_combined_loss:.4f}\\n')\n",
    "            \n",
    "            fh.write(f\"{epoch},\"\n",
    "                     f\"{train_mlm_accuracy},{test_mlm_accuracy},\"\n",
    "                     f\"{train_mlm_loss},{test_mlm_loss},\"\n",
    "                     f\"{train_blstm_loss},{test_blstm_loss},\"\n",
    "                     f\"{train_combined_loss},{test_combined_loss}\\n\")\n",
    "            fh.flush()\n",
    "            \n",
    "            save_model(model, optimizer, epoch, save_as + '.model_save')    \n",
    "\n",
    "    return metrics_csv\n",
    "\n",
    "def epoch_iteration(model, tokenizer, regression_loss_fn, masked_language_loss_fn, optimizer, data_loader, num_epochs: int, max_batch: int, alpha:float, device: str, mode: str):\n",
    "    \"\"\" Used in run_model. \"\"\"\n",
    "    \n",
    "    model.train() if mode=='train' else model.eval()\n",
    "\n",
    "    data_iter = tqdm.tqdm(enumerate(data_loader),\n",
    "                          desc=f'Epoch_{mode}: {num_epochs}',\n",
    "                          total=len(data_loader),\n",
    "                          bar_format='{l_bar}{r_bar}')\n",
    "\n",
    "    total_mlm_loss = 0\n",
    "    total_blstm_loss = 0\n",
    "    total_combined_loss = 0\n",
    "    total_masked = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for batch, batch_data in data_iter:\n",
    "        if max_batch > 0 and batch >= max_batch:\n",
    "            break\n",
    "\n",
    "        labels, seqs, targets = batch_data\n",
    "        masked_tokenized_seqs = tokenizer(seqs).to(device)\n",
    "        unmasked_tokenized_seqs = tokenizer._batch_pad(seqs).to(device)\n",
    "        target = targets.to(device).float()\n",
    "\n",
    "        if mode == 'train':\n",
    "            optimizer.zero_grad()\n",
    "            mlm_pred, blstm_pred = model(masked_tokenized_seqs)\n",
    "            batch_mlm_loss = masked_language_loss_fn(mlm_pred.transpose(1, 2), unmasked_tokenized_seqs)\n",
    "            batch_blstm_loss = regression_loss_fn(blstm_pred.flatten(), target)\n",
    "            combined_loss = batch_mlm_loss + (batch_blstm_loss * alpha)\n",
    "            combined_loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                mlm_pred, blstm_pred = model(masked_tokenized_seqs)\n",
    "                batch_mlm_loss = masked_language_loss_fn(mlm_pred.transpose(1, 2), unmasked_tokenized_seqs)\n",
    "                batch_blstm_loss = regression_loss_fn(blstm_pred.flatten(), target)\n",
    "                combined_loss = batch_mlm_loss + (batch_blstm_loss * alpha)\n",
    "\n",
    "        # Loss\n",
    "        total_mlm_loss += batch_mlm_loss.item()\n",
    "        total_blstm_loss += batch_blstm_loss.item()\n",
    "        total_combined_loss += combined_loss.item()\n",
    "\n",
    "        # Accuracy\n",
    "        predicted_tokens = torch.max(mlm_pred, dim=-1)[1]\n",
    "        masked_locations = torch.nonzero(torch.eq(masked_tokenized_seqs, token_to_index['<MASK>']), as_tuple=True)\n",
    "        correct_predictions += torch.eq(predicted_tokens[masked_locations], unmasked_tokenized_seqs[masked_locations]).sum().item()\n",
    "        total_masked += masked_locations[0].numel()\n",
    "\n",
    "    mlm_accuracy = correct_predictions / total_masked\n",
    "    return mlm_accuracy, total_mlm_loss, total_blstm_loss, total_combined_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data/results directories\n",
    "result_tag = 'bert_blstm-dms_binding' # specify expression or binding\n",
    "data_dir = '../../../data'\n",
    "results_dir = '../../../results/run_results/bert_blstm'\n",
    "\n",
    "# Create run directory for results\n",
    "now = datetime.datetime.now()\n",
    "date_hour_minute = now.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "run_dir = os.path.join(results_dir, f\"{result_tag}-{date_hour_minute}\")\n",
    "os.makedirs(run_dir, exist_ok = True)\n",
    "\n",
    "# Load in data\n",
    "# dms_train_csv = os.path.join(data_dir, 'dms_mutation_expression_meanFs_train.csv') # bert_blstm-dms_expression\n",
    "# dms_test_csv = os.path.join(data_dir, 'dms_mutation_expression_meanFs_test.csv') \n",
    "dms_train_csv = os.path.join(data_dir, 'dms_mutation_binding_Kds_train.csv') # bert_blstm-dms_binding\n",
    "dms_test_csv = os.path.join(data_dir, 'dms_mutation_binding_Kds_test.csv') \n",
    "train_dataset = DMSDataset(dms_train_csv)\n",
    "test_dataset = DMSDataset(dms_test_csv)\n",
    "\n",
    "# Load pretrained spike model weights for BERT\n",
    "model_pth = os.path.join(results_dir, '../ddp_runner/ddp-2023-10-06_20-16/ddp-2023-10-06_20-16_best_model_weights.pth') # 320 dim\n",
    "saved_state = torch.load(model_pth, map_location='cuda')\n",
    "state_dict = saved_state['model_state_dict']\n",
    "\n",
    "# For loading from ddp models, they have 'module.bert.' or 'module.mlm.' in keys of state_dict\n",
    "# Also need separated out for each corresponding model part\n",
    "bert_state_dict = {key[len('module.bert.'):]: value for key, value in state_dict.items() if key.startswith('module.bert.')}\n",
    "mlm_state_dict = {key[len('module.mlm.'):]: value for key, value in state_dict.items() if key.startswith('module.mlm.')}\n",
    "\n",
    "# BERT input\n",
    "max_len = 280\n",
    "mask_prob = 0.15\n",
    "embedding_dim = 320 \n",
    "dropout = 0.1\n",
    "n_transformer_layers = 12\n",
    "n_attn_heads = 10\n",
    "tokenizer = ProteinTokenizer(max_len, mask_prob)\n",
    "bert = BERT(embedding_dim, dropout, max_len, mask_prob, n_transformer_layers, n_attn_heads)\n",
    "bert.load_state_dict(bert_state_dict)\n",
    "\n",
    "# BLSTM input\n",
    "lstm_input_size = 320\n",
    "lstm_hidden_size = 320\n",
    "lstm_num_layers = 1        \n",
    "lstm_bidrectional = True   \n",
    "fcn_hidden_size = 320\n",
    "blstm = BLSTM(lstm_input_size, lstm_hidden_size, lstm_num_layers, lstm_bidrectional, fcn_hidden_size)\n",
    "\n",
    "# BERT_BLSTM input\n",
    "vocab_size = len(token_to_index)\n",
    "model = BERT_BLSTM(bert, blstm, vocab_size)\n",
    "model.mlm.load_state_dict(mlm_state_dict)\n",
    "\n",
    "# Run setup\n",
    "n_epochs = 10\n",
    "batch_size = 32\n",
    "max_batch = 10\n",
    "alpha = 1\n",
    "lr = 1e-5\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# Run\n",
    "count_parameters(model)\n",
    "model_result = os.path.join(run_dir, f\"{result_tag}-{date_hour_minute}_train_{len(train_dataset)}_test_{len(test_dataset)}\")\n",
    "metrics_csv  = run_model(model, tokenizer, train_dataset, test_dataset, n_epochs, batch_size, max_batch, alpha, lr, device, model_result)\n",
    "calc_train_test_history(metrics_csv, len(train_dataset), len(test_dataset), model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialized with ESM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_esm_embedding(embedding_dim:int, save_location:str) -> tuple:\n",
    "    \"\"\"\n",
    "    Map our tokenizer tokens used for NLP embedding and BERT to\n",
    "    Huggingface AutoTokenizer tokens and their corresponding embedding\n",
    "    weights in the ESM model. Then create an embedding file (.pth) to \n",
    "    be utilized as pretrained embedding weights for loading into the BERT model.\n",
    "    Returns name of the embedding file and the vocab size.\n",
    "\n",
    "    Inputs:\n",
    "    - embedding_dim: MUST be the same as embedding dimension to be used for the BERT model.\n",
    "    - save_location: Location to save embedding file.\n",
    "\n",
    "    Be sure to import from Huggingface:\n",
    "        from transformers import AutoTokenizer, EsmModel \n",
    "    \"\"\"\n",
    "    embedding_file = os.path.join(save_location, f\"esm_embeddings_{embedding_dim}_dim.pth\")\n",
    "\n",
    "    # ESM input\n",
    "    esm = EsmModel.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "    esm_embeddings = esm.embeddings.word_embeddings.weight\n",
    "    esm_tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
    "\n",
    "    # Identifying tokens that exist in both datasets\n",
    "    esm_tokens = esm_tokenizer.get_vocab()\n",
    "    main_tokens = token_to_index\n",
    "\n",
    "    # Manual mapping for special tokens that exist in both sets, main tokens as keys\n",
    "    # '<TRUNCATED>' alternate does not exist in ESM\n",
    "    special_token_mapping = {'<START>':'<cls>', \n",
    "                             '<PAD>':'<pad>', \n",
    "                             '<END>':'<eos>', \n",
    "                             '<OTHER>':'<unk>',\n",
    "                             '<MASK>':'<mask>'}\n",
    "\n",
    "    # Create the dictionary to map ESM embeddings to our tokens\n",
    "    esm_embedding_map = {}\n",
    "    for token in main_tokens:\n",
    "        if token in esm_tokens:\n",
    "            esm_embedding_map[main_tokens[token]] = esm_embeddings[esm_tokens[token]]\n",
    "        elif token in special_token_mapping and special_token_mapping[token] in esm_tokens:\n",
    "            esm_embedding_map[main_tokens[token]] = esm_embeddings[esm_tokens[special_token_mapping[token]]]\n",
    "\n",
    "    # Create a ESM embedding tensor that can be loaded into BERT \n",
    "    vocab_size = len(esm_embedding_map.keys())\n",
    "    esm_embeddings = torch.zeros(vocab_size, embedding_dim)  # Initialize a tensor of zeros\n",
    "    for token, embedding in esm_embedding_map.items():\n",
    "        esm_embeddings[token] = embedding\n",
    "    torch.save(esm_embeddings, embedding_file)\n",
    "\n",
    "    return embedding_file, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data/results directories\n",
    "result_tag = 'bert_blstm_esm-dms_binding'\n",
    "data_dir = '../../../data'\n",
    "results_dir = '../../../results/run_results/bert_blstm_esm'\n",
    "\n",
    "# Create run directory for results\n",
    "now = datetime.datetime.now()\n",
    "date_hour_minute = now.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "run_dir = os.path.join(results_dir, f\"{result_tag}-{date_hour_minute}\")\n",
    "os.makedirs(run_dir, exist_ok = True)\n",
    "\n",
    "# Load in data\n",
    "# dms_train_csv = os.path.join(data_dir, 'dms_mutation_expression_meanFs_train.csv') # 'bert_blstm_esm-dms_expression'\n",
    "# dms_test_csv = os.path.join(data_dir, 'dms_mutation_expression_meanFs_test.csv') \n",
    "dms_train_csv = os.path.join(data_dir, 'dms_mutation_binding_Kds_train.csv') # 'bert_blstm_esm-dms_binding'\n",
    "dms_test_csv = os.path.join(data_dir, 'dms_mutation_binding_Kds_test.csv') \n",
    "train_dataset = DMSDataset(dms_train_csv)\n",
    "test_dataset = DMSDataset(dms_test_csv)\n",
    "\n",
    "# BERT input\n",
    "max_len = 280\n",
    "mask_prob = 0.15\n",
    "embedding_dim = 320 \n",
    "dropout = 0.1\n",
    "n_transformer_layers = 12\n",
    "n_attn_heads = 10\n",
    "tokenizer = ProteinTokenizer(max_len, mask_prob)\n",
    "\n",
    "# Create and load esm embedding file to BERT model\n",
    "embedding_file, vocab_size = create_esm_embedding(embedding_dim, run_dir)\n",
    "\n",
    "bert = BERT(embedding_dim, dropout, max_len, mask_prob, n_transformer_layers, n_attn_heads)\n",
    "bert.embedding.load_pretrained_embeddings(embedding_file, no_grad=False)\n",
    "\n",
    "# BLSTM input\n",
    "lstm_input_size = 320\n",
    "lstm_hidden_size = 320\n",
    "lstm_num_layers = 1        \n",
    "lstm_bidrectional = True   \n",
    "fcn_hidden_size = 320\n",
    "blstm = BLSTM(lstm_input_size, lstm_hidden_size, lstm_num_layers, lstm_bidrectional, fcn_hidden_size)\n",
    "\n",
    "# BERT_BLSTM input\n",
    "model = BERT_BLSTM(bert, blstm, vocab_size)\n",
    "\n",
    "# Run setup\n",
    "n_epochs = 10\n",
    "batch_size = 32\n",
    "max_batch = 10\n",
    "alpha = 1\n",
    "lr = 1e-5\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# Run\n",
    "count_parameters(model)\n",
    "model_result = os.path.join(run_dir, f\"{result_tag}-{date_hour_minute}_train_{len(train_dataset)}_test_{len(test_dataset)}\")\n",
    "metrics_csv = run_model(model, tokenizer, train_dataset, test_dataset, n_epochs, batch_size, max_batch, alpha, lr, device, model_result)\n",
    "calc_train_test_history(metrics_csv, len(train_dataset), len(test_dataset), model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ESM-BLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESM_BLSTM(nn.Module):\n",
    "    def __init__(self, esm, blstm):\n",
    "        super().__init__()\n",
    "        self.esm = esm\n",
    "        self.blstm = blstm\n",
    "\n",
    "    def forward(self, tokenized_seqs):\n",
    "        with torch.set_grad_enabled(self.training):  # Enable gradients, managed by model.eval() or model.train() in epoch_iteration\n",
    "            esm_output = self.esm(**tokenized_seqs).last_hidden_state\n",
    "            reshaped_output = esm_output.squeeze(0)  \n",
    "            output = self.blstm(reshaped_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tqdm\n",
    "import torch\n",
    "import pickle\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Union\n",
    "from collections import defaultdict\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, EsmModel \n",
    "from pnlp.embedding.tokenizer import ProteinTokenizer, token_to_index\n",
    "from pnlp.model.language import BERT\n",
    "\n",
    "class DMSDataset(Dataset):\n",
    "    \"\"\" Binding or Expression DMS Dataset, not from pickle! \"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file:str):\n",
    "        \"\"\"\n",
    "        Load from csv file into pandas:\n",
    "        - sequence label ('labels'), \n",
    "        - binding or expression numerical target ('log10Ka' or 'ML_meanF'), and \n",
    "        - 'sequence'\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.full_df = pd.read_csv(csv_file, sep=',', header=0)\n",
    "            self.target = 'log10Ka' if 'binding' in csv_file else 'ML_meanF'\n",
    "        except (FileNotFoundError, pd.errors.ParserError, Exception) as e:\n",
    "            print(f\"Error reading in .csv file: {csv_file}\\n{e}\", file=sys.stderr)\n",
    "            sys.exit(1)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.full_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # label, seq, target\n",
    "        return self.full_df['labels'][idx], self.full_df['sequence'][idx], self.full_df[self.target][idx]\n",
    "\n",
    "def run_model(model, tokenizer, train_set, test_set, n_epochs: int, batch_size: int, lr:float, max_batch: Union[int, None], device: str, save_as: str):\n",
    "    \"\"\" Run a model through train and test epochs. \"\"\"\n",
    "    \n",
    "    if not max_batch:\n",
    "        max_batch = len(train_set)\n",
    "\n",
    "    model = model.to(device)\n",
    "    loss_fn = nn.MSELoss(reduction='sum').to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_set, batch_size, shuffle=True)\n",
    "\n",
    "    metrics_csv = save_as + \"_metrics.csv\"\n",
    "    \n",
    "    with open(metrics_csv, \"w\") as fh:\n",
    "        fh.write(f\"epoch,\"\n",
    "                 f\"train_loss,test_loss\\n\")\n",
    "\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            train_loss = epoch_iteration(model, tokenizer, loss_fn, optimizer, train_loader, epoch, max_batch, device, mode='train')\n",
    "            test_loss = epoch_iteration(model, tokenizer, loss_fn, optimizer, test_loader, epoch, max_batch, device, mode='test')\n",
    "\n",
    "            print(f'Epoch {epoch} | Train BLSTM Loss: {train_loss:.4f}, Test BLSTM Loss: {test_loss:.4f}\\n')\n",
    "           \n",
    "            fh.write(f\"{epoch},\"\n",
    "                     f\"{train_loss},{test_loss}\\n\")\n",
    "            fh.flush()\n",
    "                \n",
    "            save_model(model, optimizer, epoch, save_as + '.model_save')\n",
    "\n",
    "    return metrics_csv\n",
    "\n",
    "def epoch_iteration(model, tokenizer, loss_fn, optimizer, data_loader, num_epochs: int, max_batch: int, device: str, mode: str):\n",
    "    \"\"\" Used in run_model. \"\"\"\n",
    "    \n",
    "    model.train() if mode=='train' else model.eval()\n",
    "\n",
    "    data_iter = tqdm.tqdm(enumerate(data_loader),\n",
    "                          desc=f'Epoch_{mode}: {num_epochs}',\n",
    "                          total=len(data_loader),\n",
    "                          bar_format='{l_bar}{r_bar}')\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch, batch_data in data_iter:\n",
    "        if max_batch > 0 and batch >= max_batch:\n",
    "            break\n",
    "\n",
    "        labels, seqs, targets = batch_data\n",
    "        targets = targets.to(device).float()\n",
    "        tokenized_seqs = tokenizer(seqs,return_tensors=\"pt\").to(device)\n",
    "   \n",
    "        if mode == 'train':\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(tokenized_seqs).flatten()\n",
    "            batch_loss = loss_fn(pred, targets)\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                pred = model(tokenized_seqs).flatten()\n",
    "                batch_loss = loss_fn(pred, targets)\n",
    "\n",
    "        total_loss += batch_loss.item()\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data/results directories\n",
    "result_tag = 'esm-blstm-esm_dms_binding' # specify expression or binding\n",
    "data_dir = '../../../data'\n",
    "results_dir = '../../../results/run_results/esm-blstm'\n",
    "\n",
    "# Create run directory for results\n",
    "now = datetime.datetime.now()\n",
    "date_hour_minute = now.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "run_dir = os.path.join(results_dir, f\"{result_tag}-{date_hour_minute}\")\n",
    "os.makedirs(run_dir, exist_ok = True)\n",
    "\n",
    "# Load in data (from csv)\n",
    "# dms_train_csv = os.path.join(data_dir, 'dms_mutation_expression_meanFs_train.csv') # esm-blstm-esm_dms_expression\n",
    "# dms_test_csv = os.path.join(data_dir, 'dms_mutation_expression_meanFs_test.csv') \n",
    "dms_train_csv = os.path.join(data_dir, 'dms_mutation_binding_Kds_train.csv') # esm-blstm-esm_dms_binding\n",
    "dms_test_csv = os.path.join(data_dir, 'dms_mutation_binding_Kds_test.csv') \n",
    "\n",
    "train_dataset = DMSDataset(dms_train_csv)\n",
    "test_dataset = DMSDataset(dms_test_csv)\n",
    "\n",
    "# Run setup\n",
    "n_epochs = 10\n",
    "batch_size = 32\n",
    "max_batch = 10\n",
    "lr = 1e-5\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# ESM input\n",
    "esm = EsmModel.from_pretrained(\"facebook/esm2_t6_8M_UR50D\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
    "\n",
    "# BLSTM input\n",
    "lstm_input_size = 320\n",
    "lstm_hidden_size = 320\n",
    "lstm_num_layers = 1        \n",
    "lstm_bidrectional = True   \n",
    "fcn_hidden_size = 320\n",
    "blstm = BLSTM(lstm_input_size, lstm_hidden_size, lstm_num_layers, lstm_bidrectional, fcn_hidden_size)\n",
    "\n",
    "model = ESM_BLSTM(esm, blstm)\n",
    "\n",
    "# Run\n",
    "count_parameters(model)\n",
    "model_result = os.path.join(run_dir, f\"{result_tag}-{date_hour_minute}_train_{len(train_dataset)}_test_{len(test_dataset)}\")\n",
    "metrics_csv = run_model(model, tokenizer, train_dataset, test_dataset, n_epochs, batch_size, lr, max_batch, device, model_result)\n",
    "calc_train_test_history(metrics_csv, len(train_dataset), len(test_dataset), model_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spike_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
