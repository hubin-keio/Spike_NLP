{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test BERT-Pytorch\n",
    "\n",
    "BERT-pytorch is a PyTorch implementation of the BERT algorithm.\n",
    "\n",
    "[BERT-pytorch](https://github.com/codertimo/BERT-pytorch)\n",
    "\n",
    "\n",
    "## Embedding\n",
    "In the BERT implemetnation (bert_pytorch/model/bert.py), the masking is done after the second token (x>0) since in the original BERT paper, the first element of the input is always \\[CLS\\]. In our model, we will use the variant name as the \\[CLS\\] and the values are:\n",
    "[wt, alpha, delta, omicron, na], where \"na\" stands for not assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import copy\n",
    "import math\n",
    "from Bio import SeqIO\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# from bert_pytorch.model import BERT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Vocabulary\n",
    "In [ProteinBERT](https://academic.oup.com/bioinformatics/article/38/8/2102/6502274), Brandes et al used 26 unique tokens to represent the 20 standard amino acids, selenocysteine (U), and undefined amino acid (X), another amino acid (OTHER) and three speical tokens \\<START\\>, \\<END\\>, \\<PAD\\>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the source code of protein_bert\n",
    "ALL_AAS = 'ACDEFGHIKLMNPQRSTUVWXY'\n",
    "ADDITIONAL_TOKENS = ['<OTHER>', '<START>', '<END>', '<PAD>']\n",
    "\n",
    "# Each sequence is added <START> a\n",
    "ADDED_TOKENS_PER_SEQ = 2\n",
    "\n",
    "n_aas = len(ALL_AAS)\n",
    "aa_to_token_index = {aa: i for i, aa in enumerate(ALL_AAS)}\n",
    "additional_token_to_index = {token: i + n_aas for i, token in enumerate(ADDITIONAL_TOKENS)}\n",
    "token_to_index = {**aa_to_token_index, **additional_token_to_index}\n",
    "index_to_token = {index: token for token, index in token_to_index.items()}\n",
    "n_tokens = len(token_to_index)\n",
    "\n",
    "def tokenize_seq(seq):\n",
    "    other_token_index = additional_token_to_index['<OTHER>']\n",
    "    return [additional_token_to_index['<START>']] + [aa_to_token_index.get(aa, other_token_index) for aa in seq] + [additional_token_to_index['<END>']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amino Acid Token Embeddings\n",
    "We will derive it from the [torch.nn.Embedding class](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html). The size of the vacabulary equals the number of tokens. This approach allows the learning of the embeddings from the model intself. If we train the model with virus sepcific squences, the embeddings shall reflect the hidden properties of the amino acids in context of the trainign sequences. Note that the \\<START\\> and \\<END\\> tokens are always added at the beginning of the sequence. \\<PAD\\> tokens may be added before the \\<END\\> token if the sequence is shorter than the input sequence.\n",
    "\n",
    "Note that using the \"from_pretrained\" class method of torch.nn.Embedding, we can load pretrained weights of the embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "class TokenEmbedding(nn.Embedding):\n",
    "    def __init__(self, num_embeddings: torch.Tensor, embedding_dim: int = 512, padding_idx=None):\n",
    "        super().__init__(num_embeddings, embedding_dim, padding_idx)\n",
    "\n",
    "padding_idx = token_to_index['<PAD>']\n",
    "print(padding_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1413"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_wt_seq = \"\"\">sp|P0DTC2|SPIKE_SARS2 Spike glycoprotein OS=Severe acute respiratory syndrome coronavirus 2 OX=2697049 GN=S PE=1 SV=1\n",
    "MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSSVLHSTQDLFLPFFS\n",
    "NVTWFHAIHVSGTNGTKRFDNPVLPFNDGVYFASTEKSNIIRGWIFGTTLDSKTQSLLIV\n",
    "NNATNVVIKVCEFQFCNDPFLGVYYHKNNKSWMESEFRVYSSANNCTFEYVSQPFLMDLE\n",
    "GKQGNFKNLREFVFKNIDGYFKIYSKHTPINLVRDLPQGFSALEPLVDLPIGINITRFQT\n",
    "LLALHRSYLTPGDSSSGWTAGAAAYYVGYLQPRTFLLKYNENGTITDAVDCALDPLSETK\n",
    "CTLKSFTVEKGIYQTSNFRVQPTESIVRFPNITNLCPFGEVFNATRFASVYAWNRKRISN\n",
    "CVADYSVLYNSASFSTFKCYGVSPTKLNDLCFTNVYADSFVIRGDEVRQIAPGQTGKIAD\n",
    "YNYKLPDDFTGCVIAWNSNNLDSKVGGNYNYLYRLFRKSNLKPFERDISTEIYQAGSTPC\n",
    "NGVEGFNCYFPLQSYGFQPTNGVGYQPYRVVVLSFELLHAPATVCGPKKSTNLVKNKCVN\n",
    "FNFNGLTGTGVLTESNKKFLPFQQFGRDIADTTDAVRDPQTLEILDITPCSFGGVSVITP\n",
    "GTNTSNQVAVLYQDVNCTEVPVAIHADQLTPTWRVYSTGSNVFQTRAGCLIGAEHVNNSY\n",
    "ECDIPIGAGICASYQTQTNSPRRARSVASQSIIAYTMSLGAENSVAYSNNSIAIPTNFTI\n",
    "SVTTEILPVSMTKTSVDCTMYICGDSTECSNLLLQYGSFCTQLNRALTGIAVEQDKNTQE\n",
    "VFAQVKQIYKTPPIKDFGGFNFSQILPDPSKPSKRSFIEDLLFNKVTLADAGFIKQYGDC\n",
    "LGDIAARDLICAQKFNGLTVLPPLLTDEMIAQYTSALLAGTITSGWTFGAGAALQIPFAM\n",
    "QMAYRFNGIGVTQNVLYENQKLIANQFNSAIGKIQDSLSSTASALGKLQDVVNQNAQALN\n",
    "TLVKQLSSNFGAISSVLNDILSRLDKVEAEVQIDRLITGRLQSLQTYVTQQLIRAAEIRA\n",
    "SANLAATKMSECVLGQSKRVDFCGKGYHLMSFPQSAPHGVVFLHVTYVPAQEKNFTTAPA\n",
    "ICHDGKAHFPREGVFVSNGTHWFVTQRNFYEPQIITTDNTFVSGNCDVVIGIVNNTVYDP\n",
    "LQPELDSFKEELDKYFKNHTSPDVDLGDISGINASVVNIQKEIDRLNEVAKNLNESLIDL\n",
    "QELGKYEQYIKWPWYIWLGFIAGLIAIVMVTIMLCCMTSCCSCLKGCCSCGSCCKFDEDD\n",
    "SEPVLKGVKLHYT\"\"\"\n",
    "len(test_wt_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seqs = []\n",
    "fa_parser = SeqIO.parse(io.StringIO(test_wt_seq), 'fasta')\n",
    "for record in fa_parser:\n",
    "    seq = record.seq\n",
    "    test_seqs.append(str(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9295, -0.3930, -1.6340,  ...,  1.0091,  0.8342,  0.6325],\n",
      "        [ 1.5217,  2.7205, -0.4793,  ..., -0.9853,  1.0511,  1.3045],\n",
      "        [ 0.0953,  1.9266,  0.0882,  ..., -0.8576, -0.1565,  0.6572],\n",
      "        ...,\n",
      "        [-0.2898,  0.1287,  1.8492,  ...,  0.6741,  1.0530,  0.2916],\n",
      "        [ 0.9037, -0.3816,  0.4129,  ...,  0.0431, -0.4507, -0.2156],\n",
      "        [-0.2640, -0.2618,  1.2745,  ...,  2.4348,  1.5389, -2.0573]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "Shape of test sequence embedding: torch.Size([1275, 20])\n"
     ]
    }
   ],
   "source": [
    "num_embeddings = n_tokens\n",
    "embedding_dim = 20\n",
    "embedding = TokenEmbedding(num_embeddings, embedding_dim, padding_idx)\n",
    "test_embedding = embedding(torch.IntTensor(tokenize_seq(test_seqs[0])))\n",
    "print(test_embedding)\n",
    "print(f'Shape of test sequence embedding: {test_embedding.shape}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look of the embedding weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1655, -1.8507,  2.3808,  1.7079, -0.8344,  0.0397, -1.1526, -0.7542,\n",
       "         -0.3881,  2.5951,  0.0365, -1.1324,  2.1356,  0.6057, -0.4333, -0.2600,\n",
       "         -2.1362,  0.0072,  1.5041, -0.8161],\n",
       "        [ 1.3647,  0.4286, -0.0438, -0.0751,  0.3926, -0.2939,  1.0673, -0.7275,\n",
       "          0.4386,  0.5220,  0.3631,  0.2311,  0.9486,  0.3194, -2.0667,  1.0608,\n",
       "          0.3539, -0.9689, -0.6506, -0.4407],\n",
       "        [-0.5890, -0.9713,  0.4251, -0.7520,  1.7415, -0.1838,  0.4673, -1.9181,\n",
       "          0.4152, -0.2488, -0.3671, -1.4407,  1.4268,  1.8225,  0.2309,  0.1728,\n",
       "         -0.5015,  0.7601,  0.2511,  1.2167],\n",
       "        [-0.9015,  0.8305, -1.2921, -0.8276,  1.1848, -0.3986,  1.0509,  0.6731,\n",
       "          0.2016, -0.0459, -1.0505,  0.2374,  0.4629,  0.9688, -1.7671,  0.5760,\n",
       "          0.7361, -1.4768,  0.2024,  0.2315],\n",
       "        [ 0.0953,  1.9266,  0.0882, -1.2296,  0.8251, -0.1247, -0.4404, -1.1608,\n",
       "         -1.7103,  0.1154, -0.3154, -0.6004,  0.6913,  1.3998,  0.8749,  0.4212,\n",
       "         -0.8285, -0.8576, -0.1565,  0.6572],\n",
       "        [ 0.9542, -0.0725, -0.4085,  0.5257, -0.1725, -1.2482, -1.8186,  0.0973,\n",
       "          0.6543, -0.1080, -1.2738,  1.4588,  0.4312, -0.1003, -0.6478, -0.1750,\n",
       "          0.6239, -1.5013, -0.2356, -0.7825],\n",
       "        [-0.1005, -0.0115,  2.4353, -0.1048,  0.4414,  0.8153,  0.4025, -1.3200,\n",
       "          0.1041,  0.0429, -0.6343, -0.3592,  0.1248, -1.2069,  1.4273,  1.4228,\n",
       "          0.0718, -0.4067,  1.5535, -0.1221],\n",
       "        [-0.4518, -1.1247, -0.1447,  0.9181,  0.0883,  0.6765, -0.0899,  2.0051,\n",
       "          0.3793,  0.3982,  0.6226, -0.4209,  0.0272,  0.2352, -0.8282, -0.3563,\n",
       "         -0.0480,  0.5931,  0.9800,  0.5181],\n",
       "        [-0.6851, -0.1689, -0.3716,  0.6786,  0.8517, -1.0323, -0.9158,  1.5330,\n",
       "          1.1411,  0.3212, -0.5422,  1.8201,  0.4521, -0.8919,  0.6814, -1.0453,\n",
       "         -0.6397, -2.5895,  0.0634, -2.5131],\n",
       "        [-0.4481,  0.4166,  1.7939,  0.4434,  0.0092, -1.1778, -0.7934, -0.5288,\n",
       "          1.1290, -0.4040, -0.2294,  0.9111,  0.4343,  1.4657, -0.2546, -1.2136,\n",
       "         -1.2227, -2.3325, -0.0910,  0.4655],\n",
       "        [ 1.5217,  2.7205, -0.4793,  1.0043,  0.3974, -0.5123,  0.2190,  1.5615,\n",
       "          0.2975, -1.1909,  0.4306, -0.0494, -1.0221, -0.9632,  1.6063, -0.2325,\n",
       "         -0.2917, -0.9853,  1.0511,  1.3045],\n",
       "        [ 0.7768,  0.2769, -0.8310, -1.1903, -0.4140, -0.5679,  0.0962,  0.6935,\n",
       "          1.8602, -1.1865, -1.0970, -1.1127, -0.6537, -0.4908,  0.2390,  1.5765,\n",
       "         -1.0528,  0.5705,  0.2282,  0.0461],\n",
       "        [-0.4014,  0.2919, -0.0790,  0.6745, -0.0602,  2.8268,  0.2942, -0.7024,\n",
       "         -0.8986, -1.3939, -0.5123,  1.4289, -0.8473, -0.9707,  0.1801,  0.5672,\n",
       "          0.2039, -0.2621, -1.3393, -0.6468],\n",
       "        [ 0.1605,  1.5208, -1.8978, -0.6472,  0.5220,  0.2800, -1.6218,  0.0964,\n",
       "          1.9523, -0.6682,  1.8753,  1.8024, -1.0564, -0.4126,  0.7884, -1.1333,\n",
       "          0.3374, -3.3738,  0.3741, -1.1258],\n",
       "        [ 2.9738, -0.2514,  0.9605, -1.2631,  1.0653,  0.5126,  0.0735, -0.7690,\n",
       "          0.5308, -0.3429, -1.3500,  0.3213,  0.2254,  0.0535, -0.5915, -0.5871,\n",
       "          0.7921, -0.0412,  0.0694, -1.7292],\n",
       "        [-0.8414,  1.4648,  1.5694,  1.4231,  1.6693, -0.3803,  0.0155,  1.3457,\n",
       "          1.5623, -0.8414, -1.3242, -2.0754,  0.8539,  0.6328,  0.2320, -1.9119,\n",
       "          0.7348,  0.6104, -0.5331, -0.5624],\n",
       "        [ 0.9037, -0.3816,  0.4129,  0.0369, -0.0135, -0.8676,  0.2753, -0.8279,\n",
       "         -0.5588,  0.6716,  1.0470,  0.2022, -1.3537, -1.7928, -0.5793, -0.4859,\n",
       "          0.0570,  0.0431, -0.4507, -0.2156],\n",
       "        [ 0.0399,  2.1801, -1.1773,  0.4994, -1.8546,  0.9949,  0.1970, -0.3854,\n",
       "          0.4165,  0.4312, -1.0823, -0.7562, -1.8278, -0.6503,  1.5608, -0.1158,\n",
       "          0.2575,  0.3728,  0.2542,  1.1810],\n",
       "        [-0.6464,  1.2336, -1.5430, -0.5693,  1.0855, -0.9962, -0.4087, -0.4853,\n",
       "         -1.0900, -0.4782, -0.8591, -0.2746, -1.3320, -2.0006, -0.0230, -1.5002,\n",
       "         -0.9854,  0.4479,  0.4017,  1.8207],\n",
       "        [ 2.2823,  0.4214, -0.6010, -1.2692, -0.1216,  1.1103, -1.2735, -0.1593,\n",
       "          0.3833,  0.6402,  0.2606,  2.5189,  0.4081, -0.1675, -0.5547, -1.0779,\n",
       "          0.0983, -0.8926,  2.2480, -3.2107],\n",
       "        [ 0.0130,  0.2186,  0.7801, -0.8467,  0.7221,  0.6645,  0.0709, -1.5678,\n",
       "         -0.7089,  0.0217, -0.4091, -0.9339, -1.1607, -0.5822, -1.1778,  1.4312,\n",
       "         -1.9028,  0.2398, -0.2329, -0.5044],\n",
       "        [-0.2898,  0.1287,  1.8492,  0.2975,  0.9325,  0.7980,  1.0141,  0.9519,\n",
       "         -2.7042,  2.7713,  0.3267,  0.6730, -0.3558, -0.0997,  0.2971,  0.2754,\n",
       "         -0.8646,  0.6741,  1.0530,  0.2916],\n",
       "        [-1.0689, -0.4060,  0.9306,  0.9437, -0.8904, -0.8551, -0.7459, -0.0039,\n",
       "         -1.8435, -1.3366, -0.2325, -0.9995,  2.4699, -0.8360, -2.3739,  1.5621,\n",
       "          0.7911,  1.5429, -0.5036,  0.5336],\n",
       "        [-0.9295, -0.3930, -1.6340,  1.5825,  2.0120,  0.4234,  0.4237,  0.1245,\n",
       "         -1.3146, -0.0730, -0.2286,  0.2704, -0.2639,  0.3775, -2.1453,  0.6349,\n",
       "         -2.4453,  1.0091,  0.8342,  0.6325],\n",
       "        [-0.2640, -0.2618,  1.2745, -1.9677,  0.8036,  0.9006,  0.1732, -0.0447,\n",
       "          0.6743,  1.9026,  0.0752,  1.0146, -0.3801, -0.0860,  0.5802,  0.5118,\n",
       "          0.6047,  2.4348,  1.5389, -2.0573],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000]], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([26, 20])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.weight.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postional Encoding\n",
    "We will use the  sine and cosine functions of different frequencie to embed positional information as in the original BERT method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Impement the PE function.\n",
    "    \n",
    "    The PE forward function is different from the BERT-pytorch. Here we used the original method in BERT so\n",
    "    PE embeddings are added to the input embeddings and no graident tracking is used.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=1275):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0)/d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f'x.shape in PositionalEncoding: {x.shape}')\n",
    "        print(f'x.shape: {x.shape},pe.shape: {self.pe.shape}')\n",
    "        print(f'pe[:, : x.size(1)]: {self.pe[:, : x.size(1)].shape}')\n",
    "\n",
    "\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Encode amino acid sequence. Input sequence is represented by summing the corresponding sequence token,\n",
    "    segment (e.g. question and answer or any segments separated by <SEP>), and position embeddings. In our \n",
    "    model, we only need the token and position embedding so segment embeddign is not implemented here.    \n",
    "    \"\"\"\n",
    "    def __init__(self, num_embeddings, embedding_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_embedding = TokenEmbedding(num_embeddings, embedding_dim)\n",
    "        self.position = PositionalEncoding(embedding_dim, dropout)\n",
    "        self.embeddng_dim = embedding_dim\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, seq:str):\n",
    "        x = torch.IntTensor(tokenize_seq(seq))\n",
    "        x = self.token_embedding(x)\n",
    "        x = self.position(x)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters in SeqEncoding: 13312\n",
      "x.shape in PositionalEncoding: torch.Size([1275, 512])\n",
      "x.shape: torch.Size([1275, 512]),pe.shape: torch.Size([1275, 512])\n",
      "pe[:, : x.size(1)]: torch.Size([1275, 512])\n",
      "tensor([[ 0.2236,  0.0085,  0.3063,  ..., -0.2313, -0.6815, -0.6398],\n",
      "        [-0.1819,  2.5132, -0.0000,  ..., -0.5560,  0.0000, -1.1247],\n",
      "        [ 3.0274, -1.8674,  1.9210,  ..., -0.2102, -0.0000,  1.2327],\n",
      "        ...,\n",
      "        [ 0.5832, -1.2684, -0.4853,  ...,  0.0000, -0.0000, -1.2761],\n",
      "        [-0.7092, -0.0000,  0.5969,  ...,  3.0542,  0.5780,  1.5972],\n",
      "        [-0.0951, -1.1107, -0.3251,  ...,  1.2578,  0.0000,  3.8438]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test_seq_encode = SeqEncoding(n_tokens, 512, 0.1)\n",
    "num_parameters_seq_encoding = sum(p.numel() for p in test_seq_encode.parameters() if p.requires_grad)\n",
    "print(f'Parameters in SeqEncoding: {num_parameters_seq_encoding}')\n",
    "print(test_seq_encode(test_seqs[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition\n",
    "\n",
    "Here we define a model based on BERT. Part of the implementation is based on [BERT-pytorch](https://github.com/codertimo/BERT-pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"\"\"Produce N identical layers\"\"\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocabl_size: int = 26, hidden: int = 768, n_layer: int = 12, attn_heads: int = 12, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        vacab_size: vacabulary or token size\n",
    "        hidden: BERT model size (used as input size and hidden size)\n",
    "        n_layer: number of Transformer layers\n",
    "        attn_heads: attenion heads\n",
    "        dropout: dropout ratio\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.hidden  = hidden\n",
    "        self.n_layer = n_layer\n",
    "        self.attn_heads = attn_heads\n",
    "\n",
    "        self.feed_forward_hidden = hidden * 4\n",
    "        self.embedding = TokenEmbedding(vocabl_size, embed_size=hidden, padding_idx=25)\n",
    "\n",
    "        self.transformer_blocks = clones(Transformer(hidden, attn_heads, hidden *4, dropout), n_layer)\n",
    "\n",
    "    def forward(self)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f28ad5bbb02419dfa4fc8e8c46da9bd89100d91bb344ce9fd3a3101abf33cb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
