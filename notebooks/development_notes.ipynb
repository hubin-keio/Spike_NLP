{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spike NLP\n",
    "\n",
    "We wish to utilize NLP methods to analyze the virus protein sequences. After initial experiment with the LSTM architecture, we decided to use Transformer architecture. In this notebook, we implement a BERT model. During the process, we learned from existing implementations of BERT, especially [BERT-pytorch](https://github.com/codertimo/BERT-pytorch) and [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/) and [ProteinBERT](https://academic.oup.com/bioinformatics/article/38/8/2102/6502274), though we have to make changes to accomodate our own research interests. For example, we are interested in next word prediction in the pre-training of the model to generate contexualized embeddings using self-supervised learning at individual amino acid level through learning the language patterns but we are not interested in protein functional annotation, so we do not use annotation in our model. In other words, we are interested in leveraing the Mask LM pre-training task to derive the embeddings for a fine-tuning model to  predict the phenotype of the virus protein sequences. Example phenotypes are bind binding kinetics of the virus protein to target receptor proteins and antibodies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Embedding\n",
    "In the BERT implemetnation (bert_pytorch/model/bert.py), the masking is done after the second token (x>0) since in the original BERT paper, the first element of the input is always \\[CLS\\]. In our model, we will use the variant name as the \\[CLS\\] and the values are:\n",
    "[wt, alpha, delta, omicron, na], where \"na\" stands for not assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import copy\n",
    "import math\n",
    "from Bio import SeqIO\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "# from bert_pytorch.model import BERT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Vocabulary\n",
    "In [ProteinBERT](https://academic.oup.com/bioinformatics/article/38/8/2102/6502274), Brandes et al used 26 unique tokens to represent the 20 standard amino acids, selenocysteine (U), and undefined amino acid (X), another amino acid (OTHER) and three speical tokens \\<START\\>, \\<END\\>, \\<PAD\\>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the source code of protein_bert\n",
    "# TODO: add a <TRUNCATED> token.\n",
    "ALL_AAS = 'ACDEFGHIKLMNPQRSTUVWXY'\n",
    "ADDITIONAL_TOKENS = ['<OTHER>', '<START>', '<END>', '<PAD>']\n",
    "\n",
    "# Each sequence is added <START> and <END>. \"<PAD>\" are added to sequence shorten than max_len.\n",
    "ADDED_TOKENS_PER_SEQ = 2\n",
    "\n",
    "n_aas = len(ALL_AAS)\n",
    "aa_to_token_index = {aa: i for i, aa in enumerate(ALL_AAS)}\n",
    "additional_token_to_index = {token: i + n_aas for i, token in enumerate(ADDITIONAL_TOKENS)}\n",
    "token_to_index = {**aa_to_token_index, **additional_token_to_index}\n",
    "index_to_token = {index: token for token, index in token_to_index.items()}\n",
    "n_tokens = len(token_to_index)\n",
    "\n",
    "def tokenize_seq(seq, max_len):\n",
    "    other_token_index = additional_token_to_index['<OTHER>']\n",
    "    token_seq = [additional_token_to_index['<START>']] + [aa_to_token_index.get(aa, other_token_index) for aa in seq]\n",
    "    if len(token_seq) < max_len - 1: # -1 is for the <END> token\n",
    "        len_pad = max_len -1 - len(token_seq)\n",
    "        token_seq.extend(token_to_index['<PAD>'] for _ in range(len_pad))\n",
    "    token_seq += [additional_token_to_index['<END>']]\n",
    "    return torch.IntTensor(token_seq)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amino Acid Token Embeddings\n",
    "We will derive it from the [torch.nn.Embedding class](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html). The size of the vacabulary equals the number of tokens. This approach allows the learning of the embeddings from the model intself. If we train the model with virus sepcific squences, the embeddings shall reflect the hidden properties of the amino acids in context of the trainign sequences. Note that the \\<START\\> and \\<END\\> tokens are always added at the beginning of the sequence. \\<PAD\\> tokens may be added before the \\<END\\> token if the sequence is shorter than the input sequence.\n",
    "\n",
    "Note that using the \"from_pretrained\" class method of torch.nn.Embedding, we can load pretrained weights of the embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "class TokenEmbedding(nn.Embedding):\n",
    "    def __init__(self, num_embeddings: torch.Tensor, embedding_dim: int = 512, max_len: int=1500, padding_idx=None):\n",
    "        super().__init__(num_embeddings, embedding_dim, padding_idx)\n",
    "\n",
    "padding_idx = token_to_index['<PAD>']\n",
    "print(padding_idx)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postional Encoding\n",
    "We will use the  sine and cosine functions of different frequencie to embed positional information as in the original BERT method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Impement the PE function.\n",
    "    \n",
    "    The PE forward function is different from the BERT-pytorch. Here we used the original method in BERT so\n",
    "    PE embeddings are added to the input embeddings and no graident tracking is used.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=1500):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0)/d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Encode amino acid sequence. Input sequence is represented by summing the corresponding sequence token,\n",
    "    segment (e.g. question and answer or any segments separated by <SEP>), and position embeddings. In our \n",
    "    model, we only need the token and position embedding so segment embeddign is not implemented here.    \n",
    "    \"\"\"\n",
    "    def __init__(self, num_embeddings, embedding_dim, dropout=0.1, max_len=1500, padding_idx=25):\n",
    "        super().__init__()\n",
    "        self.token_embedding = TokenEmbedding(num_embeddings, embedding_dim, max_len, padding_idx)\n",
    "        self.position = PositionalEncoding(embedding_dim, dropout, max_len)\n",
    "        self.embeddng_dim = embedding_dim\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def forward(self, seq:str):\n",
    "        x = tokenize_seq(seq, self.max_len)\n",
    "        x = self.token_embedding(x)\n",
    "        x = self.position(x)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Sequence and Position Embedding\n",
    "\n",
    "Let's test the embedding of the first 28 amino acids of the test sequence. Notice that position 2 and 4 are the same amino acid (F) yet they have different emedding in every dimension due to they appear at different positions. For simplicity, we only use 6 dimensions to embed the sequence. In the actual model, we will use many more dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1413"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_wt_seq = \"\"\">sp|P0DTC2|SPIKE_SARS2 Spike glycoprotein OS=Severe acute respiratory syndrome coronavirus 2 OX=2697049 GN=S PE=1 SV=1\n",
    "MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSSVLHSTQDLFLPFFS\n",
    "NVTWFHAIHVSGTNGTKRFDNPVLPFNDGVYFASTEKSNIIRGWIFGTTLDSKTQSLLIV\n",
    "NNATNVVIKVCEFQFCNDPFLGVYYHKNNKSWMESEFRVYSSANNCTFEYVSQPFLMDLE\n",
    "GKQGNFKNLREFVFKNIDGYFKIYSKHTPINLVRDLPQGFSALEPLVDLPIGINITRFQT\n",
    "LLALHRSYLTPGDSSSGWTAGAAAYYVGYLQPRTFLLKYNENGTITDAVDCALDPLSETK\n",
    "CTLKSFTVEKGIYQTSNFRVQPTESIVRFPNITNLCPFGEVFNATRFASVYAWNRKRISN\n",
    "CVADYSVLYNSASFSTFKCYGVSPTKLNDLCFTNVYADSFVIRGDEVRQIAPGQTGKIAD\n",
    "YNYKLPDDFTGCVIAWNSNNLDSKVGGNYNYLYRLFRKSNLKPFERDISTEIYQAGSTPC\n",
    "NGVEGFNCYFPLQSYGFQPTNGVGYQPYRVVVLSFELLHAPATVCGPKKSTNLVKNKCVN\n",
    "FNFNGLTGTGVLTESNKKFLPFQQFGRDIADTTDAVRDPQTLEILDITPCSFGGVSVITP\n",
    "GTNTSNQVAVLYQDVNCTEVPVAIHADQLTPTWRVYSTGSNVFQTRAGCLIGAEHVNNSY\n",
    "ECDIPIGAGICASYQTQTNSPRRARSVASQSIIAYTMSLGAENSVAYSNNSIAIPTNFTI\n",
    "SVTTEILPVSMTKTSVDCTMYICGDSTECSNLLLQYGSFCTQLNRALTGIAVEQDKNTQE\n",
    "VFAQVKQIYKTPPIKDFGGFNFSQILPDPSKPSKRSFIEDLLFNKVTLADAGFIKQYGDC\n",
    "LGDIAARDLICAQKFNGLTVLPPLLTDEMIAQYTSALLAGTITSGWTFGAGAALQIPFAM\n",
    "QMAYRFNGIGVTQNVLYENQKLIANQFNSAIGKIQDSLSSTASALGKLQDVVNQNAQALN\n",
    "TLVKQLSSNFGAISSVLNDILSRLDKVEAEVQIDRLITGRLQSLQTYVTQQLIRAAEIRA\n",
    "SANLAATKMSECVLGQSKRVDFCGKGYHLMSFPQSAPHGVVFLHVTYVPAQEKNFTTAPA\n",
    "ICHDGKAHFPREGVFVSNGTHWFVTQRNFYEPQIITTDNTFVSGNCDVVIGIVNNTVYDP\n",
    "LQPELDSFKEELDKYFKNHTSPDVDLGDISGINASVVNIQKEIDRLNEVAKNLNESLIDL\n",
    "QELGKYEQYIKWPWYIWLGFIAGLIAIVMVTIMLCCMTSCCSCLKGCCSCGSCCKFDEDD\n",
    "SEPVLKGVKLHYT\"\"\"\n",
    "len(test_wt_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seqs = []\n",
    "fa_parser = SeqIO.parse(io.StringIO(test_wt_seq), 'fasta')\n",
    "for record in fa_parser:\n",
    "    seq = record.seq\n",
    "    test_seqs.append(str(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: torch.Size([30, 6])\n",
      "Parameters shape in sequence embedding: torch.Size([26, 6])\n",
      "    embedding  dimension  position aa_token\n",
      "0    0.000000          0         0  <start>\n",
      "1    0.934968          0         1        M\n",
      "2    1.892376          0         2        F\n",
      "3    1.308922          0         3        V\n",
      "4   -0.000000          0         4        F\n",
      "..        ...        ...       ...      ...\n",
      "25   3.315263          5        25        P\n",
      "26   3.314970          5        26        P\n",
      "27   2.924207          5        27        A\n",
      "28   1.109090          5        28        Y\n",
      "29   2.387270          5        29    <end>\n",
      "\n",
      "[180 rows x 4 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-8ee0656ef4e746e2b1e240fafc1cac8d\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-8ee0656ef4e746e2b1e240fafc1cac8d\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-8ee0656ef4e746e2b1e240fafc1cac8d\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-df29cd2285ac495535ba0a6ccc730c1b\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"field\": \"dimension\", \"type\": \"nominal\"}, \"x\": {\"field\": \"position\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"embedding\", \"type\": \"quantitative\"}}, \"selection\": {\"selector021\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"width\": 800, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-df29cd2285ac495535ba0a6ccc730c1b\": [{\"embedding\": 0.0, \"dimension\": 0, \"position\": 0, \"aa_token\": \"<start>\"}, {\"embedding\": 0.9349677562713623, \"dimension\": 0, \"position\": 1, \"aa_token\": \"M\"}, {\"embedding\": 1.8923763036727905, \"dimension\": 0, \"position\": 2, \"aa_token\": \"F\"}, {\"embedding\": 1.3089220523834229, \"dimension\": 0, \"position\": 3, \"aa_token\": \"V\"}, {\"embedding\": -0.0, \"dimension\": 0, \"position\": 4, \"aa_token\": \"F\"}, {\"embedding\": -1.4886080026626587, \"dimension\": 0, \"position\": 5, \"aa_token\": \"L\"}, {\"embedding\": 0.26479411125183105, \"dimension\": 0, \"position\": 6, \"aa_token\": \"V\"}, {\"embedding\": 0.7299851179122925, \"dimension\": 0, \"position\": 7, \"aa_token\": \"L\"}, {\"embedding\": 0.0, \"dimension\": 0, \"position\": 8, \"aa_token\": \"L\"}, {\"embedding\": 3.7428882122039795, \"dimension\": 0, \"position\": 9, \"aa_token\": \"P\"}, {\"embedding\": -0.4584643542766571, \"dimension\": 0, \"position\": 10, \"aa_token\": \"L\"}, {\"embedding\": -1.5242873430252075, \"dimension\": 0, \"position\": 11, \"aa_token\": \"V\"}, {\"embedding\": 1.1031250953674316, \"dimension\": 0, \"position\": 12, \"aa_token\": \"S\"}, {\"embedding\": 3.4785702228546143, \"dimension\": 0, \"position\": 13, \"aa_token\": \"S\"}, {\"embedding\": 2.567631959915161, \"dimension\": 0, \"position\": 14, \"aa_token\": \"Q\"}, {\"embedding\": 2.2203190326690674, \"dimension\": 0, \"position\": 15, \"aa_token\": \"C\"}, {\"embedding\": 0.2437201589345932, \"dimension\": 0, \"position\": 16, \"aa_token\": \"V\"}, {\"embedding\": -1.041550874710083, \"dimension\": 0, \"position\": 17, \"aa_token\": \"N\"}, {\"embedding\": -0.9723308682441711, \"dimension\": 0, \"position\": 18, \"aa_token\": \"L\"}, {\"embedding\": 0.21443983912467957, \"dimension\": 0, \"position\": 19, \"aa_token\": \"T\"}, {\"embedding\": 2.1090259552001953, \"dimension\": 0, \"position\": 20, \"aa_token\": \"T\"}, {\"embedding\": 4.063875675201416, \"dimension\": 0, \"position\": 21, \"aa_token\": \"R\"}, {\"embedding\": -0.17965979874134064, \"dimension\": 0, \"position\": 22, \"aa_token\": \"T\"}, {\"embedding\": -1.992942214012146, \"dimension\": 0, \"position\": 23, \"aa_token\": \"Q\"}, {\"embedding\": -1.3561580181121826, \"dimension\": 0, \"position\": 24, \"aa_token\": \"L\"}, {\"embedding\": -0.14705751836299896, \"dimension\": 0, \"position\": 25, \"aa_token\": \"P\"}, {\"embedding\": 0.8472872376441956, \"dimension\": 0, \"position\": 26, \"aa_token\": \"P\"}, {\"embedding\": 0.9580433964729309, \"dimension\": 0, \"position\": 27, \"aa_token\": \"A\"}, {\"embedding\": 1.234464406967163, \"dimension\": 0, \"position\": 28, \"aa_token\": \"Y\"}, {\"embedding\": -1.927992820739746, \"dimension\": 0, \"position\": 29, \"aa_token\": \"<end>\"}, {\"embedding\": 3.9345877170562744, \"dimension\": 1, \"position\": 0, \"aa_token\": \"<start>\"}, {\"embedding\": 0.600335955619812, \"dimension\": 1, \"position\": 1, \"aa_token\": \"M\"}, {\"embedding\": -0.0, \"dimension\": 1, \"position\": 2, \"aa_token\": \"F\"}, {\"embedding\": -3.3695271015167236, \"dimension\": 1, \"position\": 3, \"aa_token\": \"V\"}, {\"embedding\": -1.0402026176452637, \"dimension\": 1, \"position\": 4, \"aa_token\": \"F\"}, {\"embedding\": 1.8797155618667603, \"dimension\": 1, \"position\": 5, \"aa_token\": \"L\"}, {\"embedding\": 1.47244131565094, \"dimension\": 1, \"position\": 6, \"aa_token\": \"V\"}, {\"embedding\": 3.047252655029297, \"dimension\": 1, \"position\": 7, \"aa_token\": \"L\"}, {\"embedding\": 0.8141687512397766, \"dimension\": 1, \"position\": 8, \"aa_token\": \"L\"}, {\"embedding\": -3.2745354175567627, \"dimension\": 1, \"position\": 9, \"aa_token\": \"P\"}, {\"embedding\": -0.9078676700592041, \"dimension\": 1, \"position\": 10, \"aa_token\": \"L\"}, {\"embedding\": -0.9005324244499207, \"dimension\": 1, \"position\": 11, \"aa_token\": \"V\"}, {\"embedding\": 0.0, \"dimension\": 1, \"position\": 12, \"aa_token\": \"S\"}, {\"embedding\": 1.0082743167877197, \"dimension\": 1, \"position\": 13, \"aa_token\": \"S\"}, {\"embedding\": -0.672337532043457, \"dimension\": 1, \"position\": 14, \"aa_token\": \"Q\"}, {\"embedding\": -1.6470260620117188, \"dimension\": 1, \"position\": 15, \"aa_token\": \"C\"}, {\"embedding\": -1.0640661716461182, \"dimension\": 1, \"position\": 16, \"aa_token\": \"V\"}, {\"embedding\": -2.018491744995117, \"dimension\": 1, \"position\": 17, \"aa_token\": \"N\"}, {\"embedding\": 2.8148934841156006, \"dimension\": 1, \"position\": 18, \"aa_token\": \"L\"}, {\"embedding\": 1.0985606908798218, \"dimension\": 1, \"position\": 19, \"aa_token\": \"T\"}, {\"embedding\": 0.5540808439254761, \"dimension\": 1, \"position\": 20, \"aa_token\": \"T\"}, {\"embedding\": -2.668552875518799, \"dimension\": 1, \"position\": 21, \"aa_token\": \"R\"}, {\"embedding\": -2.9418833255767822, \"dimension\": 1, \"position\": 22, \"aa_token\": \"T\"}, {\"embedding\": -2.334782123565674, \"dimension\": 1, \"position\": 23, \"aa_token\": \"Q\"}, {\"embedding\": 2.2285983562469482, \"dimension\": 1, \"position\": 24, \"aa_token\": \"L\"}, {\"embedding\": 1.448678731918335, \"dimension\": 1, \"position\": 25, \"aa_token\": \"P\"}, {\"embedding\": 0.5938732624053955, \"dimension\": 1, \"position\": 26, \"aa_token\": \"P\"}, {\"embedding\": -0.3245986998081207, \"dimension\": 1, \"position\": 27, \"aa_token\": \"A\"}, {\"embedding\": -1.0695621967315674, \"dimension\": 1, \"position\": 28, \"aa_token\": \"Y\"}, {\"embedding\": -1.3229897022247314, \"dimension\": 1, \"position\": 29, \"aa_token\": \"<end>\"}, {\"embedding\": -2.2501237392425537, \"dimension\": 2, \"position\": 0, \"aa_token\": \"<start>\"}, {\"embedding\": -0.36034825444221497, \"dimension\": 2, \"position\": 1, \"aa_token\": \"M\"}, {\"embedding\": 0.4166240096092224, \"dimension\": 2, \"position\": 2, \"aa_token\": \"F\"}, {\"embedding\": 0.1542201191186905, \"dimension\": 2, \"position\": 3, \"aa_token\": \"V\"}, {\"embedding\": 0.20510971546173096, \"dimension\": 2, \"position\": 4, \"aa_token\": \"F\"}, {\"embedding\": -1.569381594657898, \"dimension\": 2, \"position\": 5, \"aa_token\": \"L\"}, {\"embedding\": 1.2393361330032349, \"dimension\": 2, \"position\": 6, \"aa_token\": \"V\"}, {\"embedding\": -1.3478542566299438, \"dimension\": 2, \"position\": 7, \"aa_token\": \"L\"}, {\"embedding\": -1.239532709121704, \"dimension\": 2, \"position\": 8, \"aa_token\": \"L\"}, {\"embedding\": 2.7821614742279053, \"dimension\": 2, \"position\": 9, \"aa_token\": \"P\"}, {\"embedding\": -1.0289407968521118, \"dimension\": 2, \"position\": 10, \"aa_token\": \"L\"}, {\"embedding\": 1.770094394683838, \"dimension\": 2, \"position\": 11, \"aa_token\": \"V\"}, {\"embedding\": 1.4898695945739746, \"dimension\": 2, \"position\": 12, \"aa_token\": \"S\"}, {\"embedding\": 0.0, \"dimension\": 2, \"position\": 13, \"aa_token\": \"S\"}, {\"embedding\": 1.120253324508667, \"dimension\": 2, \"position\": 14, \"aa_token\": \"Q\"}, {\"embedding\": -1.0418578386306763, \"dimension\": 2, \"position\": 15, \"aa_token\": \"C\"}, {\"embedding\": 2.235795259475708, \"dimension\": 2, \"position\": 16, \"aa_token\": \"V\"}, {\"embedding\": 0.0, \"dimension\": 2, \"position\": 17, \"aa_token\": \"N\"}, {\"embedding\": -0.29910188913345337, \"dimension\": 2, \"position\": 18, \"aa_token\": \"L\"}, {\"embedding\": 0.8577214479446411, \"dimension\": 2, \"position\": 19, \"aa_token\": \"T\"}, {\"embedding\": 3.7650022506713867, \"dimension\": 2, \"position\": 20, \"aa_token\": \"T\"}, {\"embedding\": -0.29906460642814636, \"dimension\": 2, \"position\": 21, \"aa_token\": \"R\"}, {\"embedding\": 3.894348621368408, \"dimension\": 2, \"position\": 22, \"aa_token\": \"T\"}, {\"embedding\": 1.7930668592453003, \"dimension\": 2, \"position\": 23, \"aa_token\": \"Q\"}, {\"embedding\": 0.08782433718442917, \"dimension\": 2, \"position\": 24, \"aa_token\": \"L\"}, {\"embedding\": 4.051552772521973, \"dimension\": 2, \"position\": 25, \"aa_token\": \"P\"}, {\"embedding\": 4.095063209533691, \"dimension\": 2, \"position\": 26, \"aa_token\": \"P\"}, {\"embedding\": -0.03887759521603584, \"dimension\": 2, \"position\": 27, \"aa_token\": \"A\"}, {\"embedding\": 2.5770130157470703, \"dimension\": 2, \"position\": 28, \"aa_token\": \"Y\"}, {\"embedding\": 1.8489305973052979, \"dimension\": 2, \"position\": 29, \"aa_token\": \"<end>\"}, {\"embedding\": 2.1022987365722656, \"dimension\": 3, \"position\": 0, \"aa_token\": \"<start>\"}, {\"embedding\": -0.22443626821041107, \"dimension\": 3, \"position\": 1, \"aa_token\": \"M\"}, {\"embedding\": 3.922233819961548, \"dimension\": 3, \"position\": 2, \"aa_token\": \"F\"}, {\"embedding\": 1.1003563404083252, \"dimension\": 3, \"position\": 3, \"aa_token\": \"V\"}, {\"embedding\": 0.0, \"dimension\": 3, \"position\": 4, \"aa_token\": \"F\"}, {\"embedding\": 2.9543511867523193, \"dimension\": 3, \"position\": 5, \"aa_token\": \"L\"}, {\"embedding\": 1.0683002471923828, \"dimension\": 3, \"position\": 6, \"aa_token\": \"V\"}, {\"embedding\": 2.8910107612609863, \"dimension\": 3, \"position\": 7, \"aa_token\": \"L\"}, {\"embedding\": 2.851701259613037, \"dimension\": 3, \"position\": 8, \"aa_token\": \"L\"}, {\"embedding\": 1.8441617488861084, \"dimension\": 3, \"position\": 9, \"aa_token\": \"P\"}, {\"embedding\": 0.9935538172721863, \"dimension\": 3, \"position\": 10, \"aa_token\": \"L\"}, {\"embedding\": 2.035470485687256, \"dimension\": 3, \"position\": 11, \"aa_token\": \"V\"}, {\"embedding\": 2.975121021270752, \"dimension\": 3, \"position\": 12, \"aa_token\": \"S\"}, {\"embedding\": 0.0, \"dimension\": 3, \"position\": 13, \"aa_token\": \"S\"}, {\"embedding\": -0.30618563294410706, \"dimension\": 3, \"position\": 14, \"aa_token\": \"Q\"}, {\"embedding\": 1.1799339056015015, \"dimension\": 3, \"position\": 15, \"aa_token\": \"C\"}, {\"embedding\": 1.6983298063278198, \"dimension\": 3, \"position\": 16, \"aa_token\": \"V\"}, {\"embedding\": 3.8931238651275635, \"dimension\": 3, \"position\": 17, \"aa_token\": \"N\"}, {\"embedding\": 2.203603744506836, \"dimension\": 3, \"position\": 18, \"aa_token\": \"L\"}, {\"embedding\": 1.2037283182144165, \"dimension\": 3, \"position\": 19, \"aa_token\": \"T\"}, {\"embedding\": 1.1130976676940918, \"dimension\": 3, \"position\": 20, \"aa_token\": \"T\"}, {\"embedding\": 2.0399296283721924, \"dimension\": 3, \"position\": 21, \"aa_token\": \"R\"}, {\"embedding\": 0.9224256277084351, \"dimension\": 3, \"position\": 22, \"aa_token\": \"T\"}, {\"embedding\": -1.0856350660324097, \"dimension\": 3, \"position\": 23, \"aa_token\": \"Q\"}, {\"embedding\": 1.6332290172576904, \"dimension\": 3, \"position\": 24, \"aa_token\": \"L\"}, {\"embedding\": 0.5654134750366211, \"dimension\": 3, \"position\": 25, \"aa_token\": \"P\"}, {\"embedding\": 0.4587101936340332, \"dimension\": 3, \"position\": 26, \"aa_token\": \"P\"}, {\"embedding\": -1.0741682052612305, \"dimension\": 3, \"position\": 27, \"aa_token\": \"A\"}, {\"embedding\": 2.3116581439971924, \"dimension\": 3, \"position\": 28, \"aa_token\": \"Y\"}, {\"embedding\": 0.24760942161083221, \"dimension\": 3, \"position\": 29, \"aa_token\": \"<end>\"}, {\"embedding\": 2.1940126419067383, \"dimension\": 4, \"position\": 0, \"aa_token\": \"<start>\"}, {\"embedding\": 0.0023938147351145744, \"dimension\": 4, \"position\": 1, \"aa_token\": \"M\"}, {\"embedding\": 0.44859349727630615, \"dimension\": 4, \"position\": 2, \"aa_token\": \"F\"}, {\"embedding\": -1.6702461242675781, \"dimension\": 4, \"position\": 3, \"aa_token\": \"V\"}, {\"embedding\": 0.45929163694381714, \"dimension\": 4, \"position\": 4, \"aa_token\": \"F\"}, {\"embedding\": 1.358559012413025, \"dimension\": 4, \"position\": 5, \"aa_token\": \"L\"}, {\"embedding\": -1.6541996002197266, \"dimension\": 4, \"position\": 6, \"aa_token\": \"V\"}, {\"embedding\": 1.3692563772201538, \"dimension\": 4, \"position\": 7, \"aa_token\": \"L\"}, {\"embedding\": 1.3746049404144287, \"dimension\": 4, \"position\": 8, \"aa_token\": \"L\"}, {\"embedding\": -0.2593686580657959, \"dimension\": 4, \"position\": 9, \"aa_token\": \"P\"}, {\"embedding\": 1.3853009939193726, \"dimension\": 4, \"position\": 10, \"aa_token\": \"L\"}, {\"embedding\": -1.6274585723876953, \"dimension\": 4, \"position\": 11, \"aa_token\": \"V\"}, {\"embedding\": -0.8489443063735962, \"dimension\": 4, \"position\": 12, \"aa_token\": \"S\"}, {\"embedding\": -0.8435971140861511, \"dimension\": 4, \"position\": 13, \"aa_token\": \"S\"}, {\"embedding\": -0.1622476875782013, \"dimension\": 4, \"position\": 14, \"aa_token\": \"Q\"}, {\"embedding\": -0.7994768619537354, \"dimension\": 4, \"position\": 15, \"aa_token\": \"C\"}, {\"embedding\": -1.6007241010665894, \"dimension\": 4, \"position\": 16, \"aa_token\": \"V\"}, {\"embedding\": -3.01717209815979, \"dimension\": 4, \"position\": 17, \"aa_token\": \"N\"}, {\"embedding\": 1.4280743598937988, \"dimension\": 4, \"position\": 18, \"aa_token\": \"L\"}, {\"embedding\": -0.0, \"dimension\": 4, \"position\": 19, \"aa_token\": \"T\"}, {\"embedding\": -1.292169451713562, \"dimension\": 4, \"position\": 20, \"aa_token\": \"T\"}, {\"embedding\": 0.050252996385097504, \"dimension\": 4, \"position\": 21, \"aa_token\": \"R\"}, {\"embedding\": -0.0, \"dimension\": 4, \"position\": 22, \"aa_token\": \"T\"}, {\"embedding\": -0.11414435505867004, \"dimension\": 4, \"position\": 23, \"aa_token\": \"Q\"}, {\"embedding\": 1.46013605594635, \"dimension\": 4, \"position\": 24, \"aa_token\": \"L\"}, {\"embedding\": -0.0, \"dimension\": 4, \"position\": 25, \"aa_token\": \"P\"}, {\"embedding\": -0.16850288212299347, \"dimension\": 4, \"position\": 26, \"aa_token\": \"P\"}, {\"embedding\": 0.0, \"dimension\": 4, \"position\": 27, \"aa_token\": \"A\"}, {\"embedding\": 2.0600523948669434, \"dimension\": 4, \"position\": 28, \"aa_token\": \"Y\"}, {\"embedding\": 0.06937552243471146, \"dimension\": 4, \"position\": 29, \"aa_token\": \"<end>\"}, {\"embedding\": 3.115269422531128, \"dimension\": 5, \"position\": 0, \"aa_token\": \"<start>\"}, {\"embedding\": 1.1111085414886475, \"dimension\": 5, \"position\": 1, \"aa_token\": \"M\"}, {\"embedding\": 1.1111007928848267, \"dimension\": 5, \"position\": 2, \"aa_token\": \"F\"}, {\"embedding\": 2.6465766429901123, \"dimension\": 5, \"position\": 3, \"aa_token\": \"V\"}, {\"embedding\": 4.380305767059326, \"dimension\": 5, \"position\": 4, \"aa_token\": \"F\"}, {\"embedding\": -0.11764004081487656, \"dimension\": 5, \"position\": 5, \"aa_token\": \"L\"}, {\"embedding\": 2.646421194076538, \"dimension\": 5, \"position\": 6, \"aa_token\": \"V\"}, {\"embedding\": 1.1109848022460938, \"dimension\": 5, \"position\": 7, \"aa_token\": \"L\"}, {\"embedding\": -0.11786487698554993, \"dimension\": 5, \"position\": 8, \"aa_token\": \"L\"}, {\"embedding\": 3.318396806716919, \"dimension\": 5, \"position\": 9, \"aa_token\": \"P\"}, {\"embedding\": -0.11807243525981903, \"dimension\": 5, \"position\": 10, \"aa_token\": \"L\"}, {\"embedding\": 2.6459317207336426, \"dimension\": 5, \"position\": 11, \"aa_token\": \"V\"}, {\"embedding\": 2.7782907485961914, \"dimension\": 5, \"position\": 12, \"aa_token\": \"S\"}, {\"embedding\": 2.778146743774414, \"dimension\": 5, \"position\": 13, \"aa_token\": \"S\"}, {\"embedding\": 2.483259916305542, \"dimension\": 5, \"position\": 14, \"aa_token\": \"Q\"}, {\"embedding\": 0.0, \"dimension\": 5, \"position\": 15, \"aa_token\": \"C\"}, {\"embedding\": 2.645153522491455, \"dimension\": 5, \"position\": 16, \"aa_token\": \"V\"}, {\"embedding\": 2.672719717025757, \"dimension\": 5, \"position\": 17, \"aa_token\": \"N\"}, {\"embedding\": -0.11936280876398087, \"dimension\": 5, \"position\": 18, \"aa_token\": \"L\"}, {\"embedding\": 1.4214164018630981, \"dimension\": 5, \"position\": 19, \"aa_token\": \"T\"}, {\"embedding\": 1.4211918115615845, \"dimension\": 5, \"position\": 20, \"aa_token\": \"T\"}, {\"embedding\": 2.5238356590270996, \"dimension\": 5, \"position\": 21, \"aa_token\": \"R\"}, {\"embedding\": 1.10986328125, \"dimension\": 5, \"position\": 22, \"aa_token\": \"T\"}, {\"embedding\": 2.4813413619995117, \"dimension\": 5, \"position\": 23, \"aa_token\": \"Q\"}, {\"embedding\": -0.12081445008516312, \"dimension\": 5, \"position\": 24, \"aa_token\": \"L\"}, {\"embedding\": 3.315263032913208, \"dimension\": 5, \"position\": 25, \"aa_token\": \"P\"}, {\"embedding\": 3.314969539642334, \"dimension\": 5, \"position\": 26, \"aa_token\": \"P\"}, {\"embedding\": 2.9242072105407715, \"dimension\": 5, \"position\": 27, \"aa_token\": \"A\"}, {\"embedding\": 1.1090900897979736, \"dimension\": 5, \"position\": 28, \"aa_token\": \"Y\"}, {\"embedding\": 2.387270212173462, \"dimension\": 5, \"position\": 29, \"aa_token\": \"<end>\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_encoding():\n",
    "\n",
    "    max_len = 30\n",
    "    seq = test_seqs[0][:max_len-2]\n",
    "    test_seq_encode = SeqEncoding(n_tokens, 6, 0.1, max_len, padding_idx)\n",
    "    test_pe_encode = PositionalEncoding(6, 0.1, max_len)\n",
    "    y = test_pe_encode.forward(test_seq_encode(seq))\n",
    "    print(f'Embedding shape: {y.shape}')\n",
    "    print(f'Parameters shape in sequence embedding: {test_seq_encode.token_embedding.weight.shape}')\n",
    "\n",
    "    y = y.detach().numpy()\n",
    "\n",
    "\n",
    "    data = pd.concat([pd.DataFrame({\n",
    "        \"embedding\": y[:, dim],\n",
    "        \"dimension\":dim,\n",
    "        \"position\": list(range(max_len)),\n",
    "        })for dim in range(6)])\n",
    "    \n",
    "    aa = ['<start>']\n",
    "    aa.extend(_ for _ in seq)\n",
    "    aa.append('<end>')\n",
    "    data['aa_token'] = aa * 6\n",
    "\n",
    "    print(data)\n",
    "\n",
    "    return (\n",
    "        alt.Chart(data)\n",
    "        .mark_line()\n",
    "        .properties(width=800)\n",
    "        .encode(x=\"position\", y=\"embedding\", color=\"dimension:N\")\n",
    "        .interactive())\n",
    "\n",
    "test_encoding()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"Single head scaled dot product attention\"\"\"\n",
    "    def __init__():\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, query, key, value, mask=None, dropout=None):\n",
    "        d_k = query.size(-1)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)    #sqrt(d_k) is the scaling factor.\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask==0, -1e9)\n",
    "        \n",
    "        p_attn = scores.softmax(dim=-1)\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "    \n",
    "        return torch.matmul(p_attn, value), p_attn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention\n",
    "\n",
    "    h: numer of heads\n",
    "    d_model: model size\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, h:int, d_model:int, n_linear: int=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % h == 0 # d_model/h is used as d_k and d_v\n",
    "\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = nn.ModuleList([nn.Linear(d_model, d_model), n_linear])  # n layers of linear model with the same input and output size\n",
    "        self.output_linear = nn.Linear(d_model, d_model)    # Output lienar model. This implementation follows BERT-pytorch instead of using the last linear layer, which is found in the annotated transformer.\n",
    "        self.attn = Attention() # The forward function in Attention class is called since no hooks are defined in Attention class. See __call__() and _call_impl() in nn.Module implementation.\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)    # same mask applied to all heads\n",
    "        n_batches = query.size(0)\n",
    "\n",
    "        # 1) Linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [lin(x).view(n_batches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "                             for lin, x in zip(self.linears, (query, key, value))]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch\n",
    "        x, self.attn = self.attention(query, key, value, mask=mask, dropout = self.dropout)\n",
    "\n",
    "        # 3) \"Concat using a view and apply a final linear\"\n",
    "        x = (x.transpose(1, 2)\n",
    "             .contiguous()\n",
    "             .view(n_batches, -1, self.h * self.d_k))\n",
    "        \n",
    "        del query\n",
    "        del key\n",
    "        del value\n",
    "        return self.output_linear(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Normalization\n",
    "\n",
    "Linear regression based layer normalization with parameters a_2 and b_2. An arbituary small value (epsilon or eps) is added to std to avoid the error when std is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"Construct a layernorm module\"\"\"\n",
    "\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x-mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positionwise Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"Implements FFN equation.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.activation(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (3359644668.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_13491/3359644668.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class Transformer:\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition\n",
    "\n",
    "Here we define a model based on BERT. Part of the implementation is based on [BERT-pytorch](https://github.com/codertimo/BERT-pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"\"\"Produce N identical layers\"\"\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocabl_size: int = 26, hidden: int = 768, n_layer: int = 12, attn_heads: int = 12, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        vacab_size: vacabulary or token size\n",
    "        hidden: BERT model size (used as input size and hidden size)\n",
    "        n_layer: number of Transformer layers\n",
    "        attn_heads: attenion heads\n",
    "        dropout: dropout ratio\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.hidden  = hidden\n",
    "        self.n_layer = n_layer\n",
    "        self.attn_heads = attn_heads\n",
    "\n",
    "        self.feed_forward_hidden = hidden * 4\n",
    "        self.embedding = TokenEmbedding(vocabl_size, embed_size=hidden, padding_idx=25)\n",
    "\n",
    "        self.transformer_blocks = clones(Transformer(hidden, attn_heads, hidden *4, dropout), n_layer)\n",
    "\n",
    "    def forward(self)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_parameters_seq_encoding = sum(p.numel() for p in test_seq_encode.parameters() if p.requires_grad)\n",
    "#print(f'Parameters in SeqEncoding: {num_parameters_seq_encoding}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f28ad5bbb02419dfa4fc8e8c46da9bd89100d91bb344ce9fd3a3101abf33cb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
