{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spike NLP\n",
    "\n",
    "We wish to utilize NLP methods to analyze the virus protein sequences. After initial experiment with the LSTM architecture, we decided to use Transformer architecture. In this notebook, we implement a BERT model. During the process, we learned from existing implementations of BERT, especially [BERT-pytorch](https://github.com/codertimo/BERT-pytorch) and [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/) and [ProteinBERT](https://academic.oup.com/bioinformatics/article/38/8/2102/6502274), though we have to make changes to accomodate our own research interests. For example, we are interested in next word prediction in the pre-training of the model to generate contexualized embeddings using self-supervised learning at individual amino acid level through learning the language patterns but we are not interested in protein functional annotation, so we do not use annotation in our model as a pre-training task. In other words, we are interested in leveraing the Mask LM pre-training task to derive the embeddings for a fine-tuning model to  predict the phenotype of the virus protein sequences. Example phenotypes are bind binding kinetics of the virus protein to target receptor proteins and antibodies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Embedding\n",
    "In the BERT implemetnation (bert_pytorch/model/bert.py), the masking is done after the second token (x>0) since in the original BERT paper, the first element of the input is always \\[CLS\\]. In our model, we will use the variant name as the \\[CLS\\] and the values are:\n",
    "[wt, alpha, delta, omicron, na], where \"na\" stands for not assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import copy\n",
    "import math\n",
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.optim import Adam\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import sqlite3\n",
    "import tqdm\n",
    "import random\n",
    "# from bert_pytorch.model import BERT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Vocabulary\n",
    "In [ProteinBERT](https://academic.oup.com/bioinformatics/article/38/8/2102/6502274), Brandes et al used 26 unique tokens to represent the 20 standard amino acids, selenocysteine (U), and undefined amino acid (X), another amino acid (OTHER) and three speical tokens \\<START\\>, \\<END\\>, \\<PAD\\>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Token  Index\n",
      "0             A      0\n",
      "1             C      1\n",
      "2             D      2\n",
      "3             E      3\n",
      "4             F      4\n",
      "5             G      5\n",
      "6             H      6\n",
      "7             I      7\n",
      "8             K      8\n",
      "9             L      9\n",
      "10            M     10\n",
      "11            N     11\n",
      "12            P     12\n",
      "13            Q     13\n",
      "14            R     14\n",
      "15            S     15\n",
      "16            T     16\n",
      "17            U     17\n",
      "18            V     18\n",
      "19            W     19\n",
      "20            X     20\n",
      "21            Y     21\n",
      "22      <OTHER>     22\n",
      "23      <START>     23\n",
      "24        <END>     24\n",
      "25        <PAD>     25\n",
      "26       <MASK>     26\n",
      "27  <TRUNCATED>     27\n"
     ]
    }
   ],
   "source": [
    "# Based on the source code of protein_bert\n",
    "# \"<TRUNCATED>\" is used if the longest sequence in a batch is longer than the maximum length\n",
    "# used as inputs (defined as max_len in tokenize_seq). \"<MASK>\" is used in masked language model.\n",
    "ALL_AAS = 'ACDEFGHIKLMNPQRSTUVWXY'\n",
    "ADDITIONAL_TOKENS = ['<OTHER>', '<START>', '<END>', '<PAD>', '<MASK>', '<TRUNCATED>']\n",
    "\n",
    "# Each sequence is added <START> and <END>. \"<PAD>\" are added to sequence shorten than max_len.\n",
    "ADDED_TOKENS_PER_SEQ = 2\n",
    "\n",
    "n_aas = len(ALL_AAS)\n",
    "aa_to_token_index = {aa: i for i, aa in enumerate(ALL_AAS)}\n",
    "additional_token_to_index = {token: i + n_aas for i, token in enumerate(ADDITIONAL_TOKENS)}\n",
    "\n",
    "token_to_index = {**aa_to_token_index, **additional_token_to_index}\n",
    "index_to_token = {index: token for token, index in token_to_index.items()}\n",
    "n_tokens = len(token_to_index)\n",
    "\n",
    "def tokenize_seq(seq: str, max_len:int=1500) -> torch.IntTensor:\n",
    "    \"\"\"\n",
    "    Tokenize a sequence.\n",
    "\n",
    "    It is the caller's responsibility to infer the maximum length of the input. In case of\n",
    "    tokenizing a batch of sequences, the maximum length shall be assigned to the lenght of\n",
    "    the longest sequence in the same batch. \n",
    "\n",
    "\n",
    "    seq: input insquence\n",
    "    max_len: maximum number of tokens, including the special tokens such as <START>, <END>.\n",
    "    \n",
    "    \"\"\"\n",
    "    seq = seq.upper()   # All in upper case.\n",
    "    other_token_index = additional_token_to_index['<OTHER>']\n",
    "    token_seq = [additional_token_to_index['<START>']] + [aa_to_token_index.get(aa, other_token_index) for aa in seq]\n",
    "    if len(token_seq) < max_len - 1: # -1 is for the <END> token\n",
    "        n_pads = max_len -1 - len(token_seq)\n",
    "        token_seq.extend(token_to_index['<PAD>'] for _ in range(n_pads))\n",
    "    token_seq += [additional_token_to_index['<END>']]\n",
    "    return torch.IntTensor(token_seq)\n",
    "token_df = pd.DataFrame(token_to_index.items(), columns=['Token', 'Index'])\n",
    "print(token_df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amino Acid Token Embeddings\n",
    "We will derive token embedding from the [torch.nn.Embedding class](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html). The size of the vacabulary equals the number of tokens. This approach allows the learning of the embeddings from the model intself. If we train the model with virus sepcific squences, the embeddings shall reflect the hidden properties of the amino acids in context of the trainign sequences. Note that the \\<START\\> and \\<END\\> tokens are always added at the beginning of the sequence. \\<PAD\\> tokens may be added before the \\<END\\> token if the sequence is shorter than the input sequence.\n",
    "\n",
    "Note that using the \"from_pretrained\" class method of torch.nn.Embedding, we can load pre-trained weights of the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Embedding):\n",
    "    \"\"\"Token embedding\"\"\"\n",
    "    def __init__(self, vocab_size: int,\n",
    "                 embedding_dim: int=512,\n",
    "                 padding_idx=None):\n",
    "        super().__init__(vocab_size, embedding_dim, padding_idx)\n",
    "\n",
    "padding_idx = token_to_index['<PAD>']\n",
    "\n",
    "#TODO: add support to load pre-trained embeddings.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postional Encoding\n",
    "We will use the  sine and cosine functions of different frequencie to embed positional information as in the original BERT method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Impement the PE function.\n",
    "    \n",
    "    The PE forward function is different from the BERT-pytorch. Here we used the original method in BERT so\n",
    "    PE embeddings are added to the input embeddings and no graident tracking is used.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model: int,       # model input dimension\n",
    "                 dropout: float=0.1, # dropout rate\n",
    "                 max_len=1500):      # maximum sequence length #TODO: need a truncation and the <truckated> token.\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0)/d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # adding positional embeddings on top of original embedding\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Encode amino acid sequence. Input sequence is represented by summing the corresponding sequence token,\n",
    "    segment (e.g. question and answer or any segments separated by <SEP>), and position embeddings. In our \n",
    "    model, we only need the token and position embedding so segment embeddign is not implemented here.    \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 vocab_size: int,       # vocabulary size\n",
    "                 embedding_dim: int,    # embedding dimensions\n",
    "                 dropout: float=0.1,    # dropout rate\n",
    "                 max_len: int=1500,     # maximum length of input sequence\n",
    "                 padding_idx: int=25):  # padding token index\n",
    "        super().__init__()\n",
    "        self.token_embedding = TokenEmbedding(vocab_size, embedding_dim, padding_idx)\n",
    "        self.add_position = PositionalEncoding(embedding_dim, dropout, max_len)\n",
    "        self.embeddng_dim = embedding_dim\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def forward(self, seq:str):\n",
    "        x = tokenize_seq(seq, self.max_len)\n",
    "        x = self.token_embedding(x)\n",
    "        print(x.shape)\n",
    "        x = self.add_position(x)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Sequence and Position Embedding\n",
    "\n",
    "Let's test the embedding of the first 28 amino acids of the test sequence. Notice that position 2 and 4 are the same amino acid (F) yet they have different emedding in every dimension due to they appear at different positions. For simplicity, we only use 6 dimensions to embed the sequence. In the actual model, we will use many more dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1413"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_wt_seq = \"\"\">sp|P0DTC2|SPIKE_SARS2 Spike glycoprotein OS=Severe acute respiratory syndrome coronavirus 2 OX=2697049 GN=S PE=1 SV=1\n",
    "MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSSVLHSTQDLFLPFFS\n",
    "NVTWFHAIHVSGTNGTKRFDNPVLPFNDGVYFASTEKSNIIRGWIFGTTLDSKTQSLLIV\n",
    "NNATNVVIKVCEFQFCNDPFLGVYYHKNNKSWMESEFRVYSSANNCTFEYVSQPFLMDLE\n",
    "GKQGNFKNLREFVFKNIDGYFKIYSKHTPINLVRDLPQGFSALEPLVDLPIGINITRFQT\n",
    "LLALHRSYLTPGDSSSGWTAGAAAYYVGYLQPRTFLLKYNENGTITDAVDCALDPLSETK\n",
    "CTLKSFTVEKGIYQTSNFRVQPTESIVRFPNITNLCPFGEVFNATRFASVYAWNRKRISN\n",
    "CVADYSVLYNSASFSTFKCYGVSPTKLNDLCFTNVYADSFVIRGDEVRQIAPGQTGKIAD\n",
    "YNYKLPDDFTGCVIAWNSNNLDSKVGGNYNYLYRLFRKSNLKPFERDISTEIYQAGSTPC\n",
    "NGVEGFNCYFPLQSYGFQPTNGVGYQPYRVVVLSFELLHAPATVCGPKKSTNLVKNKCVN\n",
    "FNFNGLTGTGVLTESNKKFLPFQQFGRDIADTTDAVRDPQTLEILDITPCSFGGVSVITP\n",
    "GTNTSNQVAVLYQDVNCTEVPVAIHADQLTPTWRVYSTGSNVFQTRAGCLIGAEHVNNSY\n",
    "ECDIPIGAGICASYQTQTNSPRRARSVASQSIIAYTMSLGAENSVAYSNNSIAIPTNFTI\n",
    "SVTTEILPVSMTKTSVDCTMYICGDSTECSNLLLQYGSFCTQLNRALTGIAVEQDKNTQE\n",
    "VFAQVKQIYKTPPIKDFGGFNFSQILPDPSKPSKRSFIEDLLFNKVTLADAGFIKQYGDC\n",
    "LGDIAARDLICAQKFNGLTVLPPLLTDEMIAQYTSALLAGTITSGWTFGAGAALQIPFAM\n",
    "QMAYRFNGIGVTQNVLYENQKLIANQFNSAIGKIQDSLSSTASALGKLQDVVNQNAQALN\n",
    "TLVKQLSSNFGAISSVLNDILSRLDKVEAEVQIDRLITGRLQSLQTYVTQQLIRAAEIRA\n",
    "SANLAATKMSECVLGQSKRVDFCGKGYHLMSFPQSAPHGVVFLHVTYVPAQEKNFTTAPA\n",
    "ICHDGKAHFPREGVFVSNGTHWFVTQRNFYEPQIITTDNTFVSGNCDVVIGIVNNTVYDP\n",
    "LQPELDSFKEELDKYFKNHTSPDVDLGDISGINASVVNIQKEIDRLNEVAKNLNESLIDL\n",
    "QELGKYEQYIKWPWYIWLGFIAGLIAIVMVTIMLCCMTSCCSCLKGCCSCGSCCKFDEDD\n",
    "SEPVLKGVKLHYT\"\"\"\n",
    "len(test_wt_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seqs = []\n",
    "fa_parser = SeqIO.parse(io.StringIO(test_wt_seq), 'fasta')\n",
    "for record in fa_parser:\n",
    "    seq = record.seq\n",
    "    test_seqs.append(str(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 6])\n",
      "Embedding shape: torch.Size([30, 6])\n",
      "Parameters shape in sequence embedding: torch.Size([28, 6])\n",
      "    embedding  dimension  position aa_token\n",
      "0    0.483710          0         0  <start>\n",
      "1   -1.105737          0         1        M\n",
      "2    5.332335          0         2        F\n",
      "3    0.156800          0         3        V\n",
      "4    1.195654          0         4        F\n",
      "..        ...        ...       ...      ...\n",
      "25   1.109500          5        25        P\n",
      "26   2.015575          5        26        P\n",
      "27   3.483103          5        27        A\n",
      "28   2.945545          5        28        Y\n",
      "29   3.421899          5        29    <end>\n",
      "\n",
      "[180 rows x 4 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-f7e3975ab9034e43bf64b2df5280897e\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-f7e3975ab9034e43bf64b2df5280897e\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-f7e3975ab9034e43bf64b2df5280897e\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-61ac48132dd37e72a19111eb863fac84\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"field\": \"dimension\", \"type\": \"nominal\"}, \"x\": {\"field\": \"position\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"embedding\", \"type\": \"quantitative\"}}, \"selection\": {\"selector001\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"width\": 800, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-61ac48132dd37e72a19111eb863fac84\": [{\"embedding\": 0.4837101399898529, \"dimension\": 0, \"position\": 0, \"aa_token\": \"<start>\"}, {\"embedding\": -1.1057369709014893, \"dimension\": 0, \"position\": 1, \"aa_token\": \"M\"}, {\"embedding\": 5.332335472106934, \"dimension\": 0, \"position\": 2, \"aa_token\": \"F\"}, {\"embedding\": 0.1568000167608261, \"dimension\": 0, \"position\": 3, \"aa_token\": \"V\"}, {\"embedding\": 1.1956535577774048, \"dimension\": 0, \"position\": 4, \"aa_token\": \"F\"}, {\"embedding\": -4.629916191101074, \"dimension\": 0, \"position\": 5, \"aa_token\": \"L\"}, {\"embedding\": -0.0, \"dimension\": 0, \"position\": 6, \"aa_token\": \"V\"}, {\"embedding\": 0.7299851179122925, \"dimension\": 0, \"position\": 7, \"aa_token\": \"L\"}, {\"embedding\": 0.20738397538661957, \"dimension\": 0, \"position\": 8, \"aa_token\": \"L\"}, {\"embedding\": 1.0970954895019531, \"dimension\": 0, \"position\": 9, \"aa_token\": \"P\"}, {\"embedding\": -3.5997724533081055, \"dimension\": 0, \"position\": 10, \"aa_token\": \"L\"}, {\"embedding\": -3.755889415740967, \"dimension\": 0, \"position\": 11, \"aa_token\": \"V\"}, {\"embedding\": -0.5974273681640625, \"dimension\": 0, \"position\": 12, \"aa_token\": \"S\"}, {\"embedding\": 1.7780177593231201, \"dimension\": 0, \"position\": 13, \"aa_token\": \"S\"}, {\"embedding\": 0.6279613375663757, \"dimension\": 0, \"position\": 14, \"aa_token\": \"Q\"}, {\"embedding\": 2.690913438796997, \"dimension\": 0, \"position\": 15, \"aa_token\": \"C\"}, {\"embedding\": -1.9878818988800049, \"dimension\": 0, \"position\": 16, \"aa_token\": \"V\"}, {\"embedding\": -0.7140980958938599, \"dimension\": 0, \"position\": 17, \"aa_token\": \"N\"}, {\"embedding\": -4.113638877868652, \"dimension\": 0, \"position\": 18, \"aa_token\": \"L\"}, {\"embedding\": -2.009434700012207, \"dimension\": 0, \"position\": 19, \"aa_token\": \"T\"}, {\"embedding\": -0.11484862118959427, \"dimension\": 0, \"position\": 20, \"aa_token\": \"T\"}, {\"embedding\": 0.08139146864414215, \"dimension\": 0, \"position\": 21, \"aa_token\": \"R\"}, {\"embedding\": -2.403534412384033, \"dimension\": 0, \"position\": 22, \"aa_token\": \"T\"}, {\"embedding\": -3.932612895965576, \"dimension\": 0, \"position\": 23, \"aa_token\": \"Q\"}, {\"embedding\": -4.49746561050415, \"dimension\": 0, \"position\": 24, \"aa_token\": \"L\"}, {\"embedding\": -0.25474438071250916, \"dimension\": 0, \"position\": 25, \"aa_token\": \"P\"}, {\"embedding\": 0.8472872376441956, \"dimension\": 0, \"position\": 26, \"aa_token\": \"P\"}, {\"embedding\": 1.0626399517059326, \"dimension\": 0, \"position\": 27, \"aa_token\": \"A\"}, {\"embedding\": -2.1875624656677246, \"dimension\": 0, \"position\": 28, \"aa_token\": \"Y\"}, {\"embedding\": 0.0388575941324234, \"dimension\": 0, \"position\": 29, \"aa_token\": \"<end>\"}, {\"embedding\": 1.1111111640930176, \"dimension\": 1, \"position\": 0, \"aa_token\": \"<start>\"}, {\"embedding\": 0.0, \"dimension\": 1, \"position\": 1, \"aa_token\": \"M\"}, {\"embedding\": 0.520219624042511, \"dimension\": 1, \"position\": 2, \"aa_token\": \"F\"}, {\"embedding\": -2.2404613494873047, \"dimension\": 1, \"position\": 3, \"aa_token\": \"V\"}, {\"embedding\": -0.7262707352638245, \"dimension\": 1, \"position\": 4, \"aa_token\": \"F\"}, {\"embedding\": -0.19150063395500183, \"dimension\": 1, \"position\": 5, \"aa_token\": \"L\"}, {\"embedding\": 1.0668559074401855, \"dimension\": 1, \"position\": 6, \"aa_token\": \"V\"}, {\"embedding\": 0.976036548614502, \"dimension\": 1, \"position\": 7, \"aa_token\": \"L\"}, {\"embedding\": -0.16166670620441437, \"dimension\": 1, \"position\": 8, \"aa_token\": \"L\"}, {\"embedding\": -1.0123670101165771, \"dimension\": 1, \"position\": 9, \"aa_token\": \"P\"}, {\"embedding\": -0.9323017001152039, \"dimension\": 1, \"position\": 10, \"aa_token\": \"L\"}, {\"embedding\": 0.22853344678878784, \"dimension\": 1, \"position\": 11, \"aa_token\": \"V\"}, {\"embedding\": 0.0, \"dimension\": 1, \"position\": 12, \"aa_token\": \"S\"}, {\"embedding\": 0.0, \"dimension\": 1, \"position\": 13, \"aa_token\": \"S\"}, {\"embedding\": 0.15193024277687073, \"dimension\": 1, \"position\": 14, \"aa_token\": \"Q\"}, {\"embedding\": -0.9616778492927551, \"dimension\": 1, \"position\": 15, \"aa_token\": \"C\"}, {\"embedding\": -2.1601831912994385, \"dimension\": 1, \"position\": 16, \"aa_token\": \"V\"}, {\"embedding\": -1.841757893562317, \"dimension\": 1, \"position\": 17, \"aa_token\": \"N\"}, {\"embedding\": 0.7436773180961609, \"dimension\": 1, \"position\": 18, \"aa_token\": \"L\"}, {\"embedding\": 1.0985606908798218, \"dimension\": 1, \"position\": 19, \"aa_token\": \"T\"}, {\"embedding\": 2.063770294189453, \"dimension\": 1, \"position\": 20, \"aa_token\": \"T\"}, {\"embedding\": -4.081398010253906, \"dimension\": 1, \"position\": 21, \"aa_token\": \"R\"}, {\"embedding\": -1.4321938753128052, \"dimension\": 1, \"position\": 22, \"aa_token\": \"T\"}, {\"embedding\": -0.0, \"dimension\": 1, \"position\": 23, \"aa_token\": \"Q\"}, {\"embedding\": 0.157382071018219, \"dimension\": 1, \"position\": 24, \"aa_token\": \"L\"}, {\"embedding\": 3.150855779647827, \"dimension\": 1, \"position\": 25, \"aa_token\": \"P\"}, {\"embedding\": 2.2960503101348877, \"dimension\": 1, \"position\": 26, \"aa_token\": \"P\"}, {\"embedding\": 0.035320788621902466, \"dimension\": 1, \"position\": 27, \"aa_token\": \"A\"}, {\"embedding\": -1.0695621967315674, \"dimension\": 1, \"position\": 28, \"aa_token\": \"Y\"}, {\"embedding\": -0.6104476451873779, \"dimension\": 1, \"position\": 29, \"aa_token\": \"<end>\"}, {\"embedding\": -0.9066284894943237, \"dimension\": 2, \"position\": 0, \"aa_token\": \"<start>\"}, {\"embedding\": 0.3136008381843567, \"dimension\": 2, \"position\": 1, \"aa_token\": \"M\"}, {\"embedding\": 2.3029754161834717, \"dimension\": 2, \"position\": 2, \"aa_token\": \"F\"}, {\"embedding\": -0.4744349718093872, \"dimension\": 2, \"position\": 3, \"aa_token\": \"V\"}, {\"embedding\": 2.5311503410339355, \"dimension\": 2, \"position\": 4, \"aa_token\": \"F\"}, {\"embedding\": -0.16010959446430206, \"dimension\": 2, \"position\": 5, \"aa_token\": \"L\"}, {\"embedding\": -0.0, \"dimension\": 2, \"position\": 6, \"aa_token\": \"V\"}, {\"embedding\": 0.35469406843185425, \"dimension\": 2, \"position\": 7, \"aa_token\": \"L\"}, {\"embedding\": 0.16973920166492462, \"dimension\": 2, \"position\": 8, \"aa_token\": \"L\"}, {\"embedding\": 2.0787296295166016, \"dimension\": 2, \"position\": 9, \"aa_token\": \"P\"}, {\"embedding\": 0.38033100962638855, \"dimension\": 2, \"position\": 10, \"aa_token\": \"L\"}, {\"embedding\": 0.39426758885383606, \"dimension\": 2, \"position\": 11, \"aa_token\": \"V\"}, {\"embedding\": 1.8957446813583374, \"dimension\": 2, \"position\": 12, \"aa_token\": \"S\"}, {\"embedding\": 1.992120623588562, \"dimension\": 2, \"position\": 13, \"aa_token\": \"S\"}, {\"embedding\": 0.16398115456104279, \"dimension\": 2, \"position\": 14, \"aa_token\": \"Q\"}, {\"embedding\": 2.2233493328094482, \"dimension\": 2, \"position\": 15, \"aa_token\": \"C\"}, {\"embedding\": 0.7513840794563293, \"dimension\": 2, \"position\": 16, \"aa_token\": \"V\"}, {\"embedding\": 3.4420218467712402, \"dimension\": 2, \"position\": 17, \"aa_token\": \"N\"}, {\"embedding\": 1.1101701259613037, \"dimension\": 2, \"position\": 18, \"aa_token\": \"L\"}, {\"embedding\": 0.8577214479446411, \"dimension\": 2, \"position\": 19, \"aa_token\": \"T\"}, {\"embedding\": 3.9821414947509766, \"dimension\": 2, \"position\": 20, \"aa_token\": \"T\"}, {\"embedding\": -0.17637896537780762, \"dimension\": 2, \"position\": 21, \"aa_token\": \"R\"}, {\"embedding\": 4.111488342285156, \"dimension\": 2, \"position\": 22, \"aa_token\": \"T\"}, {\"embedding\": 0.9733656644821167, \"dimension\": 2, \"position\": 23, \"aa_token\": \"Q\"}, {\"embedding\": 1.497096300125122, \"dimension\": 2, \"position\": 24, \"aa_token\": \"L\"}, {\"embedding\": 3.348120927810669, \"dimension\": 2, \"position\": 25, \"aa_token\": \"P\"}, {\"embedding\": 3.391632080078125, \"dimension\": 2, \"position\": 26, \"aa_token\": \"P\"}, {\"embedding\": 0.0, \"dimension\": 2, \"position\": 27, \"aa_token\": \"A\"}, {\"embedding\": 2.514186143875122, \"dimension\": 2, \"position\": 28, \"aa_token\": \"Y\"}, {\"embedding\": 2.978207588195801, \"dimension\": 2, \"position\": 29, \"aa_token\": \"<end>\"}, {\"embedding\": 4.235321521759033, \"dimension\": 3, \"position\": 0, \"aa_token\": \"<start>\"}, {\"embedding\": 4.572660446166992, \"dimension\": 3, \"position\": 1, \"aa_token\": \"M\"}, {\"embedding\": 2.128066301345825, \"dimension\": 3, \"position\": 2, \"aa_token\": \"F\"}, {\"embedding\": 1.1003563404083252, \"dimension\": 3, \"position\": 3, \"aa_token\": \"V\"}, {\"embedding\": 2.0960865020751953, \"dimension\": 3, \"position\": 4, \"aa_token\": \"F\"}, {\"embedding\": 1.0813225507736206, \"dimension\": 3, \"position\": 5, \"aa_token\": \"L\"}, {\"embedding\": 3.1549551486968994, \"dimension\": 3, \"position\": 6, \"aa_token\": \"V\"}, {\"embedding\": 3.671031951904297, \"dimension\": 3, \"position\": 7, \"aa_token\": \"L\"}, {\"embedding\": 3.6317226886749268, \"dimension\": 3, \"position\": 8, \"aa_token\": \"L\"}, {\"embedding\": 3.455604076385498, \"dimension\": 3, \"position\": 9, \"aa_token\": \"P\"}, {\"embedding\": 3.538247585296631, \"dimension\": 3, \"position\": 10, \"aa_token\": \"L\"}, {\"embedding\": 2.9339652061462402, \"dimension\": 3, \"position\": 11, \"aa_token\": \"V\"}, {\"embedding\": 2.2336525917053223, \"dimension\": 3, \"position\": 12, \"aa_token\": \"S\"}, {\"embedding\": 2.170482873916626, \"dimension\": 3, \"position\": 13, \"aa_token\": \"S\"}, {\"embedding\": 0.6503612995147705, \"dimension\": 3, \"position\": 14, \"aa_token\": \"Q\"}, {\"embedding\": 2.7698094844818115, \"dimension\": 3, \"position\": 15, \"aa_token\": \"C\"}, {\"embedding\": 2.5968246459960938, \"dimension\": 3, \"position\": 16, \"aa_token\": \"V\"}, {\"embedding\": 2.1792030334472656, \"dimension\": 3, \"position\": 17, \"aa_token\": \"N\"}, {\"embedding\": 2.9836251735687256, \"dimension\": 3, \"position\": 18, \"aa_token\": \"L\"}, {\"embedding\": 1.9004825353622437, \"dimension\": 3, \"position\": 19, \"aa_token\": \"T\"}, {\"embedding\": 1.8098520040512085, \"dimension\": 3, \"position\": 20, \"aa_token\": \"T\"}, {\"embedding\": 2.465545892715454, \"dimension\": 3, \"position\": 21, \"aa_token\": \"R\"}, {\"embedding\": 0.0, \"dimension\": 3, \"position\": 22, \"aa_token\": \"T\"}, {\"embedding\": 0.0, \"dimension\": 3, \"position\": 23, \"aa_token\": \"Q\"}, {\"embedding\": 2.41325044631958, \"dimension\": 3, \"position\": 24, \"aa_token\": \"L\"}, {\"embedding\": 2.1768558025360107, \"dimension\": 3, \"position\": 25, \"aa_token\": \"P\"}, {\"embedding\": 2.070152759552002, \"dimension\": 3, \"position\": 26, \"aa_token\": \"P\"}, {\"embedding\": 2.381655693054199, \"dimension\": 3, \"position\": 27, \"aa_token\": \"A\"}, {\"embedding\": 0.48078641295433044, \"dimension\": 3, \"position\": 28, \"aa_token\": \"Y\"}, {\"embedding\": 1.672245740890503, \"dimension\": 3, \"position\": 29, \"aa_token\": \"<end>\"}, {\"embedding\": -0.15680719912052155, \"dimension\": 4, \"position\": 0, \"aa_token\": \"<start>\"}, {\"embedding\": 1.6114990711212158, \"dimension\": 4, \"position\": 1, \"aa_token\": \"M\"}, {\"embedding\": 0.009770779870450497, \"dimension\": 4, \"position\": 2, \"aa_token\": \"F\"}, {\"embedding\": 1.0549814701080322, \"dimension\": 4, \"position\": 3, \"aa_token\": \"V\"}, {\"embedding\": 0.0, \"dimension\": 4, \"position\": 4, \"aa_token\": \"F\"}, {\"embedding\": 0.7440100908279419, \"dimension\": 4, \"position\": 5, \"aa_token\": \"L\"}, {\"embedding\": 1.0710281133651733, \"dimension\": 4, \"position\": 6, \"aa_token\": \"V\"}, {\"embedding\": 0.754707396030426, \"dimension\": 4, \"position\": 7, \"aa_token\": \"L\"}, {\"embedding\": 0.7600558400154114, \"dimension\": 4, \"position\": 8, \"aa_token\": \"L\"}, {\"embedding\": 1.6400089263916016, \"dimension\": 4, \"position\": 9, \"aa_token\": \"P\"}, {\"embedding\": 0.7707521319389343, \"dimension\": 4, \"position\": 10, \"aa_token\": \"L\"}, {\"embedding\": 0.026329517364501953, \"dimension\": 4, \"position\": 11, \"aa_token\": \"V\"}, {\"embedding\": 3.6717379093170166, \"dimension\": 4, \"position\": 12, \"aa_token\": \"S\"}, {\"embedding\": 0.0, \"dimension\": 4, \"position\": 13, \"aa_token\": \"S\"}, {\"embedding\": -2.576770305633545, \"dimension\": 4, \"position\": 14, \"aa_token\": \"Q\"}, {\"embedding\": 0.0, \"dimension\": 4, \"position\": 15, \"aa_token\": \"C\"}, {\"embedding\": 1.1245036125183105, \"dimension\": 4, \"position\": 16, \"aa_token\": \"V\"}, {\"embedding\": -0.10434506833553314, \"dimension\": 4, \"position\": 17, \"aa_token\": \"N\"}, {\"embedding\": 0.8135251998901367, \"dimension\": 4, \"position\": 18, \"aa_token\": \"L\"}, {\"embedding\": 1.9973589181900024, \"dimension\": 4, \"position\": 19, \"aa_token\": \"T\"}, {\"embedding\": 2.0027031898498535, \"dimension\": 4, \"position\": 20, \"aa_token\": \"T\"}, {\"embedding\": -0.17708294093608856, \"dimension\": 4, \"position\": 21, \"aa_token\": \"R\"}, {\"embedding\": 2.01339054107666, \"dimension\": 4, \"position\": 22, \"aa_token\": \"T\"}, {\"embedding\": 0.05503525212407112, \"dimension\": 4, \"position\": 23, \"aa_token\": \"Q\"}, {\"embedding\": 0.8455870151519775, \"dimension\": 4, \"position\": 24, \"aa_token\": \"L\"}, {\"embedding\": 1.7255336046218872, \"dimension\": 4, \"position\": 25, \"aa_token\": \"P\"}, {\"embedding\": 0.0622066892683506, \"dimension\": 4, \"position\": 26, \"aa_token\": \"P\"}, {\"embedding\": -1.8555189371109009, \"dimension\": 4, \"position\": 27, \"aa_token\": \"A\"}, {\"embedding\": -0.8945093750953674, \"dimension\": 4, \"position\": 28, \"aa_token\": \"Y\"}, {\"embedding\": 0.06937552243471146, \"dimension\": 4, \"position\": 29, \"aa_token\": \"<end>\"}, {\"embedding\": 1.1111111640930176, \"dimension\": 5, \"position\": 0, \"aa_token\": \"<start>\"}, {\"embedding\": 2.728255033493042, \"dimension\": 5, \"position\": 1, \"aa_token\": \"M\"}, {\"embedding\": 1.0733726024627686, \"dimension\": 5, \"position\": 2, \"aa_token\": \"F\"}, {\"embedding\": 1.1110880374908447, \"dimension\": 5, \"position\": 3, \"aa_token\": \"V\"}, {\"embedding\": 1.0733035802841187, \"dimension\": 5, \"position\": 4, \"aa_token\": \"F\"}, {\"embedding\": 1.763359785079956, \"dimension\": 5, \"position\": 5, \"aa_token\": \"L\"}, {\"embedding\": 2.5088911056518555, \"dimension\": 5, \"position\": 6, \"aa_token\": \"V\"}, {\"embedding\": 1.7632215023040771, \"dimension\": 5, \"position\": 7, \"aa_token\": \"L\"}, {\"embedding\": 1.7631351947784424, \"dimension\": 5, \"position\": 8, \"aa_token\": \"L\"}, {\"embedding\": 1.1109023094177246, \"dimension\": 5, \"position\": 9, \"aa_token\": \"P\"}, {\"embedding\": 0.0, \"dimension\": 5, \"position\": 10, \"aa_token\": \"L\"}, {\"embedding\": 2.508401393890381, \"dimension\": 5, \"position\": 11, \"aa_token\": \"V\"}, {\"embedding\": 0.0, \"dimension\": 5, \"position\": 12, \"aa_token\": \"S\"}, {\"embedding\": 2.190056800842285, \"dimension\": 5, \"position\": 13, \"aa_token\": \"S\"}, {\"embedding\": 1.3897093534469604, \"dimension\": 5, \"position\": 14, \"aa_token\": \"Q\"}, {\"embedding\": 0.0, \"dimension\": 5, \"position\": 15, \"aa_token\": \"C\"}, {\"embedding\": 2.5076234340667725, \"dimension\": 5, \"position\": 16, \"aa_token\": \"V\"}, {\"embedding\": 1.1103659868240356, \"dimension\": 5, \"position\": 17, \"aa_token\": \"N\"}, {\"embedding\": 1.7616370916366577, \"dimension\": 5, \"position\": 18, \"aa_token\": \"L\"}, {\"embedding\": 1.7253317832946777, \"dimension\": 5, \"position\": 19, \"aa_token\": \"T\"}, {\"embedding\": 1.7251070737838745, \"dimension\": 5, \"position\": 20, \"aa_token\": \"T\"}, {\"embedding\": 1.1099741458892822, \"dimension\": 5, \"position\": 21, \"aa_token\": \"R\"}, {\"embedding\": 1.7246230840682983, \"dimension\": 5, \"position\": 22, \"aa_token\": \"T\"}, {\"embedding\": 1.3877911567687988, \"dimension\": 5, \"position\": 23, \"aa_token\": \"Q\"}, {\"embedding\": 1.7601854801177979, \"dimension\": 5, \"position\": 24, \"aa_token\": \"L\"}, {\"embedding\": 1.1094998121261597, \"dimension\": 5, \"position\": 25, \"aa_token\": \"P\"}, {\"embedding\": 2.0155749320983887, \"dimension\": 5, \"position\": 26, \"aa_token\": \"P\"}, {\"embedding\": 3.483102798461914, \"dimension\": 5, \"position\": 27, \"aa_token\": \"A\"}, {\"embedding\": 2.945545196533203, \"dimension\": 5, \"position\": 28, \"aa_token\": \"Y\"}, {\"embedding\": 3.4218993186950684, \"dimension\": 5, \"position\": 29, \"aa_token\": \"<end>\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_encoding():\n",
    "    embedding_dim = 6\n",
    "    dropout = 0.1\n",
    "    padding_idx = 25\n",
    "\n",
    "    max_len = 30    # test only the first 30 aas.\n",
    "    seq = test_seqs[0][:max_len-2]    \n",
    "\n",
    "    test_seq_encode = SeqEncoding(n_tokens, embedding_dim, dropout, max_len, padding_idx)\n",
    "    test_pe_encode = PositionalEncoding(embedding_dim, dropout, max_len)\n",
    "    y = test_pe_encode.forward(test_seq_encode(seq))\n",
    "    print(f'Embedding shape: {y.shape}')\n",
    "    print(f'Parameters shape in sequence embedding: {test_seq_encode.token_embedding.weight.shape}')\n",
    "\n",
    "    y = y.detach().numpy()\n",
    "\n",
    "\n",
    "    data = pd.concat([pd.DataFrame({\n",
    "        \"embedding\": y[:, dim],\n",
    "        \"dimension\":dim,\n",
    "        \"position\": list(range(max_len)),\n",
    "        })for dim in range(6)])\n",
    "    \n",
    "    aa = ['<start>']\n",
    "    aa.extend(_ for _ in seq)\n",
    "    aa.append('<end>')\n",
    "    data['aa_token'] = aa * embedding_dim\n",
    "\n",
    "    print(data)\n",
    "\n",
    "    return (\n",
    "        alt.Chart(data)\n",
    "        .mark_line()\n",
    "        .properties(width=800)\n",
    "        .encode(x=\"position\", y=\"embedding\", color=\"dimension:N\")\n",
    "        .interactive())\n",
    "\n",
    "test_encoding()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"Single head scaled dot product attention\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, query, key, value, mask=None, dropout=None):\n",
    "        d_k = query.size(-1)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)    #sqrt(d_k) is the scaling factor.\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask==0, -1e9)\n",
    "        \n",
    "        p_attn = scores.softmax(dim=-1)\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "    \n",
    "        return torch.matmul(p_attn, value), p_attn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention\n",
    "\n",
    "    h: numer of heads\n",
    "    d_model: model size\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, h:int, d_model:int, n_linear: int=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % h == 0 # d_model/h is used as d_k and d_v\n",
    "\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(n_linear)])  # n layers of linear model with the same input and output size\n",
    "        self.output_linear = nn.Linear(d_model, d_model)    # Output lienar model. This implementation follows BERT-pytorch instead of using the last linear layer, which is found in the annotated transformer.\n",
    "        self.attn = Attention() # The forward function in Attention class is called since no hooks are defined in Attention class. See __call__() and _call_impl() in nn.Module implementation.\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)    # same mask applied to all heads\n",
    "        n_batches = query.size(0)\n",
    "\n",
    "        # 1) Linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [lin(x).view(n_batches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "                             for lin, x in zip(self.linears, (query, key, value))]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch\n",
    "        x, attn = self.attn(query, key, value, mask=mask, dropout = self.dropout)   # Returned attn is not needed since x has already been weighted by attention in Attention.forward().\n",
    "\n",
    "        # 3) \"Concat using a view and apply a final linear\"\n",
    "        x = (x.transpose(1, 2)\n",
    "             .contiguous()\n",
    "             .view(n_batches, -1, self.h * self.d_k))\n",
    "        \n",
    "        # del query\n",
    "        # del key\n",
    "        # del value\n",
    "        return self.output_linear(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Normalization\n",
    "\n",
    "Linear regression based layer normalization with parameters a_2 and b_2. An arbituary small value (epsilon or eps) is added to std to avoid the error when std is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Construct a layernorm module\n",
    "    \n",
    "\n",
    "    The normalization is a linear transformation of z-score. A small float\n",
    "    number (eps) is added to std incase std is zero.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, features: torch.tensor, eps: float=1e-6):\n",
    "        super().__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x-mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positionwise Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"Implements FFN equation.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.activation(self.w_1(x))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer\"\"\"\n",
    "\n",
    "    def __init__(self, hidden, attn_heads, feed_forward_hidden, dropout):\n",
    "        \"\"\"\n",
    "        hidden: hidden size of transformer\n",
    "        attn_heads: number of attention heads\n",
    "        feed_forward_hidden: feed forward layer hidden size, usually 4 * hidden_size\n",
    "        dropout: dropout ratio\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadedAttention(h=attn_heads, d_model=hidden)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model=hidden, d_ff=feed_forward_hidden, dropout=dropout)\n",
    "        self.input_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.output_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.input_sublayer(x, lambda _x: self.attention.forward(_x, _x, _x, mask=mask))\n",
    "        x = self.output_sublayer(x, self.feed_forward)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT\n",
    "\n",
    "Here we define a model based on BERT. Part of the implementation is based on [BERT-pytorch](https://github.com/codertimo/BERT-pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, n):\n",
    "    \"\"\"Produce N identical layers\"\"\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 vocab_size: int=27,\n",
    "                 padding_idx: int=25,\n",
    "                 hidden: int=768, \n",
    "                 n_transformer_layers: int=12, \n",
    "                 attn_heads: int=12,\n",
    "                 dropout: float=0.1):\n",
    "        \"\"\"\n",
    "        vacab_size: vacabulary or token size\n",
    "        hidden: BERT model size (used as input size and hidden size)\n",
    "        n_layers: number of Transformer layers\n",
    "        attn_heads: attenion heads\n",
    "        dropout: dropout ratio\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.hidden  = hidden\n",
    "        self.n_transformer_layers = n_transformer_layers\n",
    "        self.attn_heads = attn_heads\n",
    "\n",
    "        # 4 * hidden_size for FFN\n",
    "        self.feed_forward_hidden = hidden * 4\n",
    "\n",
    "        # embeddings with sequence and postion\n",
    "        self.embedding = SeqEncoding(vocab_size=vocab_size,\n",
    "                                     embedding_dim=hidden,\n",
    "                                     dropout=dropout,\n",
    "                                     max_len=1500,\n",
    "                                     padding_idx=padding_idx)\n",
    "\n",
    "        self.transformer_blocks = clones(TransformerBlock(hidden, \n",
    "                                                          attn_heads, \n",
    "                                                          self.feed_forward_hidden,\n",
    "                                                          dropout), n_transformer_layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask):\n",
    "\n",
    "        # mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
    "        \n",
    "        x = self.embedding(x)   # sequence and position embedding in one step.\n",
    "\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer.forward(x, mask=mask)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_parameters_seq_encoding = sum(p.numel() for p in test_seq_encode.parameters() if p.requires_grad)\n",
    "#print(f'Parameters in SeqEncoding: {num_parameters_seq_encoding}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT-based Protein Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinMaskedLanguageModel(nn.Module):\n",
    "    \"\"\"Masked language model for protein sequences\"\"\"\n",
    "\n",
    "    def __init__(self, hidden: int, vocab_size: int):\n",
    "        \"\"\"\n",
    "        hidden: input size of the hidden linear layers\n",
    "        vocab_size: vocabulary size\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinLM(nn.Module):\n",
    "    \"\"\"\"\n",
    "    BERT protein language model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bert: BERT, vocab_size):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        # self.next_amino_acid = NextAminoAcidPrediction(self.bert.hidden)  # Cannot use next word prediction in a BERT model.\n",
    "        self.mlm = ProteinMaskedLanguageModel(self.bert.hidden, vocab_size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.bert(x, mask)\n",
    "        return self.mlm(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScheduledOptim():\n",
    "    \"\"\"A simple wrapper class for learning rate scheduling.\"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, d_model: int, n_warmup_steps):\n",
    "        self._optimizer=optimizer\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_current_steps = 0\n",
    "        self.init_lr = np.power(d_model, -0.5)\n",
    "\n",
    "    def step_and_update_lr(self):\n",
    "        \"Step with the inner optimizer\"\n",
    "        self._update_learning_rate()\n",
    "        self._optimizer.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"Zero out the gradients by the inner optimizer\"\n",
    "        self._optimizer.zero_grad()\n",
    "\n",
    "    def _get_lr_scale(self):\n",
    "        return np.min([\n",
    "            np.power(self.n_current_steps, -0.5),\n",
    "            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n",
    "\n",
    "    def _update_learning_rate(self):\n",
    "        \"\"\"Learning rate scheduling per step\"\"\"\n",
    "        self.n_current_steps += 1\n",
    "        lr = self.init_lr * self._get_lr_scale()\n",
    "\n",
    "        for param_group in self._optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader\n",
    "Now we build a Dataloader that reads fasta files of the protein sequences. Since our purpose now is pre-training, we will not need any phenotype data. Dataloader class to read in fasta file and return encoded sequence at index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a database of the training and testing data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_db(db_file_path: str, train_fasta: str, test_fasta: str) -> sqlite3.Connection:\n",
    "    conn = sqlite3.connect(db_file_path)\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    cur.execute('''CREATE TABLE train (id INTEGER PRIMARY KEY AUTOINCREMENT, header TEXT, sequence TEXT)''')\n",
    "    cur.execute('''CREATE TABLE test (id INTEGER PRIMARY KEY AUTOINCREMENT, header TEXT, sequence TEXT)''')\n",
    "    \n",
    "    training_seqs = SeqIO.parse(open(train_fasta),'fasta')\n",
    "\n",
    "    for i, fasta in enumerate(training_seqs):\n",
    "        header, seq = fasta.id, str(fasta.seq)\n",
    "        cur.execute(\"INSERT INTO train (header, sequence) VALUES (?,?)\", (header, seq))\n",
    "    conn.commit()\n",
    "\n",
    "    test_seqs = SeqIO.parse(open(test_fasta), \"fasta\")\n",
    "\n",
    "    for i, fasta in enumerate(test_seqs):\n",
    "        header, seq = fasta.id, str(fasta.seq)\n",
    "        cur.execute(\"INSERT INTO test (header, sequence) VALUES (?,?)\", (header, seq))\n",
    "    conn.commit()    \n",
    "\n",
    "    print(f'Database {db_file_path} initialized.')\n",
    "    return conn\n",
    "\n",
    "# Initialize database if it does not exist already.\n",
    "db_file_path = \"../data/SARS_CoV_2_spike.db\"\n",
    "training_set_fasta = \"../data/spikeprot0203.clean.uniq.training.fasta\"\n",
    "testing_set_fasta = \"../data/spikeprot0203.clean.uniq.testing.fasta\"\n",
    "if os.path.isfile(db_file_path):\n",
    "    conn = sqlite3.connect(db_file_path)\n",
    "else:\n",
    "    conn = initialize_db(db_file_path, training_set_fasta, testing_set_fasta)\n",
    "                         \n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Create Dataset compatible indexing of fasta file\n",
    "\n",
    "    db_file: sqlite3 database file\n",
    "    table_name: table name inside the sqlite database\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, db_file: str, table_name: str) -> None:\n",
    "        self.db_file = db_file\n",
    "        self.table = table_name\n",
    "        self.conn = None    # Use lazy loading\n",
    "                \n",
    "    def __getitem__(self, idx):\n",
    "        if self.conn is None:\n",
    "            self.conn = sqlite3.connect(self.db_file, isolation_level=None)  # Read only operations in sqlite connection.\n",
    "\n",
    "        cur = self.conn.cursor()\n",
    "        _, header, sequence = cur.execute(f'''SELECT * FROM {self.table} LIMIT 1 OFFSET {idx}''').fetchone()\n",
    "        # print(f'idx: {idx}, header: {header}, first 30 aas: {seq[:30]}')\n",
    "        \n",
    "        return header, sequence\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.conn is None:\n",
    "            self.conn = sqlite3.connect(self.db_file, isolation_level=None)  # Read only operations in sqlite connection.\n",
    "\n",
    "        cur = self.conn.cursor()\n",
    "        total_seq = cur.execute(f'''SELECT COUNT(*) as total_seq FROM {self.table}''').fetchone()[0]\n",
    "        return total_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total seqs in training set: 80000\n",
      "batch 0: ('EPI_ISL_1187245', 'EPI_ISL_1674786', 'EPI_ISL_1527721', 'EPI_ISL_1360705', 'EPI_ISL_1417353')\n",
      "batch 1: ('EPI_ISL_1655653', 'EPI_ISL_1435896', 'EPI_ISL_1716391', 'EPI_ISL_1307217', 'EPI_ISL_1577119')\n",
      "batch 2: ('EPI_ISL_1573743', 'EPI_ISL_1636780', 'EPI_ISL_1456484', 'EPI_ISL_1381978', 'EPI_ISL_1465080')\n",
      "batch 3: ('EPI_ISL_1407070', 'EPI_ISL_1488649', 'EPI_ISL_1250835', 'EPI_ISL_1608659', 'EPI_ISL_1498064')\n",
      "batch 4: ('EPI_ISL_1248660', 'EPI_ISL_1518290', 'EPI_ISL_1482808', 'EPI_ISL_1651527', 'EPI_ISL_1464782')\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SeqDataset(db_file_path, \"train\")\n",
    "print(f'Total seqs in training set: {len(train_dataset)}')\n",
    "\n",
    "batch_size = 5\n",
    "shuffle = True\n",
    "num_workers = 1\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, drop_last=True)\n",
    "n = 5\n",
    "for i, batch in enumerate(train_loader):\n",
    "    if i >= n:\n",
    "        break\n",
    "    print(f'batch {i}: {batch[0]}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now add masks\n",
    "\n",
    "We will mask 15% of amino acid in sequence like in the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_pad(seqs:tuple, max_len: int,  token_dict: dict) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Tokenize a sequence batch and add padding when needed.\n",
    "\n",
    "    Returns a tuple of sequence ids and tokenized tensor of the shape [batch_size, longest_seq].\n",
    "    If the longest sequence in the batch is longer then max_len, the shape is [batch_size, max_len].\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    seqs: a tuple or list-like sequence collection\n",
    "    max_len: maximum length of the input squence\n",
    "    token_dict: token to index dictionary\n",
    "    \"\"\"\n",
    "    tokenized_seqs = []\n",
    "    for a_seq in seqs:\n",
    "        if len(a_seq) < max_len:\n",
    "            a_seq = [aa_to_token_index.get(aa, token_dict['<OTHER>']) for aa in a_seq]\n",
    "        else:                   # if more  then max_len, we will need to truncate it and mark it <TRUNCATED>\n",
    "            a_seq = [aa_to_token_index.get(aa, token_dict['<OTHER>']) for aa in a_seq[:max_len-1]]\n",
    "            a_seq.append(token_dict['<TRUNCATED>'])\n",
    "        tokenized_seqs.append(a_seq)\n",
    "\n",
    "    max_in_bach = max([len(a_seq) for a_seq in tokenized_seqs])  # length of longest sequence in batch\n",
    "    for _, seq in enumerate(tokenized_seqs):\n",
    "        n_pad = max_in_bach - len(seq)\n",
    "        if n_pad > 0:\n",
    "            for p in range(n_pad):\n",
    "                seq.append(padding_idx)\n",
    "            tokenized_seqs[_] = seq\n",
    "\n",
    "    tokenized_seqs = torch.tensor(tokenized_seqs)\n",
    "\n",
    "    return tokenized_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_mask(batch_seqs: torch.Tensor, mask_prob:float, token_dict:dict) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Add <MASK> token at random locations based on assigned probabilities.\n",
    "\n",
    "    Locations of the <MASK> are the same across one batch with the exception that \n",
    "    speical tokens, e.g. <PAD> <TRUNCATED> are not masked. Returns a tuple of\n",
    "    (seq_ids, masked sequence tokens, original tokens)\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    batch_input: tensor of tokenized sequences with the shape [batch_size, seq_lenght]\n",
    "    mask_prob: masking probability.\n",
    "    token_dict: token to index dictionary\n",
    "    \"\"\"\n",
    "    batch_masked = batch_seqs.clone()\n",
    "    seq_len = batch_seqs.size(1)\n",
    "    \n",
    "    n_mask = max(int(mask_prob * seq_len), 1) # at least one mask\n",
    "\n",
    "    row_idx = range(batch_seqs.size(0))\n",
    "\n",
    "    SPECIAL_TOKENS = [token_dict[st] for st in ['<PAD>', '<TRUNCATED>']]\n",
    "    MASK_IDX = token_dict['<MASK>']\n",
    "    \n",
    "    for _ in range(n_mask):\n",
    "        idx = int(random.random() * seq_len)\n",
    "        for row in row_idx:\n",
    "            if batch_masked[row, idx] not in SPECIAL_TOKENS:\n",
    "                batch_masked[row, idx] = torch.tensor(MASK_IDX)\n",
    "    return batch_masked"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test sequence padding and maskng \n",
    "Note that 26 is \\<MASK\\> and 27 is \\<TRUNCATED\\>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10,  4, 18,  4,  9, 18,  9,  9, 12,  9, 18, 15, 15, 13,  1, 18, 11,  9,\n",
      "         16, 16, 14, 16, 13,  9, 12, 12,  0, 21, 16, 11, 15,  4, 16, 14,  5, 18,\n",
      "         21, 21, 12,  2,  8, 18,  4, 14, 15, 15, 18,  9,  6, 27],\n",
      "        [10,  4, 18,  4,  9, 18,  9,  9, 12,  9, 18, 15, 15, 13,  1, 18, 11,  9,\n",
      "         16, 16, 14, 16, 13,  9, 12, 12,  0, 21, 16, 11, 15,  4, 16, 14,  5, 18,\n",
      "         21, 21, 12,  2,  8, 18,  4, 14, 15, 15, 18,  9,  6, 27],\n",
      "        [10,  4, 18,  4,  9, 18,  9,  9, 12,  9, 18, 15, 15, 13,  1, 18, 11,  9,\n",
      "         16, 16, 14, 16, 13,  9, 12, 12,  0, 21, 16, 11, 15,  4, 16, 14,  5, 18,\n",
      "         21, 21, 12,  2,  8, 18,  4, 14, 15, 15, 18,  9,  6, 27],\n",
      "        [10,  4, 18,  4,  9, 18,  9,  9, 12,  9, 18, 15, 15, 13,  1, 18, 11,  9,\n",
      "         16, 16, 14, 16, 13,  9, 12, 12,  0, 21, 16, 11, 15,  4, 16, 14,  5, 18,\n",
      "         21, 21, 12,  2,  8, 18,  4, 14, 15, 15, 18,  9,  6, 27],\n",
      "        [10,  4, 18,  4,  9, 18,  9,  9, 12,  9, 18, 15, 15, 13,  1, 18, 11,  9,\n",
      "         16, 16, 14, 16, 13,  9, 12, 12,  0, 21, 16, 11, 15,  4, 16, 14,  5, 18,\n",
      "         21, 21, 12,  2,  8, 18,  4, 14, 15, 15, 18,  9,  6, 27]])\n",
      "tensor([[10,  4, 18,  4, 26, 18,  9,  9, 12,  9, 18, 15, 26, 13,  1, 18, 11,  9,\n",
      "         16, 16, 14, 26, 13,  9, 12, 12,  0, 21, 16, 11, 15,  4, 16, 14, 26, 26,\n",
      "         21, 21, 26,  2,  8, 18,  4, 14, 15, 15, 18,  9,  6, 27],\n",
      "        [10,  4, 18,  4, 26, 18,  9,  9, 12,  9, 18, 15, 26, 13,  1, 18, 11,  9,\n",
      "         16, 16, 14, 26, 13,  9, 12, 12,  0, 21, 16, 11, 15,  4, 16, 14, 26, 26,\n",
      "         21, 21, 26,  2,  8, 18,  4, 14, 15, 15, 18,  9,  6, 27],\n",
      "        [10,  4, 18,  4, 26, 18,  9,  9, 12,  9, 18, 15, 26, 13,  1, 18, 11,  9,\n",
      "         16, 16, 14, 26, 13,  9, 12, 12,  0, 21, 16, 11, 15,  4, 16, 14, 26, 26,\n",
      "         21, 21, 26,  2,  8, 18,  4, 14, 15, 15, 18,  9,  6, 27],\n",
      "        [10,  4, 18,  4, 26, 18,  9,  9, 12,  9, 18, 15, 26, 13,  1, 18, 11,  9,\n",
      "         16, 16, 14, 26, 13,  9, 12, 12,  0, 21, 16, 11, 15,  4, 16, 14, 26, 26,\n",
      "         21, 21, 26,  2,  8, 18,  4, 14, 15, 15, 18,  9,  6, 27],\n",
      "        [10,  4, 18,  4, 26, 18,  9,  9, 12,  9, 18, 15, 26, 13,  1, 18, 11,  9,\n",
      "         16, 16, 14, 26, 13,  9, 12, 12,  0, 21, 16, 11, 15,  4, 16, 14, 26, 26,\n",
      "         21, 21, 26,  2,  8, 18,  4, 14, 15, 15, 18,  9,  6, 27]])\n"
     ]
    }
   ],
   "source": [
    "def test_pad_and_mask(batch_seqs):\n",
    "    tokenized_seqs = batch_pad(batch_seqs, 50, token_to_index)\n",
    "    masked_seqs = batch_mask(tokenized_seqs, 0.15, token_to_index)\n",
    "    print(tokenized_seqs)\n",
    "    print(masked_seqs)\n",
    "\n",
    "test_pad_and_mask(batch[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model Execution\n",
    "\n",
    "Now that we have all the building blocks of the model, we can test the model execution by loading a few batch of example sequences to train the masked language model. The testing process will be very smiliar except that there is no model parameter updates.\n",
    "\n",
    "## Model execution steps\n",
    "\n",
    "1. Load training data\n",
    "We will define the training data using the SeqDataset class and sepecify we want to use the sequences from the \"train\" table. Example code is avaiable in the train_dataset above.\n",
    "\n",
    "2. Mask the batched training seqeunce\n",
    "The mask_sequence function defines how masks are added. A helper function can be added to load bached sequence from SeqDataset, add masks, and return both the masked sequence and that amino acids at masked locations.\n",
    "\n",
    "3. Feed the masked sequence batch to the protein language model.\n",
    "The masked sequence will be embedded as vocabularies and feed to the forward function in the protein language model, which will initialize a BERT model inside it. The hidden BERT model includes  Transformer layers with multihead self-attention and position embedding. Status of the last Transformer will used as inputs to the feed forward layers. The last feedforward layer will be used to feed the linear layer in the ProteinMaskedLanguageModel, where a softmax function is used to predict the masked tokens.\n",
    "\n",
    "4. The cross entropy error function is used to caculate the error of the masked token prediction per batch. The goal of the training process is to optimize the parameters so that the error is minimized. After reaching the training goal (number of maximum epochs or the average error), we will need to save the model stataus so that we can reload the model with the trained parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in the model: 95260\n",
      "Total seqs in training set: 80000\n",
      "\n",
      "batch 0\n",
      "torch.Size([1500, 24])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at CPUAllocator.cpp:68] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 162000000000 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_49749/726891089.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_49749/726891089.py\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m#print(f'Batch embedding shape: {batch_embedding.shape}')    # Shape: (batch_size, max_len, embedding_dim), e.g. [5, 1500, 20]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_49749/3344377063.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_49749/1758549240.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtransformer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_blocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_49749/3824569638.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_sublayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0m_x\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_sublayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_49749/1764567.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, sublayer)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msublayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;34m\"Apply residual connection to any sublayer with the same size\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msublayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_49749/3824569638.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(_x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_sublayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0m_x\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_sublayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_49749/2862065040.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, mask)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# 2) Apply attention on all the projected vectors in batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Returned attn is not needed since x has already been weighted by attention in Attention.forward().\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# 3) \"Concat using a view and apply a final linear\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_49749/3958193682.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, mask, dropout)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0md_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_k\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m#sqrt(d_k) is the scaling factor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at CPUAllocator.cpp:68] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 162000000000 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "def test_model():\n",
    "    vocab_size = n_tokens\n",
    "    embedding_dim = 24\n",
    "    padding_idx = token_to_index['<PAD>']\n",
    "\n",
    "    bert_hidden = embedding_dim\n",
    "    n_transformer_layers = 12\n",
    "    n_attn_heads = 12\n",
    "    dropout = 0.1 \n",
    "\n",
    "    bert = BERT(vocab_size, padding_idx, bert_hidden, n_transformer_layers, n_attn_heads, dropout)\n",
    "    model = ProteinLM(bert, vocab_size)\n",
    "    n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'Total parameters in the model: {n_params}') #TODO: This does not include the parameters in the embedding process.\n",
    "\n",
    "    db_file_path =  \"../data/SARS_CoV_2_spike.db\"\n",
    "    train_dataset = SeqDataset(db_file_path, \"train\")\n",
    "    print(f'Total seqs in training set: {len(train_dataset)}')\n",
    "\n",
    "    batch_size = 5\n",
    "    num_workers = 1\n",
    "    max_len = 50\n",
    "    mask_prob = 0.15\n",
    "\n",
    "    embed_tokens = SeqEncoding(vocab_size, embedding_dim, dropout, max_len, padding_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True)\n",
    "\n",
    "    n_test_baches = 5\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        if i >= n_test_baches:\n",
    "            break\n",
    "        print(f'\\nbatch {i}')\n",
    "        [seq_ids, seqs] = batch\n",
    "\n",
    "        #batch_embedding = torch.empty((batch_size, max_len, embedding_dim))\n",
    "        #for i in range(batch_size):\n",
    "            # token_indices, masked_token_idx = mask_sequence(seqs[i], max_len, mask_prob)\n",
    "            # embeddings = embed_tokens(token_indices)\n",
    "\n",
    "            #embeddings = embed_tokens(seqs[i])\n",
    "            #batch_embedding[i] = embeddings\n",
    "        \n",
    "        batch_mask = None\n",
    "\n",
    "        #print(f'Batch embedding shape: {batch_embedding.shape}')    # Shape: (batch_size, max_len, embedding_dim), e.g. [5, 1500, 20]\n",
    "        model(batch[0][1], batch_mask)\n",
    "\n",
    "\n",
    "test_model()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPTrainer:\n",
    "    \"\"\"\n",
    "    Model trainer\n",
    "\n",
    "    Pretrain BERT Protein model with the masked language model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 bert: BERTProtein,\n",
    "                 vocab_size: int,\n",
    "                 train_dataloader: DataLoader,\n",
    "                 test_dataloader: DataLoader = None,\n",
    "                 lr: float=1e-4,\n",
    "                 betas=(0.9, 0.999),\n",
    "                 weight_decay: float=0.01,\n",
    "                 warmup_steps: int=10000,\n",
    "                 with_cuda: bool = True,\n",
    "                 cuda_device = None,\n",
    "                 log_freq: int = 10\n",
    "                 ):\n",
    "        \n",
    "        # Use CUDA device if it is available and with_cuda is Truegb\n",
    "        cuda_condition = torch.cuda.is_available() and with_cuda\n",
    "        self.device = torch.device(\"cuda:0\" if cuda_condition else \"cpu\")\n",
    "\n",
    "        # Distributed GPU training if more than one CUDA device is detected.\n",
    "        if with_cuda and torch.cuda.device_count() > 1:\n",
    "            print(f\"Using {torch.cuda.device_count()} GPUs for BERT.\")\n",
    "\n",
    "        # This BERT model will be saved every epoch\n",
    "        self.bert = bert\n",
    "        self.model = ProteinMaskedLanguageModel(bert, vocab_size).to(self.device)\n",
    "\n",
    "        self.train_data = train_dataloader\n",
    "        self.test_data = test_dataloader\n",
    "\n",
    "        # Setting the Adam optimizer with hpyer-param\n",
    "        self.optim = Adam(self.model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        self.optim_schedule = ScheduledOptim(self.optim, self.bert.hidden, n_warmup_steps=warmup_steps)\n",
    "\n",
    "        # Using negative log likelyhood loss function for predicting the masked_token\n",
    "        self.criterion = nn.NLLLoss(ignore_index=0) #TODO: check if ignore_index should be set differently.\n",
    "        \n",
    "        self.log_freq = log_freq\n",
    "\n",
    "        print(f'Total parameters: {sum(p.numel() for p in self.model.parameters() if p.requires_grad)}')\n",
    "\n",
    "    def train(self, epoch: int=10):\n",
    "        self.iteration(epoch, self.train_data)\n",
    "\n",
    "    def test(self, epoch: int=10):\n",
    "        self.iteration(epoch, self.test_data, train=False)\n",
    "\n",
    "    def iteration(self, epoch, data_loader, train: bool=True):\n",
    "        \"\"\"\n",
    "        Loop over the data_loader for training or testing.\n",
    "\n",
    "        If on train status, backward operation is activated and also auto save the model every epoch.\n",
    "        \"\"\"\n",
    "        str_code = \"train\" if train else \"test\"\n",
    "\n",
    "        # set the tqdm progress bar\n",
    "        data_iter = tqdm.tqdm(enumerate(data_loader),\n",
    "                              desc=f'EP_{str_code}: {epoch}',\n",
    "                              total = len(data_loader),\n",
    "                              bar_format='{l_bar}{r_bar}')\n",
    "\n",
    "        avg_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_element = 0\n",
    "\n",
    "        for i, data in data_iter:\n",
    "            data = {key: value.to(self.device) for key, value in data.items()}\n",
    "\n",
    "            #TODO: get the masked sequence out.\n",
    "            next_aa_predicted, mlm_predicted = self.model.forward(data['bert_input'], ...) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d224cae9022a7df69b2822db0ae60ab6c394083f94ec6064ea0f97bcb6ff035e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
