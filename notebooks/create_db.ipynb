{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create databases with sqlite 3, and call it something like 'spike_prot.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"spike_prot.db\")\n",
    "db_cursor = conn.cursor()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets create a data table for trainign sequences with it's simple data structure and one for test sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x7f23c987ab20>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create train sequences table\n",
    "db_cursor.execute('''CREATE TABLE train_sequences\n",
    "             (id INTEGER PRIMARY KEY,\n",
    "              header TEXT,\n",
    "              sequence TEXT)''')\n",
    "\n",
    "#create test sequences table\n",
    "db_cursor.execute('''CREATE TABLE test_sequences\n",
    "             (id INTEGER PRIMARY KEY,\n",
    "              header TEXT,\n",
    "              sequence TEXT)''')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the fasta files in and distrubute them to their correct collections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_seqs = SeqIO.parse(open(os.path.abspath('../data/spikeprot0203.clean.uniq.training.fasta')),'fasta')\n",
    "\n",
    "for i, fasta in enumerate(training_seqs):\n",
    "    header, seq = fasta.id, str(fasta.seq)\n",
    "    db_cursor.execute(\"INSERT INTO train_sequences (header, sequence) VALUES (?,?)\", (header,seq))\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"spike_prot.db\")\n",
    "db_cursor = conn.cursor()\n",
    "\n",
    "testing_seqs = SeqIO.parse(open(os.path.abspath('../data/spikeprot0203.clean.uniq.testing.fasta')), 'fasta')\n",
    "\n",
    "for i, fasta in enumerate(testing_seqs):\n",
    "    header, seq = fasta.id, str(fasta.seq)\n",
    "    db_cursor.execute(\"INSERT INTO test_sequences (header, sequence) VALUES (?,?)\", (header,seq))\n",
    "    \n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSSVLHSTQDLFLPFFSNVTWFHAIHVSGTNGTKRFDNPVLPFNDGVYFASTEKSNIIRGWIFGTTLDSKTQSLLIVNNATNVVIKVCEFQFCNDPFLGVYYHKNNKSWMESEFRVYSSANNCTFEYVSQPFLMDLEGKQGNFKNLREFVFKNIDGYFKIYSKHTPINLVRDLPQGFSALEPLVDLPIGINITRFQTLLALHRSYLTPGDSSSGWTAGAAAYYVGYLQPRTFLLKYNENGTITDAVDCALDPLSETKCTLKSFTVEKGIYQTSNFRVQPTESIVRFPNITNLCPFGEVFNATRFASVYAWNRKRISNCVADYSVLYNSASFSTFKCYGVSPTKLNDLCFTNVYADSFVIRGDEVRQIAPGQTGKIADYNYKLPDDFTGCVIAWNSNNLDSKVGGNYNYLYRLFRKSNLKPFERDISTEIYQAGSTPCNGVEGFNCYFPLQSYGFQPTNGVGYQPYRVVVLSFELLHAPATVCGPKKSTNLVKNKCVNFNFNGLTGTGVLTESNKKFLPFQQFGRDIADTTDAVRDPQTLEILDITPCSFGGVSVITPGTNTSNQVAVLYQDVNCTEVPVAIHADQLTPTWRVYSTGSNVFQTRAGCLIGAEHVNNSYECDIPIGAGICASYQTQTNSPRRARSVASQSIIAYTMSLGAENSVAYSNNSIAIPTNFTISVTTEILPVSMTKTSVDCTMYICGDSTECSNLLLQYGSFCTQLNRALTGIAVEQDKNTQEVFAQVKQIYKTPPIKDFGGFNFSQILPDPSKPSKRSFIEDLLFNKVTLADAGFIKQYGDCLGDIAARDLICAQKFNGLTVLPPLLTDEMIAQYTSALLAGTITSGWTFGAGAALQIPFAMQMAYRFNGIGVTQNVLYENQKLIANQFNSAIGKIQDSLSSTASALGKLQDVVNQNAQALNTLVKQLSSNFGAISSVLNDILSRLDKVEAEVQIDRLITGRLQSLQTYVTQQLIRAAEIRASANLAATKMSECVLGQSKRVDFCGKGYHLMSFPQSAPHGVVFLHVTYVPAQEKNFTTAPAICHDGKAHFPREGVFVSNGTHWFVTQRNFYEPQIITTDNTFVSGNCDVVIGIVNNTVYDPLQPELDSFKEELDKYFKNHTSPDVDLGDISGINASVVNIQKEIDRLNEVAKNLNESLIDLQELGKYEQYIKWPWYIWLGFIAGLIAIVMVTIMLCCMTSCCSCLKGCCSCGSCCKFDEDDSEPVLKGVKLHYT*\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect(\"spike_prot.db\")\n",
    "db_cursor = conn.cursor()\n",
    "\n",
    "db_cursor.execute(\"SELECT sequence FROM train_sequences\")\n",
    "train_result = db_cursor.fetchone()\n",
    "train_sequence = train_result[0]\n",
    "print(train_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSSVLHSTQDLFLPFFSNVTWFHAIHVSGTNGTKRFDNPVLPFNDGVYFASTEKSNIIRGWIFGTTLDSKTQSLLIVNNATNVVIKVCEFQFCNDPFLGVYYHKNNKSWMESEFRVYSSANNCTFEYVSQPFLMDLEGKQGNFKNLREFVFKNIDGYFKIYSKHTPINLVRDLPQGFSVLEPLVDLPIGINITRFQTLLALHRSYLTPGDSSSGWTAGAAAYYVGYLQPRTFLLKYNENGTITDAVDCALDPLSETKCTLKSFTVEKGIYQTSNFRVQPTESIVRFPNITNLCPFGEVFNATRFASVYAWNRKRISNCVADYSVLYNSASFSTFKCYGVSPTKLNDLCFTNVYADSFVIRGDEVRQIAPGQTGKIADYNYKLPDDFTGCVIAWNSNNLDSKVGGNYNYLYRLFRKSNLKPFERDISTEIYQAGSTPCNGVEGFNCYFPLQSYGFQPTNGVGYQPYRVVVLSFELLHAPATVCGPKKSTNLVKNKCVNFNFNGLTGTGVLTESNKKFLPFQQFGRDIADTTDAVRDPQTLEILDITPCSFGGVSVITPGTNTSNQVAVLYQGVNCTEVPVAIHADQLTPTWRVYSTGSNVFQTRAGCLIGAEHVNNSYECDIPIGAGICASYXTQTNSPRRARSVASQSIIAYTMSLGAENSVAYSNNSIAIPTNFTISVTTEILPVSMTKTSVDCTMYICGDSTECSNLLLQYGSFCTQLNRALTGIAVEQDKNTQEVFAQVKQIYKTPPIKDFGGFNFSQILPDPSKPSKRSFIEDLLFNKVTLADAGFIKQYGDCLGDIAARDLICAQKFNGLTVLPPLLTDEMIAQYTSALLAGTITSGWTFGAGAALQIPFAMQMAYRFNGIGVTQNVLYENQKLIANQFNSAIGKIQDSLSSTASALGKLQDVVNQNAQALNTLVKQLSSNFGAISSVLNDILSRLDKVEAEVQIDRLITGRLQSLQTYVTQQLIRAAEIRASANLAATKMSECVLGQSKRVDFCGKGYHLMSFPQSAPHGVVFLHVTYVPAQEKNFTTAPAICHDGKAHFPREGVFVSNGTHWFVTQRNFYEPQIIXTDNTFVSGNCDVVIGIVNNTVYDPLQPELDSFKEELDKYFKNHTSPDVDLGDISGINASVVNIQKEIDRLNEVAKNLNESLIDLQELGKYEQYIKWPWYIWLGFIAGLIAIVMVTIMLCCMTSCCSCLKGCCSCGSCCKFDEDDSEPVLKGVKLHYT*\n"
     ]
    }
   ],
   "source": [
    "db_cursor.execute(\"SELECT sequence FROM test_sequences\")\n",
    "test_result = db_cursor.fetchone()\n",
    "test_sequence = test_result[0]\n",
    "print(test_sequence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how to use this as way to load in our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastaDataset(Dataset):\n",
    "    \"\"\"Create Dataset compatible indexing of fasta file\n",
    "    \"\"\"\n",
    "    def __init__(self, db_file: str, table_name: str, encoding_fn) -> None:\n",
    "        self.db_file = db_file\n",
    "        self.table_name = table_name\n",
    "        self.encoding_fn = encoding_fn\n",
    "        \n",
    "        conn = sqlite3.connect(self.db_file)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT sequence FROM {} ORDER BY id\".format(self.table_name))\n",
    "        self.sequences = [row[0] for row in cursor.fetchall()]\n",
    "        conn.close()\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        sequence = sequence.replace(\"*\", \"\")\n",
    "        encoding = self.encoding_fn(sequence)\n",
    "        return encoding\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "\n",
    "class FastaDataLoader:\n",
    "    \"\"\"Wrapper for fasta dataloader\n",
    "    \"\"\"\n",
    "    def __init__(self, db_file: str, table_name: str, encoding_fn, batch_size: int, shuffle=True):\n",
    "        self.dataset = FastaDataset(db_file, table_name, encoding_fn)\n",
    "        self.dataloader = DataLoader(self.dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_AAS = 'ACDEFGHIKLMNPQRSTUVWXY'\n",
    "ADDITIONAL_TOKENS = ['<OTHER>', '<START>', '<END>', '<PAD>']\n",
    "\n",
    "# Each sequence is added <START> and <END>. \"<PAD>\" are added to sequence shorten than max_len.\n",
    "ADDED_TOKENS_PER_SEQ = 2\n",
    "\n",
    "n_aas = len(ALL_AAS)\n",
    "aa_to_token_index = {aa: i for i, aa in enumerate(ALL_AAS)}\n",
    "additional_token_to_index = {token: i + n_aas for i, token in enumerate(ADDITIONAL_TOKENS)}\n",
    "token_to_index = {**aa_to_token_index, **additional_token_to_index}\n",
    "index_to_token = {index: token for token, index in token_to_index.items()}\n",
    "n_tokens = len(token_to_index)\n",
    "\n",
    "def tokenize_seq(seq: str, max_len:int=1500) -> torch.IntTensor:\n",
    "    \"\"\"\n",
    "    Tokenize a sequence.\n",
    "\n",
    "    It is the caller's responsibility to infer the maximum length of the input. In case of\n",
    "    tokenizing a batch of sequences, the maximum length shall be assigned to the lenght of\n",
    "    the longest sequence in the same batch. \n",
    "\n",
    "\n",
    "    seq: input insquence\n",
    "    max_len: maximum number of tokens, including the special tokens such as <START>, <END>.\n",
    "    \n",
    "    \"\"\"\n",
    "    seq = seq.upper()   # All in upper case.\n",
    "    other_token_index = additional_token_to_index['<OTHER>']\n",
    "    token_seq = [additional_token_to_index['<START>']] + [aa_to_token_index.get(aa, other_token_index) for aa in seq]\n",
    "    if len(token_seq) < max_len - 1: # -1 is for the <END> token\n",
    "        n_pads = max_len -1 - len(token_seq)\n",
    "        token_seq.extend(token_to_index['<PAD>'] for _ in range(n_pads))\n",
    "    token_seq += [additional_token_to_index['<END>']]\n",
    "    return torch.IntTensor(token_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class training_config:\n",
    "    batch_size: int = 32\n",
    "    shuffle: bool = True\n",
    "    table_name: str = 'train_sequences'\n",
    "    db_file: str = 'spike_prot.db'\n",
    "    \n",
    "@dataclass\n",
    "class testing_config:\n",
    "    batch_size: int = 32\n",
    "    shuffle: bool = True\n",
    "    table_name: str = 'test_sequences'\n",
    "    db_file: str = 'spike_prot.db'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[23, 10,  4,  ..., 25, 25, 24],\n",
      "        [23, 10,  4,  ..., 25, 25, 24],\n",
      "        [23, 10,  4,  ..., 25, 25, 24],\n",
      "        ...,\n",
      "        [23, 10,  4,  ..., 25, 25, 24],\n",
      "        [23, 10,  4,  ..., 25, 25, 24],\n",
      "        [23, 10,  4,  ..., 25, 25, 24]], dtype=torch.int32)\n",
      "tensor([[23, 10,  4,  ..., 25, 25, 24],\n",
      "        [23, 10,  4,  ..., 25, 25, 24],\n",
      "        [23, 10,  4,  ..., 25, 25, 24],\n",
      "        ...,\n",
      "        [23, 20, 20,  ..., 25, 25, 24],\n",
      "        [23, 10,  4,  ..., 25, 25, 24],\n",
      "        [23, 10,  4,  ..., 25, 25, 24]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "#load in training data\n",
    "train_loader = FastaDataLoader(db_file=training_config.db_file,\n",
    "                               table_name=training_config.table_name, \n",
    "                               encoding_fn = tokenize_seq, \n",
    "                               batch_size=training_config.batch_size,\n",
    "                               shuffle=training_config.shuffle)\n",
    "\n",
    "for i, batch in enumerate(train_loader):\n",
    "    print(batch)\n",
    "    if i == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "no such table: testing_sequences",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_loader \u001b[39m=\u001b[39m FastaDataLoader(db_file\u001b[39m=\u001b[39;49mtesting_config\u001b[39m.\u001b[39;49mdb_file,\n\u001b[1;32m      2\u001b[0m                                table_name\u001b[39m=\u001b[39;49mtesting_config\u001b[39m.\u001b[39;49mtable_name, \n\u001b[1;32m      3\u001b[0m                                encoding_fn \u001b[39m=\u001b[39;49m tokenize_seq, \n\u001b[1;32m      4\u001b[0m                                batch_size\u001b[39m=\u001b[39;49mtesting_config\u001b[39m.\u001b[39;49mbatch_size,\n\u001b[1;32m      5\u001b[0m                                shuffle\u001b[39m=\u001b[39;49mtesting_config\u001b[39m.\u001b[39;49mshuffle)\n\u001b[1;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m i, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(test_loader):\n\u001b[1;32m      8\u001b[0m     \u001b[39mprint\u001b[39m(batch)\n",
      "Cell \u001b[0;32mIn[8], line 29\u001b[0m, in \u001b[0;36mFastaDataLoader.__init__\u001b[0;34m(self, db_file, table_name, encoding_fn, batch_size, shuffle)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, db_file: \u001b[39mstr\u001b[39m, table_name: \u001b[39mstr\u001b[39m, encoding_fn, batch_size: \u001b[39mint\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m---> 29\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset \u001b[39m=\u001b[39m FastaDataset(db_file, table_name, encoding_fn)\n\u001b[1;32m     30\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataloader \u001b[39m=\u001b[39m DataLoader(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39mshuffle)\n",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m, in \u001b[0;36mFastaDataset.__init__\u001b[0;34m(self, db_file, table_name, encoding_fn)\u001b[0m\n\u001b[1;32m      9\u001b[0m conn \u001b[39m=\u001b[39m sqlite3\u001b[39m.\u001b[39mconnect(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdb_file)\n\u001b[1;32m     10\u001b[0m cursor \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mcursor()\n\u001b[0;32m---> 11\u001b[0m cursor\u001b[39m.\u001b[39;49mexecute(\u001b[39m\"\u001b[39;49m\u001b[39mSELECT sequence FROM \u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m ORDER BY id\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mformat(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtable_name))\n\u001b[1;32m     12\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msequences \u001b[39m=\u001b[39m [row[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m cursor\u001b[39m.\u001b[39mfetchall()]\n\u001b[1;32m     13\u001b[0m conn\u001b[39m.\u001b[39mclose()\n",
      "\u001b[0;31mOperationalError\u001b[0m: no such table: testing_sequences"
     ]
    }
   ],
   "source": [
    "test_loader = FastaDataLoader(db_file=testing_config.db_file,\n",
    "                               table_name=testing_config.table_name, \n",
    "                               encoding_fn = tokenize_seq, \n",
    "                               batch_size=testing_config.batch_size,\n",
    "                               shuffle=testing_config.shuffle)\n",
    "\n",
    "for i, batch in enumerate(test_loader):\n",
    "    print(batch)\n",
    "    if i == 1:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
