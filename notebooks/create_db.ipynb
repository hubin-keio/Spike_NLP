{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create databases with sqlite 3, and call it something like 'spike_prot.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"spike_prot.db\")\n",
    "db_cursor = conn.cursor()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets create a data table for trainign sequences with it's simple data structure and one for test sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x7f23c9617730>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create train sequences table\n",
    "db_cursor.execute('''CREATE TABLE train_sequences\n",
    "             (id INTEGER PRIMARY KEY,\n",
    "              header TEXT,\n",
    "              sequence TEXT)''')\n",
    "\n",
    "#create test sequences table\n",
    "db_cursor.execute('''CREATE TABLE test_sequences\n",
    "             (id INTEGER PRIMARY KEY,\n",
    "              header TEXT,\n",
    "              sequence TEXT)''')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the fasta files in and distrubute them to their correct collections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_seqs = SeqIO.parse(open(os.path.abspath('../data/spikeprot0203.clean.uniq.training.fasta')),'fasta')\n",
    "\n",
    "for i, fasta in enumerate(training_seqs):\n",
    "    header, seq = fasta.id, str(fasta.seq)\n",
    "    db_cursor.execute(\"INSERT INTO train_sequences (header, sequence) VALUES (?,?)\", (header,seq))\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"spike_prot.db\")\n",
    "db_cursor = conn.cursor()\n",
    "\n",
    "testing_seqs = SeqIO.parse(open(os.path.abspath('../data/spikeprot0203.clean.uniq.testing.fasta')), 'fasta')\n",
    "\n",
    "for i, fasta in enumerate(testing_seqs):\n",
    "    header, seq = fasta.id, str(fasta.seq)\n",
    "    db_cursor.execute(\"INSERT INTO test_sequences (header, sequence) VALUES (?,?)\", (header,seq))\n",
    "    \n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSSVLHSTQDLFLPFFSNVTWFHAIHVSGTNGTKRFDNPVLPFNDGVYFASTEKSNIIRGWIFGTTLDSKTQSLLIVNNATNVVIKVCEFQFCNDPFLGVYYHKNNKSWMESEFRVYSSANNCTFEYVSQPFLMDLEGKQGNFKNLREFVFKNIDGYFKIYSKHTPINLVRDLPQGFSALEPLVDLPIGINITRFQTLLALHRSYLTPGDSSSGWTAGAAAYYVGYLQPRTFLLKYNENGTITDAVDCALDPLSETKCTLKSFTVEKGIYQTSNFRVQPTESIVRFPNITNLCPFGEVFNATRFASVYAWNRKRISNCVADYSVLYNSASFSTFKCYGVSPTKLNDLCFTNVYADSFVIRGDEVRQIAPGQTGKIADYNYKLPDDFTGCVIAWNSNNLDSKVGGNYNYLYRLFRKSNLKPFERDISTEIYQAGSTPCNGVEGFNCYFPLQSYGFQPTNGVGYQPYRVVVLSFELLHAPATVCGPKKSTNLVKNKCVNFNFNGLTGTGVLTESNKKFLPFQQFGRDIADTTDAVRDPQTLEILDITPCSFGGVSVITPGTNTSNQVAVLYQDVNCTEVPVAIHADQLTPTWRVYSTGSNVFQTRAGCLIGAEHVNNSYECDIPIGAGICASYQTQTNSPRRARSVASQSIIAYTMSLGAENSVAYSNNSIAIPTNFTISVTTEILPVSMTKTSVDCTMYICGDSTECSNLLLQYGSFCTQLNRALTGIAVEQDKNTQEVFAQVKQIYKTPPIKDFGGFNFSQILPDPSKPSKRSFIEDLLFNKVTLADAGFIKQYGDCLGDIAARDLICAQKFNGLTVLPPLLTDEMIAQYTSALLAGTITSGWTFGAGAALQIPFAMQMAYRFNGIGVTQNVLYENQKLIANQFNSAIGKIQDSLSSTASALGKLQDVVNQNAQALNTLVKQLSSNFGAISSVLNDILSRLDKVEAEVQIDRLITGRLQSLQTYVTQQLIRAAEIRASANLAATKMSECVLGQSKRVDFCGKGYHLMSFPQSAPHGVVFLHVTYVPAQEKNFTTAPAICHDGKAHFPREGVFVSNGTHWFVTQRNFYEPQIITTDNTFVSGNCDVVIGIVNNTVYDPLQPELDSFKEELDKYFKNHTSPDVDLGDISGINASVVNIQKEIDRLNEVAKNLNESLIDLQELGKYEQYIKWPWYIWLGFIAGLIAIVMVTIMLCCMTSCCSCLKGCCSCGSCCKFDEDDSEPVLKGVKLHYT*\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect(\"spike_prot.db\")\n",
    "db_cursor = conn.cursor()\n",
    "\n",
    "db_cursor.execute(\"SELECT sequence FROM train_sequences\")\n",
    "train_result = db_cursor.fetchone()\n",
    "train_sequence = train_result[0]\n",
    "print(train_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSSVLHSTQDLFLPFFSNVTWFHAIHVSGTNGTKRFDNPVLPFNDGVYFASTEKSNIIRGWIFGTTLDSKTQSLLIVNNATNVVIKVCEFQFCNDPFLGVYYHKNNKSWMESEFRVYSSANNCTFEYVSQPFLMDLEGKQGNFKNLREFVFKNIDGYFKIYSKHTPINLVRDLPQGFSVLEPLVDLPIGINITRFQTLLALHRSYLTPGDSSSGWTAGAAAYYVGYLQPRTFLLKYNENGTITDAVDCALDPLSETKCTLKSFTVEKGIYQTSNFRVQPTESIVRFPNITNLCPFGEVFNATRFASVYAWNRKRISNCVADYSVLYNSASFSTFKCYGVSPTKLNDLCFTNVYADSFVIRGDEVRQIAPGQTGKIADYNYKLPDDFTGCVIAWNSNNLDSKVGGNYNYLYRLFRKSNLKPFERDISTEIYQAGSTPCNGVEGFNCYFPLQSYGFQPTNGVGYQPYRVVVLSFELLHAPATVCGPKKSTNLVKNKCVNFNFNGLTGTGVLTESNKKFLPFQQFGRDIADTTDAVRDPQTLEILDITPCSFGGVSVITPGTNTSNQVAVLYQGVNCTEVPVAIHADQLTPTWRVYSTGSNVFQTRAGCLIGAEHVNNSYECDIPIGAGICASYXTQTNSPRRARSVASQSIIAYTMSLGAENSVAYSNNSIAIPTNFTISVTTEILPVSMTKTSVDCTMYICGDSTECSNLLLQYGSFCTQLNRALTGIAVEQDKNTQEVFAQVKQIYKTPPIKDFGGFNFSQILPDPSKPSKRSFIEDLLFNKVTLADAGFIKQYGDCLGDIAARDLICAQKFNGLTVLPPLLTDEMIAQYTSALLAGTITSGWTFGAGAALQIPFAMQMAYRFNGIGVTQNVLYENQKLIANQFNSAIGKIQDSLSSTASALGKLQDVVNQNAQALNTLVKQLSSNFGAISSVLNDILSRLDKVEAEVQIDRLITGRLQSLQTYVTQQLIRAAEIRASANLAATKMSECVLGQSKRVDFCGKGYHLMSFPQSAPHGVVFLHVTYVPAQEKNFTTAPAICHDGKAHFPREGVFVSNGTHWFVTQRNFYEPQIIXTDNTFVSGNCDVVIGIVNNTVYDPLQPELDSFKEELDKYFKNHTSPDVDLGDISGINASVVNIQKEIDRLNEVAKNLNESLIDLQELGKYEQYIKWPWYIWLGFIAGLIAIVMVTIMLCCMTSCCSCLKGCCSCGSCCKFDEDDSEPVLKGVKLHYT*\n"
     ]
    }
   ],
   "source": [
    "db_cursor.execute(\"SELECT sequence FROM test_sequences\")\n",
    "test_result = db_cursor.fetchone()\n",
    "test_sequence = test_result[0]\n",
    "print(test_sequence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how to use this as way to load in our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastaDataset(Dataset):\n",
    "    \"\"\"Create Dataset compatible indexing of fasta file\n",
    "    \"\"\"\n",
    "    def __init__(self, db_file: str, table_name: str, encoding_fn) -> None:\n",
    "        self.db_file = db_file\n",
    "        self.table_name = table_name\n",
    "        self.encoding_fn = encoding_fn\n",
    "        \n",
    "        conn = sqlite3.connect(self.db_file)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT sequence FROM {} ORDER BY id\".format(self.table_name))\n",
    "        self.sequences = [row[0] for row in cursor.fetchall()]\n",
    "        conn.close()\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        sequence = sequence.replace(\"*\", \"\")\n",
    "        encoding = self.encoding_fn(sequence)\n",
    "        return encoding\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "\n",
    "class FastaDataLoader:\n",
    "    \"\"\"Wrapper for fasta dataloader\n",
    "    \"\"\"\n",
    "    def __init__(self, db_file: str, table_name: str, encoding_fn, batch_size: int, shuffle=True):\n",
    "        self.dataset = FastaDataset(db_file, table_name, encoding_fn)\n",
    "        self.dataloader = DataLoader(self.dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_AAS = 'ACDEFGHIKLMNPQRSTUVWXY'\n",
    "ADDITIONAL_TOKENS = ['<OTHER>', '<START>', '<END>', '<PAD>']\n",
    "\n",
    "# Each sequence is added <START> and <END>. \"<PAD>\" are added to sequence shorten than max_len.\n",
    "ADDED_TOKENS_PER_SEQ = 2\n",
    "\n",
    "n_aas = len(ALL_AAS)\n",
    "aa_to_token_index = {aa: i for i, aa in enumerate(ALL_AAS)}\n",
    "additional_token_to_index = {token: i + n_aas for i, token in enumerate(ADDITIONAL_TOKENS)}\n",
    "token_to_index = {**aa_to_token_index, **additional_token_to_index}\n",
    "index_to_token = {index: token for token, index in token_to_index.items()}\n",
    "n_tokens = len(token_to_index)\n",
    "\n",
    "def tokenize_seq(seq: str, max_len:int=1500) -> torch.IntTensor:\n",
    "    \"\"\"\n",
    "    Tokenize a sequence.\n",
    "\n",
    "    It is the caller's responsibility to infer the maximum length of the input. In case of\n",
    "    tokenizing a batch of sequences, the maximum length shall be assigned to the lenght of\n",
    "    the longest sequence in the same batch. \n",
    "\n",
    "\n",
    "    seq: input insquence\n",
    "    max_len: maximum number of tokens, including the special tokens such as <START>, <END>.\n",
    "    \n",
    "    \"\"\"\n",
    "    seq = seq.upper()   # All in upper case.\n",
    "    other_token_index = additional_token_to_index['<OTHER>']\n",
    "    token_seq = [additional_token_to_index['<START>']] + [aa_to_token_index.get(aa, other_token_index) for aa in seq]\n",
    "    if len(token_seq) < max_len - 1: # -1 is for the <END> token\n",
    "        n_pads = max_len -1 - len(token_seq)\n",
    "        token_seq.extend(token_to_index['<PAD>'] for _ in range(n_pads))\n",
    "    token_seq += [additional_token_to_index['<END>']]\n",
    "    return torch.IntTensor(token_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class training_config:\n",
    "    batch_size: int = 32\n",
    "    shuffle: bool = True\n",
    "    table_name: str = 'train_sequences'\n",
    "    db_file: str = 'spike_prot.db'\n",
    "    \n",
    "@dataclass\n",
    "class testing_config:\n",
    "    batch_size: int = 32\n",
    "    shuffle: bool = True\n",
    "    table_name: str = 'test_sequences'\n",
    "    db_file: str = 'spike_prot.db'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[23, 10,  4,  ..., 25, 25, 24],\n",
      "        [23, 10,  4,  ..., 25, 25, 24],\n",
      "        [23, 10,  4,  ..., 25, 25, 24],\n",
      "        ...,\n",
      "        [23, 10,  4,  ..., 25, 25, 24],\n",
      "        [23, 10,  4,  ..., 25, 25, 24],\n",
      "        [23, 10,  4,  ..., 25, 25, 24]], dtype=torch.int32)\n",
      "tensor([[23, 10,  4,  ..., 25, 25, 24],\n",
      "        [23, 10,  4,  ..., 25, 25, 24],\n",
      "        [23, 10,  4,  ..., 25, 25, 24],\n",
      "        ...,\n",
      "        [23, 10,  4,  ..., 25, 25, 24],\n",
      "        [23, 10,  4,  ..., 25, 25, 24],\n",
      "        [23, 10,  4,  ..., 25, 25, 24]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "#load in training data\n",
    "train_loader = FastaDataLoader(db_file=training_config.db_file,\n",
    "                               table_name=training_config.table_name, \n",
    "                               encoding_fn = tokenize_seq, \n",
    "                               batch_size=training_config.batch_size,\n",
    "                               shuffle=training_config.shuffle)\n",
    "\n",
    "for i, batch in enumerate(train_loader):\n",
    "    print(batch)\n",
    "    if i == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[23, 10,  4,  ..., 25, 25, 24],\n",
      "        [23, 10,  4,  ..., 25, 25, 24],\n",
      "        [23, 10,  4,  ..., 25, 25, 24],\n",
      "        ...,\n",
      "        [23, 10,  4,  ..., 25, 25, 24],\n",
      "        [23, 20, 20,  ..., 25, 25, 24],\n",
      "        [23, 10,  4,  ..., 25, 25, 24]], dtype=torch.int32)\n",
      "tensor([[23, 10,  4,  ..., 25, 25, 24],\n",
      "        [23, 10,  4,  ..., 25, 25, 24],\n",
      "        [23, 20, 20,  ..., 25, 25, 24],\n",
      "        ...,\n",
      "        [23, 10,  4,  ..., 25, 25, 24],\n",
      "        [23, 10,  4,  ..., 25, 25, 24],\n",
      "        [23, 10,  4,  ..., 25, 25, 24]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "test_loader = FastaDataLoader(db_file=testing_config.db_file,\n",
    "                               table_name=testing_config.table_name, \n",
    "                               encoding_fn = tokenize_seq, \n",
    "                               batch_size=testing_config.batch_size,\n",
    "                               shuffle=testing_config.shuffle)\n",
    "\n",
    "for i, batch in enumerate(test_loader):\n",
    "    print(batch)\n",
    "    if i == 1:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
